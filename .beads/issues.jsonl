{"id":"bd-12d","title":"Phase 6: Intelligence - Advanced Capabilities","description":"# Phase 6: Intelligence - Advanced Capabilities\n\n## Purpose\nImplement the advanced intelligence features: solution mining, playbooks, A/B testing, evolutionary optimization, natural language interface, and MCP server mode. This phase realizes the full \"autonomous agent empire command center\" vision.\n\n## Key Deliverables\n1. **Solution Mining**: Extract successful patterns from cass session data\n2. **Playbook Generation**: Create reusable operational procedures from experience\n3. **A/B Testing Framework**: Controlled experiments for agent configurations\n4. **Evolutionary Optimization**: Genetic algorithm for config optimization\n5. **Natural Language Interface**: Query system state with plain English\n6. **MCP Server Mode**: vc as an MCP server for agent queries\n\n## Success Criteria\n- Solutions mined and stored from past sessions\n- Playbooks track execution success rates\n- A/B tests run with statistical significance tracking\n- Evolutionary optimizer discovers improved configurations\n- Natural language queries return accurate, actionable answers\n- Agents can query vc via MCP tools\n\n## Technical Context\n\n### Solution Mining Architecture\n```rust\npub struct SolutionMiner {\n    pub async fn mine_solutions(&self) -> Vec<MinedSolution> {\n        // Find successful sessions\n        // Extract key decision points\n        // Identify success factors\n        // Generalize into reusable patterns\n    }\n}\n\npub struct MinedSolution {\n    pub problem_pattern: ProblemSignature,\n    pub solution_steps: Vec<Step>,\n    pub success_rate: f64,\n    pub applicable_contexts: Vec<Context>,\n    pub confidence: f64,\n}\n```\n\n### Gotcha Database (from Phase 5)\n```rust\npub struct Gotcha {\n    pub title: String,\n    pub triggers: Vec<GotchaTrigger>,\n    pub failure_pattern: FailurePattern,\n    pub prevention: String,\n    pub fix: String,\n    pub occurrences: u32,\n}\n```\n\nGotchas are surfaced proactively when triggers match current context.\n\n### A/B Testing Framework\n```rust\npub struct Experiment {\n    pub hypothesis: String,\n    pub control: ExperimentArm,\n    pub treatment: ExperimentArm,\n    pub primary_metric: MetricId,\n    pub stop_conditions: Vec<StopCondition>,\n    pub auto_rollback: bool,\n}\n```\n\nRun controlled comparisons of agent configurations with automatic statistical analysis.\n\n### Evolutionary Optimization\n```rust\npub struct EvolutionaryOptimizer {\n    pub population: Vec<AgentConfig>,\n    pub fitness: Box<dyn Fn(&AgentConfig, &PerformanceData) -> f64>,\n    \n    pub fn evolve(&mut self) -> Vec<AgentConfig> {\n        // Tournament selection\n        // Crossover\n        // Mutation\n        // Evaluate fitness\n        // Return top performers\n    }\n}\n```\n\nOver generations, discover optimal agent configurations for specific task types.\n\n### Natural Language Interface\nMap intents to predefined queries:\n- \"Why is orko slow?\" -> cpu + rate limit + stuck agent analysis\n- \"Which account should I use?\" -> Oracle recommendations\n- \"What's blocking progress?\" -> bv triage summary\n\nAvoid free-form SQL generation until strict guardrails exist.\n\n### MCP Server Mode\nvc exposes tools for agent queries:\n```rust\nvc_fleet_status { ... }        // Fleet health overview\nvc_find_capacity { ... }       // Available agents for work\nvc_request_resources { ... }   // Resource allocation\nvc_report_completion { ... }   // Task completion metrics\nvc_get_recommendations { ... } // AI-powered suggestions\n```\n\n## Dependencies\n- Requires Phase 5 (web dashboard, autopilot, knowledge base foundation)\n\n## Estimated Scope\n- ~3-4 weeks\n- Final milestone: full autonomous operation capability\n\n## Testing & E2E Expectations\n- Every deliverable in this phase must add unit tests per bd-1x9.\n- Extend bd-30z with phase-specific e2e scenarios and structured logs.\n- Phase completion requires `cargo test --workspace` + phase e2e suite with JSON summaries in tests/logs/.\n","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-27T16:18:56.572865376Z","created_by":"ubuntu","updated_at":"2026-01-27T17:02:01.234954723Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-12d","depends_on_id":"bd-3e2","type":"blocks","created_at":"2026-01-27T16:19:04.470180374Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12d.1","title":"Implement solution mining from agent sessions","description":"## Overview\nBuild automatic solution mining that extracts reusable solutions, patterns, and learnings from agent session transcripts.\n\n## Background\nAgent sessions contain valuable problem-solving knowledge that currently gets lost. Solution mining automatically identifies successful problem resolutions and extracts them into the knowledge base.\n\n## Mining Pipeline\n```\n┌─────────────────────────────────────────────────────────────┐\n│                    Solution Mining Pipeline                  │\n├─────────────────────────────────────────────────────────────┤\n│  1. Session Selection  → Find completed successful sessions │\n│  2. Transcript Analysis → Identify problem-solution pairs   │\n│  3. Solution Extraction → Extract reusable patterns         │\n│  4. Quality Scoring    → Rank by usefulness                 │\n│  5. Knowledge Storage  → Store in knowledge base            │\n│  6. Deduplication      → Merge similar solutions            │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Session Selection\n```rust\npub struct SessionSelector {\n    store: Arc<VcStore>,\n}\n\nimpl SessionSelector {\n    /// Find sessions worth mining\n    pub async fn select_candidates(&self, since: DateTime<Utc>) -> Result<Vec<SessionCandidate>> {\n        self.store.query_vec::<SessionCandidate>(r#\"\n            SELECT \n                s.session_id,\n                s.project,\n                s.started_at,\n                s.ended_at,\n                s.model,\n                COUNT(DISTINCT e.error_type) as error_count,\n                COUNT(DISTINCT r.resolution_id) as resolution_count\n            FROM cass_sessions s\n            LEFT JOIN session_errors e ON s.session_id = e.session_id\n            LEFT JOIN session_resolutions r ON s.session_id = r.session_id\n            WHERE s.ended_at IS NOT NULL\n              AND s.ended_at > ?\n              AND s.outcome = \"success\"\n              AND NOT EXISTS (SELECT 1 FROM mined_sessions WHERE session_id = s.session_id)\n            ORDER BY resolution_count DESC, s.ended_at DESC\n            LIMIT 100\n        \"#).bind(since).await\n    }\n}\n```\n\n## Transcript Analysis\n```rust\npub struct TranscriptAnalyzer {\n    llm: Arc<LlmClient>,\n}\n\nimpl TranscriptAnalyzer {\n    /// Analyze transcript for problem-solution pairs\n    pub async fn analyze(&self, transcript: &str) -> Result<Vec<ProblemSolutionPair>> {\n        let prompt = format!(r#\"\n            Analyze this agent session transcript and identify problem-solution pairs.\n            \n            For each problem solved in the session:\n            1. Describe the problem clearly\n            2. Describe the solution that worked\n            3. Identify key insights or patterns\n            4. Rate the solution quality (1-5)\n            5. List relevant tags\n            \n            Return JSON array of:\n            {{\n                \"problem\": \"description\",\n                \"solution\": \"description\",\n                \"insights\": [\"insight1\", \"insight2\"],\n                \"code_snippets\": [\"snippet1\"],\n                \"quality\": 4,\n                \"tags\": [\"rust\", \"error-handling\"]\n            }}\n            \n            Transcript:\n            {}\n        \"#, transcript);\n        \n        let response = self.llm.complete(&prompt).await?;\n        let pairs: Vec<ProblemSolutionPair> = serde_json::from_str(&response)?;\n        \n        Ok(pairs)\n    }\n}\n```\n\n## Solution Extraction\n```rust\npub struct SolutionExtractor {\n    analyzer: TranscriptAnalyzer,\n    store: Arc<VcStore>,\n}\n\nimpl SolutionExtractor {\n    /// Extract and store solutions from a session\n    pub async fn extract(&self, session_id: &str) -> Result<Vec<KnowledgeEntry>> {\n        // Get session transcript\n        let transcript = self.store.get_session_transcript(session_id).await?;\n        \n        // Analyze for problem-solution pairs\n        let pairs = self.analyzer.analyze(&transcript).await?;\n        \n        let mut entries = Vec::new();\n        \n        for pair in pairs {\n            if pair.quality >= 3 {  // Only keep quality solutions\n                let entry = KnowledgeEntry {\n                    entry_type: EntryType::Solution,\n                    title: self.generate_title(&pair),\n                    summary: pair.problem.clone(),\n                    content: self.format_solution(&pair),\n                    source_session_id: Some(session_id.to_string()),\n                    tags: pair.tags,\n                    usefulness_score: pair.quality as f64 / 5.0,\n                    ..Default::default()\n                };\n                \n                // Check for duplicates before storing\n                if !self.is_duplicate(&entry).await? {\n                    self.store.insert_knowledge_entry(&entry).await?;\n                    entries.push(entry);\n                }\n            }\n        }\n        \n        // Mark session as mined\n        self.store.mark_session_mined(session_id).await?;\n        \n        Ok(entries)\n    }\n    \n    fn format_solution(&self, pair: &ProblemSolutionPair) -> String {\n        let mut content = String::new();\n        \n        content.push_str(\"## Problem\\n\");\n        content.push_str(&pair.problem);\n        content.push_str(\"\\n\\n## Solution\\n\");\n        content.push_str(&pair.solution);\n        \n        if !pair.insights.is_empty() {\n            content.push_str(\"\\n\\n## Key Insights\\n\");\n            for insight in &pair.insights {\n                content.push_str(&format!(\"- {}\\n\", insight));\n            }\n        }\n        \n        if !pair.code_snippets.is_empty() {\n            content.push_str(\"\\n\\n## Code Examples\\n\");\n            for snippet in &pair.code_snippets {\n                content.push_str(&format!(\"```\\n{}\\n```\\n\", snippet));\n            }\n        }\n        \n        content\n    }\n}\n```\n\n## Mining Scheduler\n```rust\npub struct MiningScheduler {\n    extractor: SolutionExtractor,\n    selector: SessionSelector,\n}\n\nimpl MiningScheduler {\n    /// Run mining on a schedule\n    pub async fn run_scheduled(&self) {\n        let mut interval = tokio::time::interval(Duration::from_hours(1));\n        \n        loop {\n            interval.tick().await;\n            \n            let since = Utc::now() - chrono::Duration::hours(24);\n            match self.selector.select_candidates(since).await {\n                Ok(candidates) => {\n                    for candidate in candidates.into_iter().take(10) {\n                        if let Err(e) = self.extractor.extract(&candidate.session_id).await {\n                            tracing::error!(\"Mining failed for {}: {}\", candidate.session_id, e);\n                        }\n                    }\n                }\n                Err(e) => tracing::error!(\"Candidate selection failed: {}\", e),\n            }\n        }\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] Session selection finds unmined successful sessions\n- [ ] LLM analysis extracts problem-solution pairs\n- [ ] Solutions formatted with problem, solution, insights, code\n- [ ] Quality scoring filters low-quality extractions\n- [ ] Deduplication prevents redundant entries\n- [ ] Scheduled mining runs automatically\n- [ ] Mining status tracked per session\n- [ ] Manual trigger available via CLI\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:35:51.668933879Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:06.338098464Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-12d.1","depends_on_id":"bd-12d","type":"parent-child","created_at":"2026-01-27T16:35:51.668933879Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.1","depends_on_id":"bd-2mv.3","type":"blocks","created_at":"2026-01-27T16:38:25.810180782Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.1","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:06.337776458Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.1","depends_on_id":"bd-3e2.4","type":"blocks","created_at":"2026-01-27T16:38:25.601478010Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12d.2","title":"Implement automatic playbook generation","description":"## Overview\nBuild automatic playbook generation that creates Guardian playbooks from observed successful remediation patterns.\n\n## Background\nWhen operators manually resolve issues (rate limits, stuck agents, errors), their actions can be captured and converted into automated playbooks for future similar situations.\n\n## Playbook Generation Pipeline\n```\n┌─────────────────────────────────────────────────────────────┐\n│                Playbook Generation Pipeline                  │\n├─────────────────────────────────────────────────────────────┤\n│  1. Event Detection    → Alert fired + manual resolution    │\n│  2. Action Capture     → Record operator actions taken      │\n│  3. Pattern Recognition → Identify repeated patterns        │\n│  4. Playbook Draft     → Generate playbook from pattern     │\n│  5. Validation         → Test playbook safely               │\n│  6. Approval           → Human review before activation     │\n└─────────────────────────────────────────────────────────────┘\n```\n\n## Action Capture\n```rust\npub struct ActionCapture {\n    store: Arc<VcStore>,\n}\n\nimpl ActionCapture {\n    /// Capture actions taken after an alert\n    pub async fn capture_resolution(\n        &self,\n        alert_id: i64,\n        actions: Vec<CapturedAction>,\n        outcome: ResolutionOutcome,\n    ) -> Result<Resolution> {\n        let alert = self.store.get_alert(alert_id).await?;\n        \n        let resolution = Resolution {\n            alert_id,\n            alert_type: alert.rule_id.clone(),\n            trigger_context: alert.context.clone(),\n            actions: actions.clone(),\n            outcome,\n            captured_at: Utc::now(),\n        };\n        \n        self.store.insert_resolution(&resolution).await?;\n        \n        // Check if we have enough similar resolutions for pattern detection\n        self.check_pattern_threshold(&alert.rule_id).await?;\n        \n        Ok(resolution)\n    }\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub enum CapturedAction {\n    Command { cmd: String, args: Vec<String>, success: bool },\n    AccountSwitch { from: String, to: String },\n    ProcessKill { pid: u32, name: String },\n    ConfigChange { key: String, old: String, new: String },\n    ServiceRestart { name: String },\n    Custom { description: String, metadata: Value },\n}\n```\n\n## Pattern Recognition\n```rust\npub struct PatternRecognizer {\n    store: Arc<VcStore>,\n    llm: Arc<LlmClient>,\n}\n\nimpl PatternRecognizer {\n    /// Find patterns in successful resolutions\n    pub async fn find_patterns(&self, alert_type: &str) -> Result<Vec<ResolutionPattern>> {\n        // Get successful resolutions for this alert type\n        let resolutions = self.store.query_vec::<Resolution>(r#\"\n            SELECT * FROM resolutions\n            WHERE alert_type = ?\n              AND outcome = \"success\"\n            ORDER BY captured_at DESC\n            LIMIT 50\n        \"#).bind(alert_type).await?;\n        \n        if resolutions.len() < 3 {\n            return Ok(vec![]);  // Not enough data\n        }\n        \n        // Use LLM to identify common patterns\n        let prompt = format!(r#\"\n            Analyze these successful resolutions for the alert type \"{}\" and identify common patterns.\n            \n            Resolutions:\n            {}\n            \n            For each pattern found:\n            1. Describe the pattern\n            2. List the common steps\n            3. Identify required conditions\n            4. Estimate confidence (0-1)\n            \n            Return JSON array of patterns.\n        \"#, alert_type, serde_json::to_string_pretty(&resolutions)?);\n        \n        let response = self.llm.complete(&prompt).await?;\n        let patterns: Vec<ResolutionPattern> = serde_json::from_str(&response)?;\n        \n        Ok(patterns)\n    }\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct ResolutionPattern {\n    pub description: String,\n    pub steps: Vec<PatternStep>,\n    pub conditions: Vec<String>,\n    pub confidence: f64,\n    pub sample_count: usize,\n}\n```\n\n## Playbook Draft Generation\n```rust\npub struct PlaybookGenerator {\n    recognizer: PatternRecognizer,\n    store: Arc<VcStore>,\n}\n\nimpl PlaybookGenerator {\n    /// Generate a playbook from a recognized pattern\n    pub fn generate_from_pattern(&self, pattern: &ResolutionPattern, alert_type: &str) -> PlaybookDraft {\n        let steps: Vec<PlaybookStep> = pattern.steps.iter().map(|s| {\n            match s {\n                PatternStep::Command { cmd, args } => PlaybookStep::Command {\n                    cmd: cmd.clone(),\n                    args: args.clone(),\n                    timeout_seconds: 30,\n                    allow_failure: false,\n                },\n                PatternStep::AccountSwitch { strategy } => PlaybookStep::SwitchAccount {\n                    program: \"auto\".into(),\n                    strategy: strategy.clone(),\n                },\n                PatternStep::Wait { seconds } => PlaybookStep::Wait { seconds: *seconds },\n                PatternStep::Notify { message } => PlaybookStep::Notify {\n                    channel: \"tui\".into(),\n                    message: message.clone(),\n                },\n            }\n        }).collect();\n        \n        PlaybookDraft {\n            playbook_id: format!(\"auto-{}-{}\", alert_type, Uuid::new_v4()),\n            name: format!(\"Auto-generated: {}\", pattern.description),\n            description: format!(\n                \"Automatically generated from {} successful resolutions. Confidence: {:.0}%\",\n                pattern.sample_count,\n                pattern.confidence * 100.0\n            ),\n            trigger: PlaybookTrigger::OnAlert { rule_id: alert_type.to_string() },\n            steps,\n            requires_approval: true,  // Always require approval for auto-generated\n            source_pattern: pattern.clone(),\n            status: DraftStatus::PendingReview,\n        }\n    }\n    \n    /// Generate playbooks for all alert types with enough data\n    pub async fn generate_all(&self) -> Result<Vec<PlaybookDraft>> {\n        let alert_types = self.store.query_vec::<String>(\n            \"SELECT DISTINCT alert_type FROM resolutions WHERE outcome = \\\"success\\\"\"\n        ).await?;\n        \n        let mut drafts = Vec::new();\n        \n        for alert_type in alert_types {\n            let patterns = self.recognizer.find_patterns(&alert_type).await?;\n            \n            for pattern in patterns {\n                if pattern.confidence >= 0.7 && pattern.sample_count >= 3 {\n                    let draft = self.generate_from_pattern(&pattern, &alert_type);\n                    self.store.insert_playbook_draft(&draft).await?;\n                    drafts.push(draft);\n                }\n            }\n        }\n        \n        Ok(drafts)\n    }\n}\n```\n\n## Validation & Approval\n```rust\npub struct PlaybookValidator {\n    store: Arc<VcStore>,\n}\n\nimpl PlaybookValidator {\n    /// Validate a playbook draft before activation\n    pub async fn validate(&self, draft: &PlaybookDraft) -> Result<ValidationResult> {\n        let mut issues = Vec::new();\n        \n        // Check for dangerous commands\n        for step in &draft.steps {\n            if let PlaybookStep::Command { cmd, args, .. } = step {\n                if is_dangerous_command(cmd, args) {\n                    issues.push(ValidationIssue::DangerousCommand(cmd.clone()));\n                }\n            }\n        }\n        \n        // Check trigger condition exists\n        if let PlaybookTrigger::OnAlert { rule_id } = &draft.trigger {\n            if !self.store.alert_rule_exists(rule_id).await? {\n                issues.push(ValidationIssue::InvalidTrigger(rule_id.clone()));\n            }\n        }\n        \n        // Check step feasibility\n        for step in &draft.steps {\n            if let Err(e) = self.validate_step(step).await {\n                issues.push(ValidationIssue::InvalidStep(e.to_string()));\n            }\n        }\n        \n        Ok(ValidationResult {\n            valid: issues.is_empty(),\n            issues,\n            recommendations: self.generate_recommendations(draft),\n        })\n    }\n    \n    /// Approve and activate a playbook\n    pub async fn approve(&self, draft_id: &str, approver: &str) -> Result<Playbook> {\n        let draft = self.store.get_playbook_draft(draft_id).await?;\n        \n        // Validate first\n        let validation = self.validate(&draft).await?;\n        if !validation.valid {\n            return Err(Error::ValidationFailed(validation.issues));\n        }\n        \n        // Convert to active playbook\n        let playbook = Playbook::from_draft(draft, approver);\n        self.store.insert_playbook(&playbook).await?;\n        self.store.mark_draft_approved(draft_id, approver).await?;\n        \n        Ok(playbook)\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] Action capture records operator remediation steps\n- [ ] Pattern recognition finds common resolution sequences\n- [ ] Playbook drafts generated from patterns\n- [ ] Confidence threshold filters low-quality patterns\n- [ ] Validation checks for dangerous commands\n- [ ] Approval workflow before activation\n- [ ] Generated playbooks marked as auto-generated\n- [ ] Audit trail links playbooks to source patterns\n\n## E2E & Logging\n- Add scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:36:26.860183534Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:07.165076662Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-12d.2","depends_on_id":"bd-12d","type":"parent-child","created_at":"2026-01-27T16:36:26.860183534Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.2","depends_on_id":"bd-12d.1","type":"blocks","created_at":"2026-01-27T16:38:26.198858557Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.2","depends_on_id":"bd-2xm.6","type":"blocks","created_at":"2026-01-27T16:38:26.008689543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.2","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:07.164842621Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12d.3","title":"Implement A/B testing framework for agent configurations","description":"## Overview\nBuild an A/B testing framework that enables controlled experiments with different agent configurations, prompts, and settings.\n\n## Background\nTo optimize agent performance, we need to scientifically compare different configurations. A/B testing provides the framework for running controlled experiments and measuring statistically significant differences.\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE experiments (\n    experiment_id VARCHAR PRIMARY KEY,\n    name VARCHAR NOT NULL,\n    description TEXT,\n    hypothesis VARCHAR,\n    status VARCHAR NOT NULL,              -- draft, running, paused, completed\n    started_at TIMESTAMP,\n    ended_at TIMESTAMP,\n    target_sample_size INTEGER,\n    actual_sample_size INTEGER DEFAULT 0,\n    primary_metric VARCHAR NOT NULL,      -- What we are measuring\n    secondary_metrics VARCHAR[],\n    created_at TIMESTAMP DEFAULT now(),\n    created_by VARCHAR\n);\n\nCREATE TABLE experiment_variants (\n    variant_id VARCHAR PRIMARY KEY,\n    experiment_id VARCHAR REFERENCES experiments(experiment_id),\n    name VARCHAR NOT NULL,                -- \"control\", \"variant_a\", \"variant_b\"\n    is_control BOOLEAN DEFAULT FALSE,\n    config JSON NOT NULL,                 -- The configuration for this variant\n    traffic_weight FLOAT DEFAULT 1.0,     -- Relative traffic allocation\n    sample_count INTEGER DEFAULT 0\n);\n\nCREATE TABLE experiment_assignments (\n    id INTEGER PRIMARY KEY,\n    experiment_id VARCHAR REFERENCES experiments(experiment_id),\n    variant_id VARCHAR REFERENCES experiment_variants(variant_id),\n    session_id VARCHAR NOT NULL,\n    assigned_at TIMESTAMP DEFAULT now(),\n    UNIQUE (experiment_id, session_id)\n);\n\nCREATE TABLE experiment_observations (\n    id INTEGER PRIMARY KEY,\n    experiment_id VARCHAR REFERENCES experiments(experiment_id),\n    variant_id VARCHAR REFERENCES experiment_variants(variant_id),\n    session_id VARCHAR NOT NULL,\n    metric_name VARCHAR NOT NULL,\n    metric_value FLOAT NOT NULL,\n    observed_at TIMESTAMP DEFAULT now()\n);\n\nCREATE TABLE experiment_results (\n    experiment_id VARCHAR PRIMARY KEY REFERENCES experiments(experiment_id),\n    computed_at TIMESTAMP,\n    winner_variant VARCHAR,\n    confidence_level FLOAT,\n    primary_metric_lift FLOAT,            -- % improvement over control\n    is_significant BOOLEAN,\n    full_results JSON                     -- Detailed stats\n);\n```\n\n## Experiment Configuration\n```rust\n#[derive(Debug, Serialize, Deserialize)]\npub struct ExperimentConfig {\n    pub experiment_id: String,\n    pub name: String,\n    pub description: String,\n    pub hypothesis: String,\n    \n    pub variants: Vec<VariantConfig>,\n    \n    pub primary_metric: Metric,\n    pub secondary_metrics: Vec<Metric>,\n    \n    pub target_sample_size: u32,\n    pub min_runtime_hours: u32,\n    pub max_runtime_hours: u32,\n    \n    pub stop_early_on_significance: bool,\n    pub significance_threshold: f64,      // Usually 0.05\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub struct VariantConfig {\n    pub name: String,\n    pub is_control: bool,\n    pub traffic_weight: f64,\n    pub config: AgentConfig,              // The config to apply\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub enum Metric {\n    TaskCompletionRate,\n    AverageTokensPerTask,\n    ErrorRate,\n    AverageResponseTime,\n    CostPerTask,\n    Custom { name: String, query: String },\n}\n```\n\n## Experiment Manager\n```rust\npub struct ExperimentManager {\n    store: Arc<VcStore>,\n}\n\nimpl ExperimentManager {\n    /// Create a new experiment\n    pub async fn create(&self, config: ExperimentConfig) -> Result<Experiment> {\n        // Validate config\n        self.validate_config(&config)?;\n        \n        // Store experiment\n        let experiment = Experiment::from_config(config);\n        self.store.insert_experiment(&experiment).await?;\n        \n        // Create variants\n        for variant in &experiment.variants {\n            self.store.insert_variant(variant).await?;\n        }\n        \n        Ok(experiment)\n    }\n    \n    /// Start an experiment\n    pub async fn start(&self, experiment_id: &str) -> Result<()> {\n        let mut experiment = self.store.get_experiment(experiment_id).await?;\n        \n        if experiment.status != ExperimentStatus::Draft {\n            return Err(Error::InvalidState(\"Can only start draft experiments\"));\n        }\n        \n        experiment.status = ExperimentStatus::Running;\n        experiment.started_at = Some(Utc::now());\n        \n        self.store.update_experiment(&experiment).await?;\n        \n        tracing::info!(\"Started experiment: {}\", experiment_id);\n        Ok(())\n    }\n    \n    /// Assign a session to an experiment variant\n    pub async fn assign(&self, experiment_id: &str, session_id: &str) -> Result<VariantConfig> {\n        let experiment = self.store.get_experiment(experiment_id).await?;\n        \n        if experiment.status != ExperimentStatus::Running {\n            return Err(Error::ExperimentNotRunning);\n        }\n        \n        // Check if already assigned\n        if let Some(existing) = self.store.get_assignment(experiment_id, session_id).await? {\n            return self.store.get_variant(&existing.variant_id).await;\n        }\n        \n        // Weighted random assignment\n        let variant = self.select_variant(&experiment)?;\n        \n        let assignment = Assignment {\n            experiment_id: experiment_id.to_string(),\n            variant_id: variant.variant_id.clone(),\n            session_id: session_id.to_string(),\n            assigned_at: Utc::now(),\n        };\n        \n        self.store.insert_assignment(&assignment).await?;\n        self.store.increment_variant_count(&variant.variant_id).await?;\n        \n        Ok(variant.config)\n    }\n    \n    fn select_variant(&self, experiment: &Experiment) -> Result<&Variant> {\n        let total_weight: f64 = experiment.variants.iter().map(|v| v.traffic_weight).sum();\n        let mut rng = rand::thread_rng();\n        let roll: f64 = rng.gen::<f64>() * total_weight;\n        \n        let mut cumulative = 0.0;\n        for variant in &experiment.variants {\n            cumulative += variant.traffic_weight;\n            if roll <= cumulative {\n                return Ok(variant);\n            }\n        }\n        \n        Ok(experiment.variants.last().unwrap())\n    }\n}\n```\n\n## Metric Collection\n```rust\npub struct MetricCollector {\n    store: Arc<VcStore>,\n}\n\nimpl MetricCollector {\n    /// Record an observation for an experiment\n    pub async fn record(&self, session_id: &str, metric: Metric, value: f64) -> Result<()> {\n        // Find experiments this session is assigned to\n        let assignments = self.store.get_session_assignments(session_id).await?;\n        \n        for assignment in assignments {\n            let experiment = self.store.get_experiment(&assignment.experiment_id).await?;\n            \n            // Check if this metric is relevant\n            let is_relevant = experiment.primary_metric == metric\n                || experiment.secondary_metrics.contains(&metric);\n            \n            if is_relevant {\n                let observation = Observation {\n                    experiment_id: assignment.experiment_id,\n                    variant_id: assignment.variant_id,\n                    session_id: session_id.to_string(),\n                    metric_name: metric.name(),\n                    metric_value: value,\n                    observed_at: Utc::now(),\n                };\n                \n                self.store.insert_observation(&observation).await?;\n            }\n        }\n        \n        Ok(())\n    }\n}\n```\n\n## Statistical Analysis\n```rust\npub struct ExperimentAnalyzer {\n    store: Arc<VcStore>,\n}\n\nimpl ExperimentAnalyzer {\n    /// Analyze experiment results\n    pub async fn analyze(&self, experiment_id: &str) -> Result<ExperimentResults> {\n        let experiment = self.store.get_experiment(experiment_id).await?;\n        let variants = self.store.get_variants(experiment_id).await?;\n        \n        let control = variants.iter().find(|v| v.is_control)\n            .ok_or(Error::NoControl)?;\n        \n        let mut results = ExperimentResults {\n            experiment_id: experiment_id.to_string(),\n            computed_at: Utc::now(),\n            variant_stats: Vec::new(),\n            winner_variant: None,\n            confidence_level: 0.0,\n            is_significant: false,\n        };\n        \n        // Compute stats for each variant\n        for variant in &variants {\n            let observations = self.store.get_observations(\n                experiment_id,\n                &variant.variant_id,\n                &experiment.primary_metric.name()\n            ).await?;\n            \n            let stats = VariantStats {\n                variant_id: variant.variant_id.clone(),\n                sample_size: observations.len(),\n                mean: stats::mean(&observations),\n                std_dev: stats::std_dev(&observations),\n                confidence_interval: stats::confidence_interval(&observations, 0.95),\n            };\n            \n            results.variant_stats.push(stats);\n        }\n        \n        // Compare variants to control using t-test\n        let control_stats = results.variant_stats.iter()\n            .find(|s| s.variant_id == control.variant_id)\n            .unwrap();\n        \n        for stats in &results.variant_stats {\n            if stats.variant_id != control.variant_id {\n                let (t_stat, p_value) = stats::two_sample_t_test(\n                    control_stats,\n                    stats\n                );\n                \n                if p_value < experiment.significance_threshold {\n                    if stats.mean > control_stats.mean {\n                        // This variant is significantly better\n                        let lift = (stats.mean - control_stats.mean) / control_stats.mean;\n                        if results.winner_variant.is_none() || lift > results.primary_metric_lift {\n                            results.winner_variant = Some(stats.variant_id.clone());\n                            results.primary_metric_lift = lift;\n                            results.confidence_level = 1.0 - p_value;\n                            results.is_significant = true;\n                        }\n                    }\n                }\n            }\n        }\n        \n        // Store results\n        self.store.upsert_results(&results).await?;\n        \n        Ok(results)\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] Experiments created with control and variant(s)\n- [ ] Weighted random assignment to variants\n- [ ] Metric collection for assigned sessions\n- [ ] Statistical analysis with t-test\n- [ ] Significance detection with configurable threshold\n- [ ] Early stopping on significance (optional)\n- [ ] Results visualization in TUI/web\n- [ ] Experiment lifecycle management (draft/running/paused/completed)\n\n## E2E & Logging\n- Add scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:37:03.339962850Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:08.235334627Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-12d.3","depends_on_id":"bd-12d","type":"parent-child","created_at":"2026-01-27T16:37:03.339962850Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.3","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:08.234191513Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.3","depends_on_id":"bd-3e2.5","type":"blocks","created_at":"2026-01-27T16:38:26.387781704Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12d.4","title":"Implement evolutionary optimization for agent configs","description":"## Overview\nBuild evolutionary optimization that automatically evolves agent configurations toward better performance using genetic algorithms.\n\n## Background\nManual tuning of agent configurations is time-consuming. Evolutionary optimization uses genetic algorithms to automatically discover high-performing configurations.\n\n## Algorithm\n1. Initialize - Create random population of configs\n2. Evaluate - Score each config on fitness metrics\n3. Select - Choose top performers via tournament\n4. Crossover - Combine traits from parents\n5. Mutate - Random variations\n6. Repeat - Until convergence or generation limit\n\n## Configuration Genome\n- genes: HashMap of configurable parameters\n- Float genes (temperature, etc.)\n- Int genes (max_tokens, retry_count, timeout)\n- Bool genes (streaming, etc.)\n- Choice genes (model selection)\n- fitness: Evaluated score\n\n## Fitness Function\nWeighted combination of:\n- task_success_rate (positive weight)\n- avg_tokens_per_task (negative weight - lower is better)\n- avg_response_time (negative weight)\n- error_rate (negative weight)\n- cost_per_task (negative weight)\n\n## Genetic Operations\n- Crossover: Uniform crossover from two parents\n- Mutation: Gaussian mutation for floats, random flip for bools\n- Elitism: Top N preserved unchanged each generation\n- Tournament selection for parent choice\n\n## Evolution Manager\n- Tracks population across generations\n- Runs fitness evaluation\n- Applies genetic operations\n- Detects convergence\n- Exports best configuration\n\n## Acceptance Criteria\n- [ ] Configuration genome with typed genes\n- [ ] Fitness function combining multiple metrics\n- [ ] Crossover combines parent configurations\n- [ ] Mutation introduces random variations\n- [ ] Tournament selection for parent choice\n- [ ] Elitism preserves top performers\n- [ ] Convergence detection stops evolution\n- [ ] Best configuration exported for use\n- [ ] Evolution history tracked and visualized\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-27T16:37:48.338942923Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:09.249571736Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-12d.4","depends_on_id":"bd-12d","type":"parent-child","created_at":"2026-01-27T16:37:48.338942923Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.4","depends_on_id":"bd-12d.3","type":"blocks","created_at":"2026-01-27T16:38:26.583540887Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.4","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:09.248828155Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12d.5","title":"Implement natural language query interface","description":"## Overview\nBuild a natural language interface that allows users to query vibe_cockpit data using plain English instead of SQL or specific commands.\n\n## Background\nNot all users are comfortable with SQL or CLI commands. A natural language interface democratizes access to monitoring data by translating questions like 'Which agent used the most tokens today?' into appropriate queries.\n\n## Architecture\n1. User asks question in natural language\n2. LLM translates to SQL or internal query\n3. Query executed against DuckDB\n4. Results formatted and returned\n5. Optional: explain the query that was run\n\n## Query Translation\n- Parse user intent from natural language\n- Map entities (agents, accounts, machines) to tables\n- Generate appropriate SQL or API calls\n- Handle ambiguity with clarifying questions\n\n## Example Queries\n- 'Which agent used the most tokens today?' -> SELECT query on caut_usage\n- 'Are any machines offline?' -> SELECT query on machines table\n- 'Show me the rate limit status' -> Query caam_accounts\n- 'What errors happened in the last hour?' -> Query dcg_events\n- 'How is rch cache performance?' -> Query rch_compilations with aggregation\n\n## Safety Considerations\n- Only allow SELECT queries (no mutations)\n- Validate generated SQL before execution\n- Rate limit queries to prevent abuse\n- Log all NL queries for audit\n\n## Implementation Components\n- Intent classifier (what type of query?)\n- Entity extractor (which tables/columns?)\n- SQL generator (produce valid DuckDB SQL)\n- Result formatter (human-readable output)\n- Ambiguity resolver (ask clarifying questions)\n\n## Acceptance Criteria\n- [ ] NL questions translated to SQL\n- [ ] Common monitoring questions handled\n- [ ] Entity recognition for agents, machines, accounts\n- [ ] Time expressions parsed (today, last hour, etc.)\n- [ ] Aggregations supported (most, average, count)\n- [ ] Query explanation available\n- [ ] Read-only safety enforced\n- [ ] Graceful handling of ambiguous queries\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:38:03.204046902Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:10.377063524Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-12d.5","depends_on_id":"bd-12d","type":"parent-child","created_at":"2026-01-27T16:38:03.204046902Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.5","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:38:26.772143921Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.5","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:10.376548113Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.5","depends_on_id":"bd-3e2.2","type":"blocks","created_at":"2026-01-27T17:00:08.631659434Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.5","depends_on_id":"bd-3e2.4","type":"blocks","created_at":"2026-01-27T17:00:07.690442116Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12d.6","title":"Implement MCP server for external access (vc_mcp crate)","description":"## Overview\nBuild an MCP (Model Context Protocol) server that exposes vibe_cockpit data and actions to external AI agents.\n\n## Background\nOther AI agents may need to query vc data or trigger actions. An MCP server provides a standardized interface for external access, enabling agent-to-agent coordination through vibe_cockpit.\n\n## MCP Tools to Expose\n\n### Query Tools\n- vc_query_machines: List machines with optional filters\n- vc_query_accounts: Get account status and usage\n- vc_query_alerts: List active and recent alerts\n- vc_query_sessions: Search session history\n- vc_query_metrics: Get specific metric values\n- vc_query_nl: Natural language query interface\n\n### Action Tools\n- vc_acknowledge_alert: Acknowledge an alert\n- vc_approve_playbook: Approve pending Guardian playbook\n- vc_switch_account: Trigger account switch\n- vc_probe_machine: Probe a machine for tools\n- vc_refresh_collector: Force a collector to run\n\n### Resource Tools\n- vc_get_dashboard: Get current dashboard state\n- vc_get_machine_detail: Get detailed machine info\n- vc_get_agent_dna: Get agent DNA fingerprint\n\n## Implementation\n- Use MCP SDK for server implementation\n- Connect to VcStore for data access\n- Validate permissions for actions\n- Rate limit external requests\n- Full audit logging\n\n## Security Considerations\n- Authentication required for action tools\n- Query tools may be read-only accessible\n- Rate limiting per client\n- Audit log of all MCP calls\n- Configurable access control\n\n## Configuration\nAllow enabling/disabling specific tools:\n[mcp]\nenabled = true\nport = 3002\nallowed_tools = [query_*, vc_acknowledge_alert]\nrequire_auth = true\n\n## Acceptance Criteria\n- [ ] MCP server runs on configurable port\n- [ ] Query tools return appropriate data\n- [ ] Action tools execute with proper validation\n- [ ] Authentication enforced for sensitive actions\n- [ ] Rate limiting prevents abuse\n- [ ] Audit log captures all requests\n- [ ] Error responses follow MCP spec\n- [ ] Health check endpoint available\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:38:18.587943890Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:11.350821119Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-12d.6","depends_on_id":"bd-12d","type":"parent-child","created_at":"2026-01-27T16:38:18.587943890Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.6","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:11.350116852Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12d.6","depends_on_id":"bd-3e2.2","type":"blocks","created_at":"2026-01-27T16:38:26.963797321Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-13o","title":"Implement TUI Events screen","description":"Implement the Events TUI screen showing dcg denies, rano anomalies, and pt findings.\n\nSCREEN CONTENT:\n- DCG denies: Recent blocked commands by machine, rule ID, severity\n- RANO anomalies: Unusual network activity, unknown providers\n- PT findings: Process triage results, stuck/zombie processes\n\nLAYOUT:\n+------------------------------------------------+\n| EVENTS                          [f]ilter [/]search |\n+------------------------------------------------+\n| DCG DENIES (last 24h)                           |\n| ├─ orko: 12 denies (3 critical)                 |\n| │   └─ rm -rf / (critical) [12:34]              |\n| │   └─ git reset --hard (high) [11:22]          |\n| ├─ sydneymc: 5 denies (0 critical)              |\n+------------------------------------------------+\n| NETWORK ANOMALIES                               |\n| ├─ Unknown provider: suspicious.domain.com      |\n| │   └─ PID 1234 (node) -> 12 connections        |\n| ├─ Auth loop detected: claude.api (rate limit?) |\n+------------------------------------------------+\n| PROCESS ISSUES                                  |\n| ├─ Zombie processes: 3 on orko                  |\n| ├─ Stuck agents: cc_5 (velocity: 0 for 15min)   |\n| ├─ Runaway: cargo build (CPU 95% for 30min)     |\n+------------------------------------------------+\n\nNAVIGATION:\n- Tab between sections (DCG, Network, Processes)\n- Enter to drill into specific event\n- f to filter by machine/severity\n- / to search event content\n\nDATA SOURCES:\n- dcg_events table for DCG denies\n- rano_events table for network anomalies\n- process_triage_findings table for PT findings\n\nIMPLEMENTATION:\n- Create events.rs in vc_tui/src/screens/\n- Use ratatui List and Table widgets\n- Color-code by severity (critical=red, high=orange, etc.)\n- Support sorting by time, severity, machine\n- Auto-refresh every poll cycle\n\nTESTS:\n- Snapshot tests for rendering\n- Navigation state tests\n- Filter application tests\n\n## E2E & Logging\n- Add/extend `tests/e2e/tui/test_<screen>.sh` with expect/pexpect.\n- Capture logs and screenshots on failure; emit JSON summary in tests/logs/.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:49:07.931200156Z","created_by":"ubuntu","updated_at":"2026-01-27T21:37:44.900778076Z","closed_at":"2026-01-27T21:37:44.900756535Z","close_reason":"Merged into bd-2xm.7 (comprehensive Phase 4 TUI screens bead)","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-13o","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:55:40.455732413Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-13o","depends_on_id":"bd-2xm.2","type":"blocks","created_at":"2026-01-27T16:54:00.938665292Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-13o","depends_on_id":"bd-2xm.3","type":"blocks","created_at":"2026-01-27T16:53:59.442770894Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-13o","depends_on_id":"bd-2xm.4","type":"blocks","created_at":"2026-01-27T16:54:02.548245629Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-13o","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:12.320312256Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-13o","depends_on_id":"bd-33h.3","type":"blocks","created_at":"2026-01-27T16:53:57.971138515Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-15a","title":"Implement audit trail system","description":"Implement comprehensive audit trail system.\n\nPURPOSE:\nTrack all actions taken by vc, collectors, autopilot, and users for accountability, debugging, and compliance.\n\nDATA MODEL (from plan Section 9.4):\nCREATE TABLE audit_events(\n    ts TIMESTAMP,\n    event_type TEXT,  -- collector_run, autopilot_action, user_command, guardian_action\n    actor TEXT,       -- collector name, autopilot, user, guardian\n    machine_id TEXT,\n    action TEXT,\n    result TEXT,      -- success, failure, skipped\n    details_json TEXT\n);\n\nCREATE INDEX idx_audit_ts ON audit_events(ts);\nCREATE INDEX idx_audit_type ON audit_events(event_type);\nCREATE INDEX idx_audit_machine ON audit_events(machine_id);\n\nAUDIT EVENT TYPES:\n\n1. COLLECTOR RUNS\n- Collector name, machine_id, start/end time, duration\n- Bytes parsed, rows inserted, errors/warnings\n- Cursor state changes\n\n2. AUTOPILOT EXECUTIONS\n- Exact command run\n- Machine_id\n- User confirmation state (if applicable)\n- Result exit code\n- Before/after state summary\n\n3. USER COMMANDS\n- All vc CLI invocations\n- TUI/web interactions that trigger actions\n- Parameters and results\n\n4. GUARDIAN ACTIONS\n- Playbook executions\n- Individual step results\n- Rollback events\n\nIMPLEMENTATION:\npub struct AuditEvent {\n    pub ts: DateTime<Utc>,\n    pub event_type: AuditEventType,\n    pub actor: String,\n    pub machine_id: Option<String>,\n    pub action: String,\n    pub result: AuditResult,\n    pub details: serde_json::Value,\n}\n\npub trait Auditable {\n    fn to_audit_event(&self) -> AuditEvent;\n}\n\n// Implement for all actionable types\nimpl Auditable for CollectorRun { ... }\nimpl Auditable for AutopilotAction { ... }\nimpl Auditable for GuardianStep { ... }\n\nCLI COMMANDS:\nvc audit list                        # Recent audit events\nvc audit list --type guardian        # Filter by type\nvc audit list --machine orko         # Filter by machine\nvc audit list --since 24h            # Time filter\nvc audit show <event_id>             # Full details\n\nRETENTION:\n- Keep detailed audit for 90 days\n- Aggregate to daily summaries forever\n\nTESTS:\n- Test audit event creation for each type\n- Test filtering and searching\n- Test retention/aggregation\n\n## Testing & Logging\n- Unit tests for audit event normalization, enum mapping, and JSON serialization.\n- Integration tests that verify collector runs, alert triggers, and guardian actions emit audit_events rows with correct details.\n- E2E: `tests/e2e/system/test_audit_trail.sh` exercising poll → alert → guardian → cli actions; assert row counts and field integrity.\n- Structured logs and JSON summary in `tests/logs/` including event counts by type and failure reasons.\n","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-01-27T18:19:40.772853941Z","created_by":"ubuntu","updated_at":"2026-01-28T18:08:44.865952891Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-15a","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T18:20:58.040357041Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15a","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T18:21:12.912118050Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-15a","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:13.215972257Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-17d","title":"Implement ntm (Named Tmux Manager) collector","description":"Implement collector for ntm (Named Tmux Manager) - the nerve center of agent orchestration.\n\nINTEGRATION METHOD:\n- Shell out to ntm --robot-status and ntm --robot-list for JSON output\n- Optionally parse events.jsonl if ntm provides it\n- Use CLI Snapshot pattern (stateless)\n\nDUCKDB TABLES:\nCREATE TABLE ntm_sessions_snapshot(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    session_name TEXT,\n    work_dir TEXT,\n    git_branch TEXT,\n    agent_counts_json TEXT,\n    panes_json TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, collected_at, session_name)\n);\n\nCREATE TABLE ntm_activity_snapshot(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    total_agents INTEGER,\n    by_type_json TEXT,\n    by_state_json TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, collected_at)\n);\n\nCREATE TABLE ntm_events(\n    machine_id TEXT,\n    ts TIMESTAMP,\n    event_type TEXT,\n    session_name TEXT,\n    agent_id TEXT,\n    details_json TEXT\n);\n\nCOLLECTOR IMPLEMENTATION:\n- Implement Collector trait for NtmCollector\n- Handle missing ntm binary gracefully (fail-soft)\n- Parse JSON output from ntm --robot-status\n- Extract session details: name, work_dir, branch, agent counts\n- Store raw_json for forward compatibility\n\nUI VALUE:\n- Agent fleet map by machine/repo/session\n- Real-time view of which agents are working where\n- Session health monitoring\n\nTESTS:\n- Mock ntm CLI outputs\n- Verify table population\n- Test missing binary handling\n\n## E2E & Logging\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:48:01.244715845Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:14.095284895Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-17d","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T17:00:09.458414130Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-17d","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:53:14.787355696Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-17d","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:53:13.668040633Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-17d","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:14.094557825Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-17d","depends_on_id":"bd-33h","type":"parent-child","created_at":"2026-01-27T16:55:35.991715075Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-17d","depends_on_id":"bd-3nb.4","type":"blocks","created_at":"2026-01-27T16:53:15.956694944Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1h6","title":"Implement health score calculation system","description":"Implement the health score calculation system in vc_query crate.\n\nPURPOSE:\nCompute health scores for machines and the overall fleet based on multiple factors. This is the core intelligence that powers the at a glance view.\n\nDATA MODEL (from plan):\nCREATE TABLE health_factors(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    factor_id TEXT,\n    severity TEXT,\n    score REAL,\n    details_json TEXT\n);\n\nCREATE TABLE health_summary(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    overall_score REAL,\n    worst_factor_id TEXT,\n    details_json TEXT,\n    PRIMARY KEY (machine_id, collected_at)\n);\n\nHEALTH FACTORS TO COMPUTE:\n1. System resources (CPU, memory, disk, load)\n2. Agent velocity (tokens/min, stuck detection)\n3. Rate limit headroom (time to limit)\n4. DCG denies (frequency and severity)\n5. Network anomalies (unknown providers)\n6. RCH queue depth\n7. Mail backlog (urgent unread)\n8. Repo cleanliness (dirty count)\n9. Process issues (zombies, stuck)\n10. Data freshness + drift (from collector_health/drift_events)\n\nSCORING ALGORITHM:\n- Weighted average with worst-factor penalty\n- Each factor has weight and score (0.0=critical to 1.0=healthy)\n- Apply penalty for critical factors (each critical = -0.1)\n- Penalize stale data (freshness_seconds over threshold reduces score)\n- Final score clamped to [0.0, 1.0]\n\nTHRESHOLDS (configurable):\n- Disk: less than 10% free = critical, less than 20% = warning\n- CPU: greater than 90% for 5m = critical, greater than 75% for 5m = warning\n- Load: greater than 2x cores = critical, greater than 1.5x cores = warning\n- Rate limit: less than 10min to limit = critical, less than 30min = warning\n- Agent velocity: less than 10 tok/m for 15m = critical\n- Freshness: collector stale beyond threshold reduces score\n\nINTEGRATION:\n- Called after each poll cycle\n- Results stored in health_factors and health_summary tables\n- Used by TUI overview, web dashboard, alerts, autopilot\n\nTESTS:\n- Test factor computation for each metric type\n- Test weighted scoring algorithm\n- Test threshold classification\n- Test fleet-wide aggregation\n\n## Testing & Logging\n- Unit tests for factor weights, thresholds, penalty rules, and score clamping.\n- Fixture-based tests for multi-factor scenarios (disk critical + mail backlog + dcg spikes).\n- Integration tests: insert synthetic data into DuckDB and validate health_summary outputs.\n- E2E: `tests/e2e/system/test_health_scores.sh` to verify score behavior under controlled inputs.\n- Structured logs: factors, weights, penalties, final_score, worst_factor_id.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T18:18:05.250645595Z","created_by":"ubuntu","updated_at":"2026-01-27T22:17:47.401306084Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1h6","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T18:20:56.242361306Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-2mv.3","type":"blocks","created_at":"2026-01-27T18:24:30.627693088Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-2mv.4","type":"blocks","created_at":"2026-01-27T18:24:31.206709900Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-2mv.5","type":"blocks","created_at":"2026-01-27T18:24:31.771209960Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-2xm.1","type":"blocks","created_at":"2026-01-27T18:24:32.258717062Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-2xm.2","type":"blocks","created_at":"2026-01-27T18:24:32.773435904Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-2xm.3","type":"blocks","created_at":"2026-01-27T18:24:33.244729476Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-2xm.4","type":"blocks","created_at":"2026-01-27T18:24:33.766218869Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-2yb","type":"blocks","created_at":"2026-01-27T18:24:29.591390376Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:14.850381390Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-33h","type":"parent-child","created_at":"2026-01-27T18:21:11.497843499Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-33h.1","type":"blocks","created_at":"2026-01-27T18:20:56.677365550Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-33h.2","type":"blocks","created_at":"2026-01-27T18:24:30.073301481Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1h6","depends_on_id":"bd-new.1","type":"blocks","created_at":"2026-01-27T21:40:32.251467395Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ly","title":"Implement fleet orchestration commands","description":"Implement fleet orchestration commands for advanced agent management.\n\nPURPOSE:\nProvide high-level commands for scaling, load balancing, migration, and emergency operations across the agent fleet.\n\nFLEET COMMANDS (from plan Section 6.3):\n\n1. SPAWN AGENTS\nvc fleet spawn --type claude --count 5 --machine gpu-box --model opus\n- Spawns new agent instances on target machine\n- Validates machine capacity before spawning\n- Respects rate limits and quotas\n\n2. REBALANCE FLEET\nvc fleet rebalance --strategy even-load --preserve-locality\nStrategies:\n- even-load: Distribute by CPU/memory\n- even-agents: Equal agent counts\n- cost-optimized: Prefer cheaper machines\n- latency-optimized: Keep agents near their repos\n\n3. EMERGENCY STOP\nvc fleet emergency-stop --scope machine:orko --reason \"maintenance\"\nScopes: machine, provider, all, agent-type\n- Graceful shutdown with state preservation\n- Requires confirmation unless --force\n- Logs to audit trail\n\n4. MIGRATE WORKLOAD\nvc fleet migrate --from orko --to sydneymc --workload \"repo:smartedgar*\"\n- Moves active workloads between machines\n- Supports glob patterns for workload selection\n- Tracks migration progress\n\n5. CANARY DEPLOY\nvc fleet canary --config new_prompt_v2.toml --percentage 10 --auto-rollback\n- Deploy new config to subset of agents\n- Monitor for regressions\n- Auto-rollback if metrics degrade\n\nDATA MODEL:\nCREATE TABLE fleet_commands(\n    command_id TEXT PRIMARY KEY,\n    command_type TEXT NOT NULL,\n    params_json TEXT NOT NULL,\n    status TEXT NOT NULL,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    result_json TEXT,\n    error_message TEXT\n);\n\nCREATE TABLE fleet_migrations(\n    migration_id TEXT PRIMARY KEY,\n    from_machine TEXT,\n    to_machine TEXT,\n    workload_spec TEXT,\n    status TEXT,\n    agents_migrated INTEGER,\n    agents_total INTEGER,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP\n);\n\nIMPLEMENTATION:\n- vc_guardian crate handles execution\n- Integrate with ntm for agent spawning/stopping\n- Validate machine capabilities before operations\n- Full audit logging\n\nTESTS:\n- Test dry-run mode for all commands\n- Test emergency stop with mock agents\n- Test rebalance strategy calculations\n- Test canary rollback triggers\n\n## Testing & Logging\n- Unit tests for command validation, capacity checks, and safety confirmations (including --dry-run).\n- Integration tests with SSH runner mocks and audit trail verification.\n- E2E: `tests/e2e/system/test_fleet_commands.sh` using localhost/mocks; verify no destructive actions without explicit flags; JSON summary + audit events.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T18:19:05.444169885Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:15.385971862Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1ly","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T18:24:36.239045498Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ly","depends_on_id":"bd-2xm.6","type":"blocks","created_at":"2026-01-27T18:20:57.559130126Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ly","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:15.385784589Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ly","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T18:21:12.418663698Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ly","depends_on_id":"bd-3nb.1","type":"blocks","created_at":"2026-01-27T18:24:34.394450632Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ly","depends_on_id":"bd-3nb.2","type":"blocks","created_at":"2026-01-27T18:24:35.103642199Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ly","depends_on_id":"bd-3nb.3","type":"blocks","created_at":"2026-01-27T18:24:35.690972470Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1x9","title":"Create unit test infrastructure","description":"Implement comprehensive unit test infrastructure for all vc crates:\n\nTEST FRAMEWORK:\n- Use cargo test with proptest for property-based testing\n- Use mockall for mocking external dependencies (SSH, DuckDB)\n- Coverage target: >80% for core logic\n\nPER-CRATE TESTS:\n- vc_config: TOML parsing, defaults, env overrides, secret validation\n- vc_collect: Collector trait, all 5 ingestion patterns, cursors, timeouts, error recovery\n- vc_store: Migrations, table creation, insert/query round-trips, retention\n- vc_query: Health scores, rollups, anomaly detection, time-travel\n- vc_oracle: Forecasting algorithms, confidence calculations\n- vc_guardian: Healing protocol state machines, trigger evaluation\n- vc_alert: Rule evaluation, severity classification, deduplication\n- vc_tui: Widget rendering (snapshot tests), navigation states\n- vc_cli: Argument parsing, robot JSON schema compliance\n- vc_mcp: Tool registration, request/response serialization\n\nTEST UTILITIES:\n- tests/common/mod.rs with shared fixtures\n- Test DuckDB instances with known data\n- Mock SSH executor for remote tests\n- Mock collector outputs\n\nLOGGING:\n- All tests use tracing with test-friendly formatting\n- Failed tests output detailed context\n\n## E2E Note\n- End-to-end coverage is defined in bd-30z; unit-test infra should emit logs compatible with e2e JSON summaries.\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-27T16:47:29.607404605Z","created_by":"ubuntu","updated_at":"2026-01-28T08:07:22.965512938Z","closed_at":"2026-01-28T08:07:22.965472532Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1x9","depends_on_id":"bd-2a9","type":"parent-child","created_at":"2026-01-27T16:55:34.674508102Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1x9","depends_on_id":"bd-2a9.1","type":"blocks","created_at":"2026-01-27T16:52:48.786271391Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-264","title":"Implement retention policy and vc vacuum command","description":"Implement retention policy system and vc vacuum command.\n\nPURPOSE:\nManage data lifecycle to prevent unbounded storage growth while preserving critical history.\n\nRETENTION POLICY (from plan Section 5.3):\n| Data Type | Full Resolution | Aggregates |\n|-----------|-----------------|------------|\n| System samples | 7-30 days | Forever |\n| Event logs (dcg, rano) | 90 days raw | Forever |\n| Snapshots | 30 days | Forever |\n| Derived rollups | N/A | Forever |\n\nDUCKDB SCHEMA:\nCREATE TABLE retention_policies(\n    policy_id TEXT PRIMARY KEY,\n    table_name TEXT NOT NULL,\n    retention_days INTEGER NOT NULL,\n    aggregate_table TEXT,\n    enabled BOOLEAN DEFAULT TRUE,\n    last_vacuum_at TIMESTAMP\n);\n\nCREATE TABLE retention_log(\n    ts TIMESTAMP,\n    policy_id TEXT,\n    table_name TEXT,\n    rows_deleted BIGINT,\n    rows_aggregated BIGINT,\n    duration_ms BIGINT\n);\n\nCLI COMMANDS:\nvc vacuum                    # Run all retention policies\nvc vacuum --dry-run          # Show what would be deleted\nvc vacuum --table sys_samples  # Run for specific table\nvc retention list            # Show all policies\nvc retention set --table sys_samples --days 14\n\nIMPLEMENTATION:\n1. Before deleting: aggregate data to rollup tables\n2. Use transactions for safety\n3. Log all deletions to retention_log\n4. Respect no deletion discipline - make retention OPT-IN and transparent\n5. Support Parquet export before truncation\n\nAGGREGATION EXAMPLES:\n- sys_samples: aggregate to hourly/daily averages\n- dcg_events: aggregate to daily counts by severity\n- repo_status_snapshots: keep only latest per day\n\nSAFETY:\n- Never delete without explicit user configuration\n- Show preview before deletion\n- Maintain audit trail\n- Support recovery via Parquet archives\n\nTESTS:\n- Test dry-run mode\n- Test aggregation correctness\n- Test retention_log entries\n- Test Parquet export/archive\n\n## Testing & Logging\n- Unit tests for policy parsing, retention math, and dry-run output formatting.\n- Integration tests with temporary DuckDB: aggregate-then-delete workflow, rollback on failure.\n- E2E: `tests/e2e/system/test_retention_vacuum.sh` verifying opt-in behavior, log entries, and Parquet export; JSON summary in tests/logs/.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T18:18:33.673926567Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:15.977352336Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-264","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T18:20:57.116046013Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-264","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T18:24:36.813252073Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-264","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:15.977228202Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-264","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T18:21:11.950269169Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2a9","title":"Phase 0: Foundation - Core Infrastructure","description":"# Phase 0: Foundation - Core Infrastructure\n\n## Purpose\nEstablish the foundational infrastructure for vibe_cockpit: the Cargo workspace, configuration system, database layer, and basic CLI. This phase creates the scaffolding upon which all other features are built.\n\n## Key Deliverables\n1. **Cargo Workspace**: Multi-crate workspace with proper organization\n2. **Config System**: TOML-based configuration with validation\n3. **DuckDB Layer**: Schema migrations, connection pooling, query helpers\n4. **Collector Trait**: The standardized interface for all data collectors\n5. **Basic CLI**: clap-based CLI with `vc robot health` stub\n\n## Success Criteria\n- `cargo build --workspace` succeeds\n- `vc --version` shows build info\n- `vc robot health --json` returns valid JSON envelope\n- DuckDB file created with migration tracking\n- Config file parsing works with sensible defaults\n\n## Technical Context\n\n### Why Cargo Workspace?\nThe 12-crate architecture (vc_config, vc_collect, vc_store, etc.) keeps concerns separated:\n- Faster incremental builds\n- Clear dependency boundaries\n- Easier testing of individual components\n- Future-proofed for selective compilation\n\n### Why DuckDB?\nDuckDB is the analytics backbone because:\n- Embedded (no server process)\n- Columnar storage ideal for time-series analytics\n- SQL interface familiar to operators\n- Easy export to Parquet/JSON for external tools\n- OLAP optimized for the read-heavy query patterns we need\n\n### Why Pull-First Architecture?\nStarting with pull-only SSH-based collection:\n- Zero deployment footprint on remote machines\n- Simpler security model (only SSH access needed)\n- Easier debugging (all logic centralized)\n- Push can be added later for scale\n\n## Dependencies\nNone - this is the foundation that everything else depends on.\n\n## Estimated Scope\n- ~1-2 weeks of focused work\n- Creates the patterns that all subsequent phases follow\n\n## Testing & E2E Expectations\n- Every deliverable in this phase must add unit tests per bd-1x9.\n- Extend bd-30z with phase-specific e2e scenarios and structured logs.\n- Phase completion requires `cargo test --workspace` + phase e2e suite with JSON summaries in tests/logs/.\n","status":"closed","priority":0,"issue_type":"epic","created_at":"2026-01-27T16:17:02.537316217Z","created_by":"ubuntu","updated_at":"2026-01-28T08:07:44.568768524Z","closed_at":"2026-01-28T08:07:44.568718740Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2a9.1","title":"Create Cargo workspace with 12-crate architecture","description":"# Task: Create Cargo workspace with 12-crate architecture\n\n## What to Build\nInitialize a Cargo workspace in /data/projects/vibe_cockpit with 12 internal crates following the architecture spec.\n\n## Crate Structure\n```\nvibe_cockpit/\n├── Cargo.toml (workspace)\n├── vc.toml (runtime config, created later)\n├── crates/\n│   ├── vc_config/       # Config parsing + validation, secrets/paths\n│   ├── vc_collect/      # Collectors (one per upstream tool + system collectors)\n│   ├── vc_store/        # DuckDB schema migrations + ingestion helpers + query library\n│   ├── vc_query/        # Canonical queries (health, rollups, leaderboards, anomalies)\n│   ├── vc_oracle/       # Prediction engine, pattern recognition, forecasting\n│   ├── vc_guardian/     # Self-healing protocols, fleet orchestration\n│   ├── vc_knowledge/    # Solution mining, gotcha database, playbooks\n│   ├── vc_alert/        # Rule engine + notifications + autopilot hooks\n│   ├── vc_tui/          # ratatui UI + navigation + charts\n│   ├── vc_web/          # axum server + embedded static web assets + JSON API\n│   ├── vc_cli/          # clap commands; robot mode output formatting\n│   └── vc_mcp/          # MCP server mode for agent queries\n└── src/\n    └── main.rs          # Binary entry point\n```\n\n## Workspace Cargo.toml Template\n```toml\n[workspace]\nresolver = \"2\"\nmembers = [\n    \"crates/vc_config\",\n    \"crates/vc_collect\",\n    \"crates/vc_store\",\n    \"crates/vc_query\",\n    \"crates/vc_oracle\",\n    \"crates/vc_guardian\",\n    \"crates/vc_knowledge\",\n    \"crates/vc_alert\",\n    \"crates/vc_tui\",\n    \"crates/vc_web\",\n    \"crates/vc_cli\",\n    \"crates/vc_mcp\",\n]\n\n[workspace.package]\nversion = \"0.1.0\"\nedition = \"2024\"\nauthors = [\"vibe_cockpit contributors\"]\nlicense = \"MIT\"\nrepository = \"https://github.com/Dicklesworthstone/vibe_cockpit\"\n\n[workspace.dependencies]\n# Async runtime\ntokio = { version = \"1.44\", features = [\"full\"] }\n\n# Database\nduckdb = { version = \"1.1\", features = [\"bundled\"] }\n\n# Serialization\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\ntoml = \"0.8\"\n\n# CLI\nclap = { version = \"4.5\", features = [\"derive\"] }\n\n# TUI\nratatui = \"0.29\"\ncrossterm = \"0.28\"\n\n# Web\naxum = \"0.7\"\ntower = \"0.5\"\ntower-http = { version = \"0.6\", features = [\"cors\", \"fs\", \"trace\"] }\n\n# SSH (for remote execution)\n# russh = \"0.45\"  # Consider later; MVP uses ssh binary\n\n# Time\nchrono = { version = \"0.4\", features = [\"serde\"] }\n\n# Error handling\nthiserror = \"2.0\"\nanyhow = \"1.0\"\n\n# Tracing\ntracing = \"0.1\"\ntracing-subscriber = { version = \"0.3\", features = [\"env-filter\", \"json\"] }\n\n# Other utilities\nuuid = { version = \"1.11\", features = [\"v4\", \"serde\"] }\nregex = \"1.11\"\n```\n\n## Each Crate's Initial Cargo.toml\nEach crate should:\n- Use `[package]` with `version.workspace = true`, `edition.workspace = true`\n- Declare minimal dependencies from workspace\n- Export a `lib.rs` with placeholder types\n\n## Acceptance Criteria\n- [ ] `cargo build --workspace` succeeds\n- [ ] Each crate compiles independently\n- [ ] No circular dependencies between crates\n- [ ] Binary produces help output: `cargo run -- --help`\n\n## Technical Rationale\nThe 12-crate split keeps concerns separated:\n- **vc_config**: Pure data structures, no I/O (enables testing)\n- **vc_collect**: Plugin-like collectors, each can evolve independently\n- **vc_store**: Single point of DuckDB access (connection pooling, migrations)\n- **vc_query**: Reusable query library (shared by TUI, web, CLI)\n- **vc_oracle/guardian/knowledge**: Intelligence layers can be developed in parallel\n- **vc_alert**: Separate from oracle to allow simpler rule-based alerts\n- **vc_tui/web/cli**: Three UI surfaces share vc_query but have different dependencies\n- **vc_mcp**: Optional feature, can be compiled out if not needed\n\n## Notes for Future Self\n- Start with stub implementations that compile\n- Don't over-engineer initial versions - just get the structure right\n- Cross-crate dependencies will be refined as features are built\n\n## E2E & Logging\n- Add scenario scripts under `tests/e2e/<area>/` with structured logs and JSON summaries.\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-27T16:19:31.271113660Z","created_by":"ubuntu","updated_at":"2026-01-27T19:57:58.357259227Z","closed_at":"2026-01-27T19:57:58.357120486Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2a9.1","depends_on_id":"bd-2a9","type":"parent-child","created_at":"2026-01-27T16:19:31.271113660Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2a9.2","title":"Implement vc_config crate with TOML parsing","description":"# Task: Implement vc_config crate with TOML parsing\n\n## What to Build\nThe configuration system for vibe_cockpit, reading from `vc.toml` with sensible defaults and validation.\n\n## Config Structure\n```rust\n// crates/vc_config/src/lib.rs\n\nuse std::path::PathBuf;\nuse std::time::Duration;\nuse serde::{Deserialize, Serialize};\n\n#[derive(Debug, Clone, Deserialize, Serialize)]\npub struct VcConfig {\n    pub vc: VcCore,\n    pub polling: PollingConfig,\n    #[serde(default)]\n    pub machines: Vec<MachineConfig>,\n    #[serde(default)]\n    pub autopilot: AutopilotConfig,\n}\n\n#[derive(Debug, Clone, Deserialize, Serialize)]\npub struct VcCore {\n    /// Path to DuckDB database file\n    #[serde(default = \"default_db_path\")]\n    pub db_path: PathBuf,\n    \n    /// Data directory for artifacts\n    #[serde(default = \"default_data_dir\")]\n    pub data_dir: PathBuf,\n    \n    /// Log level (trace, debug, info, warn, error)\n    #[serde(default = \"default_log_level\")]\n    pub log_level: String,\n}\n\n#[derive(Debug, Clone, Deserialize, Serialize)]\npub struct PollingConfig {\n    /// Default interval between collection runs (seconds)\n    #[serde(default = \"default_interval\")]\n    pub default_interval_seconds: u64,\n    \n    /// Maximum machines to poll concurrently\n    #[serde(default = \"default_max_machines\")]\n    pub max_machines_in_flight: u32,\n    \n    /// Maximum collectors per machine concurrently\n    #[serde(default = \"default_max_collectors\")]\n    pub max_collectors_in_flight_per_machine: u32,\n}\n\n#[derive(Debug, Clone, Deserialize, Serialize)]\npub struct MachineConfig {\n    /// Unique identifier for this machine\n    pub id: String,\n    \n    /// SSH target: \"local\" for local execution, or \"user@host:port\"\n    pub ssh: String,\n    \n    /// Tags for filtering and grouping\n    #[serde(default)]\n    pub tags: Vec<String>,\n    \n    /// Allow write commands on this machine\n    #[serde(default)]\n    pub allow_write_commands: bool,\n    \n    /// Allow autopilot execution on this machine\n    #[serde(default)]\n    pub allow_autopilot: bool,\n}\n\n#[derive(Debug, Clone, Deserialize, Serialize, Default)]\npub struct AutopilotConfig {\n    /// off | suggest | execute-safe | execute-with-approval\n    #[serde(default = \"default_autopilot_mode\")]\n    pub mode: String,\n    \n    /// Commands allowed in execute-safe mode\n    #[serde(default)]\n    pub safe_commands: SafeCommands,\n}\n\n#[derive(Debug, Clone, Deserialize, Serialize, Default)]\npub struct SafeCommands {\n    #[serde(default)]\n    pub allow: Vec<String>,\n}\n\n// Default functions\nfn default_db_path() -> PathBuf {\n    dirs::data_local_dir()\n        .unwrap_or_else(|| PathBuf::from(\".\"))\n        .join(\"vc\")\n        .join(\"vc.duckdb\")\n}\n\nfn default_data_dir() -> PathBuf {\n    dirs::data_local_dir()\n        .unwrap_or_else(|| PathBuf::from(\".\"))\n        .join(\"vc\")\n}\n\nfn default_log_level() -> String { \"info\".to_string() }\nfn default_interval() -> u64 { 120 }\nfn default_max_machines() -> u32 { 8 }\nfn default_max_collectors() -> u32 { 4 }\nfn default_autopilot_mode() -> String { \"suggest\".to_string() }\n```\n\n## Config Loading Logic\n```rust\nimpl VcConfig {\n    /// Load config from default paths, with precedence:\n    /// 1. ./vc.toml (project-local)\n    /// 2. ~/.config/vc/vc.toml (user)\n    /// 3. /etc/vc/vc.toml (system)\n    /// 4. Built-in defaults\n    pub fn load() -> Result<Self, ConfigError> {\n        let paths = [\n            PathBuf::from(\"vc.toml\"),\n            dirs::config_dir().map(|d| d.join(\"vc/vc.toml\")),\n            Some(PathBuf::from(\"/etc/vc/vc.toml\")),\n        ];\n        \n        for path in paths.into_iter().flatten() {\n            if path.exists() {\n                return Self::load_from(&path);\n            }\n        }\n        \n        // Return defaults if no config file found\n        Ok(Self::default())\n    }\n    \n    pub fn load_from(path: &Path) -> Result<Self, ConfigError> {\n        let content = std::fs::read_to_string(path)?;\n        let config: Self = toml::from_str(&content)?;\n        config.validate()?;\n        Ok(config)\n    }\n    \n    pub fn validate(&self) -> Result<(), ConfigError> {\n        // Validate machine IDs are unique\n        // Validate SSH targets are well-formed\n        // Validate autopilot mode is recognized\n        // etc.\n    }\n}\n```\n\n## Example vc.toml\n```toml\n[vc]\ndb_path = \"~/.local/share/vc/vc.duckdb\"\ndata_dir = \"~/.local/share/vc\"\nlog_level = \"info\"\n\n[polling]\ndefault_interval_seconds = 120\nmax_machines_in_flight = 8\nmax_collectors_in_flight_per_machine = 4\n\n[[machines]]\nid = \"orko\"\nssh = \"local\"\ntags = [\"primary\", \"claude\", \"codex\"]\nallow_write_commands = true\nallow_autopilot = true\n\n[[machines]]\nid = \"sydneymc\"\nssh = \"ubuntu@sydneymc.internal:22\"\ntags = [\"worker\", \"rch\", \"claude\"]\n\n[[machines]]\nid = \"mac-mini\"\nssh = \"admin@mac-mini.local:22\"\ntags = [\"gemini\", \"backup\"]\n\n[autopilot]\nmode = \"suggest\"\n\n[autopilot.safe_commands]\nallow = [\n    \"caam recommend\",\n    \"caam limits\",\n    \"pt robot plan\",\n]\n```\n\n## Dependencies\nThis crate needs:\n- serde + serde_derive (serialization)\n- toml (TOML parsing)\n- thiserror (error types)\n- dirs (XDG paths)\n\n## Acceptance Criteria\n- [ ] Config loads from file with validation\n- [ ] Missing config uses sensible defaults\n- [ ] Path expansion works (~/ becomes home dir)\n- [ ] Validation errors have clear messages\n- [ ] Unit tests for all config scenarios\n\n## Notes for Future Self\n- Keep config flat initially; avoid deep nesting\n- Add config versioning later if schema changes significantly\n- Consider config reload at runtime (SIGHUP handler)\n\n## E2E & Logging\n- Add scenario scripts under `tests/e2e/<area>/` with structured logs and JSON summaries.\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-27T16:19:57.299354823Z","created_by":"ubuntu","updated_at":"2026-01-28T04:28:11.868163417Z","closed_at":"2026-01-28T04:28:11.868128010Z","close_reason":"Completed: all acceptance criteria met, 15 tests pass","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2a9.2","depends_on_id":"bd-2a9","type":"parent-child","created_at":"2026-01-27T16:19:57.299354823Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2a9.2","depends_on_id":"bd-2a9.1","type":"blocks","created_at":"2026-01-27T16:21:43.720365241Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2a9.3","title":"Implement vc_store crate with DuckDB + migrations","description":"# Task: Implement vc_store crate with DuckDB + migrations\n\n## What to Build\nThe DuckDB storage layer with schema migrations, connection management, and ingestion helpers.\n\n## Core Components\n\n### 1. Database Connection Pool\n```rust\n// crates/vc_store/src/lib.rs\n\nuse duckdb::{Connection, Config};\nuse std::sync::Arc;\nuse tokio::sync::Mutex;\n\npub struct Store {\n    conn: Arc<Mutex<Connection>>,\n    config: StoreConfig,\n}\n\npub struct StoreConfig {\n    pub db_path: PathBuf,\n    pub read_only: bool,\n}\n\nimpl Store {\n    pub async fn open(config: StoreConfig) -> Result<Self, StoreError> {\n        let conn = Connection::open_with_flags(\n            &config.db_path,\n            if config.read_only {\n                Config::default().read_only()?\n            } else {\n                Config::default()\n            },\n        )?;\n        \n        let store = Self {\n            conn: Arc::new(Mutex::new(conn)),\n            config,\n        };\n        \n        if !config.read_only {\n            store.run_migrations().await?;\n        }\n        \n        Ok(store)\n    }\n    \n    pub async fn run_migrations(&self) -> Result<(), StoreError> {\n        let conn = self.conn.lock().await;\n        migrations::run_all(&conn)?;\n        Ok(())\n    }\n}\n```\n\n### 2. Migration Framework\n```rust\n// crates/vc_store/src/migrations.rs\n\npub struct Migration {\n    pub version: u32,\n    pub description: &'static str,\n    pub up: &'static str,\n    pub checksum: &'static str,\n}\n\npub static MIGRATIONS: &[Migration] = &[\n    Migration {\n        version: 1,\n        description: \"Initial schema - core tables\",\n        up: include_str!(\"../migrations/001_initial.sql\"),\n        checksum: \"...\",\n    },\n    Migration {\n        version: 2,\n        description: \"Add system metrics tables\",\n        up: include_str!(\"../migrations/002_sys_metrics.sql\"),\n        checksum: \"...\",\n    },\n    // ... more migrations\n];\n\npub fn run_all(conn: &Connection) -> Result<(), MigrationError> {\n    // Create migrations table if not exists\n    conn.execute(r#\"\n        CREATE TABLE IF NOT EXISTS schema_migrations(\n            version INTEGER PRIMARY KEY,\n            applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n            description TEXT,\n            checksum TEXT\n        )\n    \"#, [])?;\n    \n    let applied: HashSet<u32> = conn\n        .prepare(\"SELECT version FROM schema_migrations\")?\n        .query_map([], |row| row.get(0))?\n        .collect::<Result<_, _>>()?;\n    \n    for migration in MIGRATIONS {\n        if !applied.contains(&migration.version) {\n            tracing::info!(version = migration.version, desc = migration.description, \"Running migration\");\n            conn.execute_batch(migration.up)?;\n            conn.execute(\n                \"INSERT INTO schema_migrations (version, description, checksum) VALUES (?, ?, ?)\",\n                [&migration.version as &dyn ToSql, &migration.description, &migration.checksum],\n            )?;\n        }\n    }\n    \n    Ok(())\n}\n```\n\n### 3. Initial Schema (migrations/001_initial.sql)\n```sql\n-- Machines\nCREATE TABLE machines(\n    machine_id TEXT PRIMARY KEY,\n    tags_json TEXT,\n    ssh_target TEXT,\n    first_seen_at TIMESTAMP,\n    last_seen_at TIMESTAMP\n);\n\nCREATE TABLE machine_tool_capabilities(\n    machine_id TEXT,\n    tool_name TEXT,\n    available BOOLEAN,\n    version TEXT,\n    checked_at TIMESTAMP,\n    PRIMARY KEY (machine_id, tool_name)\n);\n\n-- Ingestion State\nCREATE TABLE ingestion_cursors(\n    machine_id TEXT,\n    source TEXT,\n    cursor_key TEXT,\n    cursor_value TEXT,\n    updated_at TIMESTAMP,\n    PRIMARY KEY (machine_id, source, cursor_key)\n);\n\n-- Audit Log\nCREATE TABLE audit_events(\n    ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    event_type TEXT,\n    actor TEXT,\n    machine_id TEXT,\n    action TEXT,\n    result TEXT,\n    details_json TEXT\n);\n```\n\n### 4. Ingestion Helpers\n```rust\n// crates/vc_store/src/ingest.rs\n\n/// Batch insert with conflict handling\npub trait Ingestable {\n    fn table_name() -> &'static str;\n    fn columns() -> &'static [&'static str];\n    fn values(&self) -> Vec<Box<dyn ToSql>>;\n}\n\nimpl Store {\n    pub async fn ingest<T: Ingestable>(&self, rows: &[T]) -> Result<usize, StoreError> {\n        if rows.is_empty() {\n            return Ok(0);\n        }\n        \n        let conn = self.conn.lock().await;\n        let table = T::table_name();\n        let cols = T::columns().join(\", \");\n        let placeholders = T::columns().iter().map(|_| \"?\").collect::<Vec<_>>().join(\", \");\n        \n        let sql = format!(\n            \"INSERT OR REPLACE INTO {} ({}) VALUES ({})\",\n            table, cols, placeholders\n        );\n        \n        let mut stmt = conn.prepare(&sql)?;\n        let mut count = 0;\n        for row in rows {\n            stmt.execute(row.values().as_slice())?;\n            count += 1;\n        }\n        \n        Ok(count)\n    }\n    \n    /// Get cursor for incremental collection\n    pub async fn get_cursor(&self, machine_id: &str, source: &str, key: &str) -> Result<Option<String>, StoreError> {\n        let conn = self.conn.lock().await;\n        let result = conn.query_row(\n            \"SELECT cursor_value FROM ingestion_cursors WHERE machine_id = ? AND source = ? AND cursor_key = ?\",\n            [machine_id, source, key],\n            |row| row.get(0),\n        );\n        \n        match result {\n            Ok(v) => Ok(Some(v)),\n            Err(duckdb::Error::QueryReturnedNoRows) => Ok(None),\n            Err(e) => Err(e.into()),\n        }\n    }\n    \n    /// Update cursor after successful collection\n    pub async fn set_cursor(&self, machine_id: &str, source: &str, key: &str, value: &str) -> Result<(), StoreError> {\n        let conn = self.conn.lock().await;\n        conn.execute(\n            r#\"\n            INSERT OR REPLACE INTO ingestion_cursors (machine_id, source, cursor_key, cursor_value, updated_at)\n            VALUES (?, ?, ?, ?, CURRENT_TIMESTAMP)\n            \"#,\n            [machine_id, source, key, value],\n        )?;\n        Ok(())\n    }\n}\n```\n\n## Dependencies\n- duckdb (with bundled feature)\n- thiserror\n- tracing\n\n## Acceptance Criteria\n- [ ] Database file created on first run\n- [ ] Migrations run idempotently (can re-run safely)\n- [ ] Cursor get/set works for incremental collection\n- [ ] Batch ingestion handles conflicts correctly\n- [ ] Connection properly shared across async tasks\n\n## Notes for Future Self\n- DuckDB is single-writer; the Mutex serializes writes\n- Consider WAL mode for better read concurrency\n- Migration checksums catch accidental modifications\n- Keep migrations small and focused\n\n## Testing & Logging\n- Unit tests for validation, defaults, or timeout behavior.\n- Integration tests with temporary DuckDB/fixtures.\n- Failure-mode tests (invalid config, corrupt db, partial output).\n- E2E: add scenario scripts under `tests/e2e/<area>/` with structured logs.\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-27T16:20:23.692271897Z","created_by":"ubuntu","updated_at":"2026-01-27T20:40:55.527441068Z","closed_at":"2026-01-27T20:40:55.527287649Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2a9.3","depends_on_id":"bd-2a9","type":"parent-child","created_at":"2026-01-27T16:20:23.692271897Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2a9.3","depends_on_id":"bd-2a9.1","type":"blocks","created_at":"2026-01-27T16:21:43.829779819Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2a9.4","title":"Implement Collector trait in vc_collect crate","description":"# Task: Implement Collector trait in vc_collect crate\n\n## What to Build\nThe standardized collector interface that all upstream integrations implement. This is the heart of the data ingestion architecture.\n\n## Core Trait Definition\n```rust\n// crates/vc_collect/src/lib.rs\n\nuse async_trait::async_trait;\nuse std::time::Duration;\nuse chrono::{DateTime, Utc};\n\n/// All collectors implement this trait\n#[async_trait]\npub trait Collector: Send + Sync {\n    /// Unique identifier for this collector (e.g., \"sysmoni\", \"caut\", \"ru\")\n    fn name(&self) -> &'static str;\n    \n    /// Schema version for output normalization\n    fn schema_version(&self) -> u32;\n    \n    /// Perform collection with given context\n    async fn collect(&self, ctx: &CollectContext) -> CollectResult;\n    \n    /// Default timeout for this collector\n    fn default_timeout(&self) -> Duration {\n        Duration::from_secs(30)\n    }\n    \n    /// Whether this collector requires a specific tool on the target machine\n    fn required_tool(&self) -> Option<&'static str> {\n        None\n    }\n}\n\n/// Context provided to collectors during collection\npub struct CollectContext {\n    /// Which machine we're collecting from\n    pub machine_id: MachineId,\n    \n    /// How to execute commands (local or SSH)\n    pub executor: Box<dyn CommandExecutor>,\n    \n    /// Last cursor for incremental collection\n    pub last_cursor: Option<Cursor>,\n    \n    /// Time window for incremental queries (e.g., \"last 10 minutes\")\n    pub poll_window: Duration,\n    \n    /// Hard timeout for this collection\n    pub timeout: Duration,\n    \n    /// Maximum bytes to read from command output\n    pub max_bytes: usize,\n    \n    /// Maximum rows to insert\n    pub max_rows: usize,\n}\n\n/// Result of a collection run\npub struct CollectResult {\n    /// Normalized rows ready for DuckDB insertion\n    pub rows: Vec<RowBatch>,\n    \n    /// Updated cursor for next poll (None = stateless)\n    pub new_cursor: Option<Cursor>,\n    \n    /// Optional raw artifacts for debugging\n    pub raw_artifacts: Vec<ArtifactRef>,\n    \n    /// Warnings to surface in UI\n    pub warnings: Vec<Warning>,\n    \n    /// How long collection took\n    pub duration: Duration,\n    \n    /// Whether collection succeeded\n    pub success: bool,\n    \n    /// Error message if failed\n    pub error: Option<String>,\n}\n\n/// Cursor types for different incremental patterns\n#[derive(Debug, Clone, Serialize, Deserialize)]\npub enum Cursor {\n    /// For time-bounded queries (e.g., \"since timestamp\")\n    Timestamp(DateTime<Utc>),\n    \n    /// For JSONL tail (inode + byte offset)\n    FileOffset { inode: u64, offset: u64 },\n    \n    /// For SQLite incremental (last seen primary key)\n    PrimaryKey(i64),\n    \n    /// For custom cursor formats\n    Opaque(String),\n}\n\n/// Batch of rows for a single table\npub struct RowBatch {\n    pub table: &'static str,\n    pub rows: Vec<serde_json::Value>,\n}\n\n/// Warning to surface in UI\n#[derive(Debug)]\npub struct Warning {\n    pub level: WarningLevel,\n    pub message: String,\n    pub context: Option<String>,\n}\n\n#[derive(Debug)]\npub enum WarningLevel {\n    Info,\n    Warn,\n    Error,\n}\n```\n\n## Command Executor Trait\n```rust\n// crates/vc_collect/src/executor.rs\n\n#[async_trait]\npub trait CommandExecutor: Send + Sync {\n    /// Run a command and return stdout\n    async fn run(&self, command: &str) -> Result<String, ExecutorError>;\n    \n    /// Run a command with timeout\n    async fn run_timeout(&self, command: &str, timeout: Duration) -> Result<String, ExecutorError>;\n    \n    /// Read a file (or range of bytes)\n    async fn read_file(&self, path: &str) -> Result<Vec<u8>, ExecutorError>;\n    \n    /// Read file from offset to end (for JSONL tail)\n    async fn read_file_range(&self, path: &str, offset: u64) -> Result<Vec<u8>, ExecutorError>;\n    \n    /// Get file stat (inode, size)\n    async fn stat(&self, path: &str) -> Result<FileStat, ExecutorError>;\n    \n    /// Run SQLite query\n    async fn sqlite_query(&self, db_path: &str, query: &str) -> Result<Vec<serde_json::Value>, ExecutorError>;\n    \n    /// HTTP GET (for Prometheus scraping)\n    async fn http_get(&self, url: &str) -> Result<String, ExecutorError>;\n}\n\npub struct FileStat {\n    pub inode: u64,\n    pub size: u64,\n    pub mtime: DateTime<Utc>,\n}\n\n/// Local executor (runs commands directly)\npub struct LocalExecutor;\n\n/// SSH executor (runs commands via SSH)\npub struct SshExecutor {\n    pub target: String,  // user@host:port\n    pub timeout: Duration,\n}\n```\n\n## Dummy Collector (for testing)\n```rust\n// crates/vc_collect/src/collectors/dummy.rs\n\npub struct DummyCollector;\n\n#[async_trait]\nimpl Collector for DummyCollector {\n    fn name(&self) -> &'static str { \"dummy\" }\n    fn schema_version(&self) -> u32 { 1 }\n    \n    async fn collect(&self, ctx: &CollectContext) -> CollectResult {\n        // Just return a test row\n        CollectResult {\n            rows: vec![RowBatch {\n                table: \"dummy_snapshots\",\n                rows: vec![serde_json::json!({\n                    \"machine_id\": ctx.machine_id.as_str(),\n                    \"collected_at\": Utc::now().to_rfc3339(),\n                    \"test_value\": 42,\n                })],\n            }],\n            new_cursor: None,\n            raw_artifacts: vec![],\n            warnings: vec![],\n            duration: Duration::from_millis(1),\n            success: true,\n            error: None,\n        }\n    }\n}\n```\n\n## Collector Registry\n```rust\n// crates/vc_collect/src/registry.rs\n\npub struct CollectorRegistry {\n    collectors: HashMap<String, Arc<dyn Collector>>,\n}\n\nimpl CollectorRegistry {\n    pub fn new() -> Self {\n        let mut registry = Self { collectors: HashMap::new() };\n        \n        // Register built-in collectors\n        registry.register(Arc::new(DummyCollector));\n        // More collectors registered as they're implemented\n        \n        registry\n    }\n    \n    pub fn register(&mut self, collector: Arc<dyn Collector>) {\n        self.collectors.insert(collector.name().to_string(), collector);\n    }\n    \n    pub fn get(&self, name: &str) -> Option<Arc<dyn Collector>> {\n        self.collectors.get(name).cloned()\n    }\n    \n    pub fn all(&self) -> impl Iterator<Item = Arc<dyn Collector>> + '_ {\n        self.collectors.values().cloned()\n    }\n}\n```\n\n## Collector Design Goals\nThese are critical principles baked into the trait:\n\n1. **Idempotent inserts**: Same source payload should not create duplicates\n2. **Incremental by default**: Avoid rescanning large histories every poll\n3. **Versioned outputs**: Every collector has `schema_version` for evolution\n4. **Fail-soft**: Broken collector degrades cockpit (shows \"stale\"), doesn't crash\n5. **Timeout-bounded**: No collector can hang the system\n\n## Dependencies\n- async-trait\n- serde + serde_json\n- chrono\n- thiserror\n- tracing\n\n## Acceptance Criteria\n- [ ] Collector trait compiles with async support\n- [ ] DummyCollector implements trait correctly\n- [ ] LocalExecutor runs commands\n- [ ] CollectorRegistry manages collectors\n- [ ] Unit tests for cursor serialization\n\n## Notes for Future Self\n- Each real collector will be its own module under `collectors/`\n- The executor abstraction is key for local vs remote\n- Consider collector metrics (duration, success rate) for observability\n\n## E2E & Logging\n- Add scenario scripts under `tests/e2e/<area>/` with structured logs and JSON summaries.\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-27T16:20:59.142534309Z","created_by":"ubuntu","updated_at":"2026-01-28T04:41:25.332008506Z","closed_at":"2026-01-28T04:41:25.331846811Z","close_reason":"Implementation verified complete by FrostyRiver - all acceptance criteria met","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2a9.4","depends_on_id":"bd-2a9","type":"parent-child","created_at":"2026-01-27T16:20:59.142534309Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2a9.4","depends_on_id":"bd-2a9.1","type":"blocks","created_at":"2026-01-27T16:21:43.915792581Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2a9.5","title":"Implement vc_cli crate with clap + robot mode","description":"# Task: Implement vc_cli crate with clap + robot mode\n\n## What to Build\nThe CLI entry point using clap, with robot mode output formatting for agent consumption.\n\n## CLI Structure\n```rust\n// crates/vc_cli/src/lib.rs\n\nuse clap::{Parser, Subcommand};\n\n#[derive(Parser)]\n#[command(name = \"vc\")]\n#[command(about = \"Vibe Cockpit - Agent Fleet Command Center\")]\n#[command(version)]\npub struct Cli {\n    #[command(subcommand)]\n    pub command: Commands,\n    \n    /// Path to config file\n    #[arg(short, long, global = true)]\n    pub config: Option<PathBuf>,\n    \n    /// Increase verbosity\n    #[arg(short, long, global = true, action = clap::ArgAction::Count)]\n    pub verbose: u8,\n    \n    /// Quiet mode (errors only)\n    #[arg(short, long, global = true)]\n    pub quiet: bool,\n}\n\n#[derive(Subcommand)]\npub enum Commands {\n    /// Start the TUI dashboard\n    Ui {\n        /// Refresh interval in seconds\n        #[arg(short, long, default_value = \"5\")]\n        interval: u64,\n    },\n    \n    /// Start the web server\n    Serve {\n        /// Address to bind (default: 127.0.0.1:8080)\n        #[arg(short, long, default_value = \"127.0.0.1:8080\")]\n        bind: String,\n    },\n    \n    /// Robot mode commands (agent-friendly JSON output)\n    Robot {\n        #[command(subcommand)]\n        command: RobotCommands,\n    },\n    \n    /// Run a single poll cycle\n    Poll {\n        /// Specific machine to poll (default: all)\n        #[arg(short, long)]\n        machine: Option<String>,\n        \n        /// Specific collector to run (default: all)\n        #[arg(short, long)]\n        collector: Option<String>,\n    },\n    \n    /// Query DuckDB directly (read-only)\n    Query {\n        /// SQL query string\n        sql: String,\n        \n        /// Output format\n        #[arg(short, long, default_value = \"table\")]\n        format: OutputFormat,\n    },\n    \n    /// Watch mode (streaming events)\n    Watch {\n        /// Output format\n        #[arg(short, long, default_value = \"jsonl\")]\n        format: OutputFormat,\n        \n        /// Interval between checks (seconds)\n        #[arg(short, long, default_value = \"30\")]\n        interval: u64,\n    },\n    \n    /// Database operations\n    Db {\n        #[command(subcommand)]\n        command: DbCommands,\n    },\n    \n    /// Show version and build info\n    Version,\n}\n\n#[derive(Subcommand)]\npub enum RobotCommands {\n    /// Overall health summary\n    Health,\n    \n    /// Triage view (top issues + suggested commands)\n    Triage,\n    \n    /// Machine details\n    Machine {\n        /// Machine ID\n        id: String,\n    },\n    \n    /// Account usage summary\n    Accounts,\n    \n    /// Repository status\n    Repos,\n    \n    /// Open alerts\n    Alerts,\n    \n    /// Oracle predictions\n    Oracle,\n    \n    /// Guardian healing status\n    Guardian,\n    \n    /// Schema documentation\n    Docs {\n        #[command(subcommand)]\n        which: RobotDocs,\n    },\n}\n\n#[derive(Subcommand)]\npub enum RobotDocs {\n    /// JSON schema documentation\n    Schemas,\n    /// Example outputs\n    Examples,\n}\n\n#[derive(Subcommand)]\npub enum DbCommands {\n    /// Run pending migrations\n    Migrate,\n    /// Vacuum and optimize\n    Vacuum,\n    /// Export to Parquet\n    Export {\n        /// Output directory\n        #[arg(short, long)]\n        output: PathBuf,\n    },\n}\n\n#[derive(Clone, Copy, Debug, Default)]\npub enum OutputFormat {\n    #[default]\n    Table,\n    Json,\n    Jsonl,\n    Csv,\n}\n```\n\n## Robot Output Envelope\n```rust\n// crates/vc_cli/src/robot.rs\n\nuse serde::Serialize;\nuse chrono::{DateTime, Utc};\n\n/// Standard envelope for all robot output\n#[derive(Serialize)]\npub struct RobotEnvelope<T: Serialize> {\n    /// Schema identifier with version\n    pub schema_version: String,\n    \n    /// When this output was generated\n    pub generated_at: DateTime<Utc>,\n    \n    /// The actual data\n    pub data: T,\n    \n    /// Data staleness (seconds since last collection per source)\n    pub staleness: HashMap<String, u64>,\n    \n    /// Any warnings about data quality\n    pub warnings: Vec<String>,\n}\n\nimpl<T: Serialize> RobotEnvelope<T> {\n    pub fn new(schema: &str, data: T) -> Self {\n        Self {\n            schema_version: schema.to_string(),\n            generated_at: Utc::now(),\n            data,\n            staleness: HashMap::new(),\n            warnings: vec![],\n        }\n    }\n    \n    pub fn with_staleness(mut self, staleness: HashMap<String, u64>) -> Self {\n        self.staleness = staleness;\n        self\n    }\n    \n    pub fn with_warnings(mut self, warnings: Vec<String>) -> Self {\n        self.warnings = warnings;\n        self\n    }\n    \n    pub fn to_json(&self) -> String {\n        serde_json::to_string_pretty(self).unwrap_or_else(|e| {\n            format!(r#\"{{\"error\": \"serialization failed: {}\"}}\"#, e)\n        })\n    }\n}\n```\n\n## Robot Health Implementation (Stub)\n```rust\n// crates/vc_cli/src/commands/robot_health.rs\n\n#[derive(Serialize)]\npub struct HealthData {\n    pub overall: OverallHealth,\n    pub machines: Vec<MachineHealth>,\n}\n\n#[derive(Serialize)]\npub struct OverallHealth {\n    pub score: f64,\n    pub severity: String,  // healthy, medium, critical\n    pub active_alerts: u32,\n}\n\n#[derive(Serialize)]\npub struct MachineHealth {\n    pub id: String,\n    pub score: f64,\n    pub top_issue: Option<String>,\n    pub last_seen: DateTime<Utc>,\n}\n\npub async fn robot_health(store: &Store) -> RobotEnvelope<HealthData> {\n    // TODO: Query actual data from store\n    // For now, return stub\n    RobotEnvelope::new(\n        \"vc.robot.health.v1\",\n        HealthData {\n            overall: OverallHealth {\n                score: 1.0,\n                severity: \"healthy\".to_string(),\n                active_alerts: 0,\n            },\n            machines: vec![],\n        },\n    )\n}\n```\n\n## Main Binary\n```rust\n// src/main.rs\n\nuse vc_cli::{Cli, Commands, RobotCommands};\nuse clap::Parser;\n\n#[tokio::main]\nasync fn main() -> anyhow::Result<()> {\n    let cli = Cli::parse();\n    \n    // Setup logging based on verbosity\n    let log_level = match cli.verbose {\n        0 => \"info\",\n        1 => \"debug\",\n        _ => \"trace\",\n    };\n    tracing_subscriber::fmt()\n        .with_env_filter(log_level)\n        .init();\n    \n    // Load config\n    let config = vc_config::VcConfig::load()?;\n    \n    match cli.command {\n        Commands::Robot { command } => {\n            let store = vc_store::Store::open_readonly(&config.vc.db_path)?;\n            \n            match command {\n                RobotCommands::Health => {\n                    let output = vc_cli::commands::robot_health(&store).await;\n                    println!(\"{}\", output.to_json());\n                }\n                // ... other robot commands\n            }\n        }\n        Commands::Version => {\n            println!(\"vc {} ({})\", env!(\"CARGO_PKG_VERSION\"), env!(\"VERGEN_GIT_SHA\"));\n        }\n        _ => todo!(\"Command not yet implemented\"),\n    }\n    \n    Ok(())\n}\n```\n\n## TOON Format (Token-Optimized Object Notation)\nFor extremely token-constrained agents, provide compact output:\n\n```rust\n// crates/vc_cli/src/toon.rs\n\npub trait ToToon {\n    fn to_toon(&self) -> String;\n}\n\nimpl ToToon for HealthData {\n    fn to_toon(&self) -> String {\n        // Convert to compact format:\n        // F:4on1off|orko:on,15ag(14h),cpu45,mem68|sydneymc:on,12ag,cpu78\n        // AL:3[!ratelim@orko:95%,!zombie@syd:2,!disk@mac:89%]\n        // PR:[ratelim:swap@23m,fail:cc5@34%,cost:+$12@eod]\n        todo!()\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] `vc --help` shows all commands\n- [ ] `vc --version` shows version and git SHA\n- [ ] `vc robot health --json` returns valid envelope\n- [ ] JSON output is machine-parseable\n- [ ] Schema version included in all robot output\n\n## Notes for Future Self\n- stdout = data, stderr = diagnostics\n- No color in robot mode\n- Consider `vc robot health --format toon` for compact output\n- Add completions generation later\n\n## Testing & Logging\n- Clap parsing tests for flags/subcommands and invalid input.\n- JSON schema compliance tests with golden outputs.\n- Error-path tests (missing config/db, read-only store).\n- E2E: tests/e2e/cli/test_<cmd>.sh with JSON summary and stderr capture.\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-27T16:21:37.561292331Z","created_by":"ubuntu","updated_at":"2026-01-28T04:46:20.646657395Z","closed_at":"2026-01-28T04:46:20.646394871Z","close_reason":"CLI implementation complete: clap structure, robot mode envelope, all subcommands defined, 12 tests passing","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2a9.5","depends_on_id":"bd-2a9","type":"parent-child","created_at":"2026-01-27T16:21:37.561292331Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2a9.5","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T16:21:44.063723994Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2a9.5","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:21:44.212711165Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2a9.5","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:21:44.301469818Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2br","title":"Implement alert delivery channels (webhook, Slack, desktop)","description":"Implement alert delivery channels beyond TUI.\n\nPURPOSE:\nDeliver alerts through multiple channels: webhooks, Slack, Discord, desktop notifications, and Agent Mail integration.\n\nCHANNELS (from plan Section 8.1):\n\n1. TUI HIGHLIGHT (MVP - already in Alerts MVP)\n- Visual highlighting in TUI\n- Sound alerts (optional, configurable)\n\n2. FILE OUTPUT (MVP - already in Alerts MVP)\n- Write to alerts.jsonl file\n- Enables external tool integration\n\n3. WEBHOOK\n- POST JSON to configurable URL\n- Support custom headers (auth tokens)\n- Retry with exponential backoff\n\n4. SLACK\n- Slack incoming webhook integration\n- Format messages with blocks for rich display\n- Thread replies for updates\n\n5. DISCORD\n- Discord webhook integration\n- Embed formatting for rich display\n\n6. AGENT MAIL\n- Send alert as mcp_agent_mail message\n- Thread per alert for updates\n- Tag with alert metadata\n\n7. DESKTOP NOTIFICATIONS\n- Use system notification APIs\n- macOS: osascript/notification center\n- Linux: notify-send or D-Bus\n\n8. EMAIL (stretch goal)\n- SMTP integration\n- Template-based formatting\n\nCHANNEL CONFIGURATION:\n[alert_channels]\n\n[alert_channels.slack]\nenabled = true\nwebhook_url = \"https://hooks.slack.com/...\"\nmin_severity = \"warning\"\n\n[alert_channels.webhook]\nenabled = true\nurl = \"https://my-service.com/alerts\"\nheaders = { \"Authorization\" = \"Bearer ${VC_WEBHOOK_TOKEN}\" }\nmin_severity = \"critical\"\n\n[alert_channels.desktop]\nenabled = true\nmin_severity = \"critical\"\n\n[alert_channels.agent_mail]\nenabled = true\nrecipient = \"HumanOperator\"\nmin_severity = \"warning\"\n\nDATA MODEL:\nCREATE TABLE alert_delivery_log(\n    id INTEGER PRIMARY KEY,\n    alert_id TEXT NOT NULL,\n    channel_type TEXT NOT NULL,\n    delivered_at TIMESTAMP,\n    status TEXT,  -- pending, delivered, failed\n    error_message TEXT,\n    retry_count INTEGER DEFAULT 0\n);\n\nIMPLEMENTATION:\npub trait AlertChannel: Send + Sync {\n    fn name(&self) -> &str;\n    async fn deliver(&self, alert: &Alert) -> Result<(), ChannelError>;\n    fn supports_severity(&self, severity: Severity) -> bool;\n}\n\npub struct ChannelManager {\n    channels: Vec<Box<dyn AlertChannel>>,\n\n    pub async fn deliver_all(&self, alert: &Alert) -> Vec<DeliveryResult> {\n        let futures = self.channels.iter()\n            .filter(|c| c.supports_severity(alert.severity))\n            .map(|c| c.deliver(alert));\n        join_all(futures).await\n    }\n}\n\nTESTS:\n- Test webhook delivery with mock server\n- Test retry logic\n- Test severity filtering\n- Test delivery log updates\n\n## Testing & Logging\n- Unit tests for channel selection, severity filtering, retry/backoff, and templating.\n- Integration tests with mocked webhook/Slack/Discord endpoints and agent_mail stub.\n- E2E: `tests/e2e/system/test_alert_delivery.sh` validates payloads, retries, and audit events; JSON summary in tests/logs/.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T18:20:13.515827450Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:16.665608187Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2br","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T18:24:37.875492724Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2br","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T18:24:37.347245334Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2br","depends_on_id":"bd-2mv.4","type":"blocks","created_at":"2026-01-27T18:24:38.408610444Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2br","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T18:20:58.500134288Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2br","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:16.665137701Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2br","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T18:21:13.360800074Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2cw","title":"Merge OPUS plan into GPT plan","description":"Compare COMPREHENSIVE_PLAN_TO_MAKE_VC__OPUS.md vs COMPREHENSIVE_PLAN_TO_MAKE_VC__GPT.md, extract superior ideas, and revise GPT plan into a hybrid best-of document. Include analysis notes and ensure plan remains practical and buildable.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T06:27:44.080966923Z","created_by":"ubuntu","updated_at":"2026-01-27T06:30:18.981539768Z","closed_at":"2026-01-27T06:30:18.981522636Z","close_reason":"Merged OPUS ideas into GPT plan (north star, time machine, forecasting, playbooks, integrations, success metrics)","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-2cz","title":"Implement TUI Oracle screen","description":"Implement the Oracle TUI screen showing predictions, forecasts, and risk assessment.\n\nSCREEN CONTENT:\n- Rate limit forecasts: Time to limit per account\n- Failure predictions: Agents at risk of failure\n- Cost trajectory: Current spend vs budget\n- Resource forecasts: Disk, CPU, memory projections\n\nLAYOUT:\n+------------------------------------------------+\n| ORACLE - PREDICTIONS               [r]efresh    |\n+------------------------------------------------+\n| RATE LIMIT FORECASTS                            |\n| ├─ claude (jeff@email.com)                      |\n| │   └─ 92% used, limit in 8 min at current rate |\n| │   └─ RECOMMENDATION: Swap to backup@email.com |\n| ├─ openai (dev@company.com)                     |\n| │   └─ 45% used, 4hr headroom                   |\n| └─ gemini (user@gmail.com)                      |\n|     └─ 12% used, plenty of headroom             |\n+------------------------------------------------+\n| FAILURE RISK                                    |\n| ├─ cc_5 on orko: 78% stuck risk in 15 min      |\n| │   └─ Indicators: velocity down, context high  |\n| │   └─ Similar patterns: n=47 past occurrences  |\n| ├─ codex_2 on sydneymc: healthy                 |\n+------------------------------------------------+\n| COST TRAJECTORY                                 |\n| ├─ Today:  spent (budget: )              |\n| │   └─ Projection:  by EOD (on track)       |\n| ├─ This week:  (budget: )              |\n| │   └─ Savings opportunity: shift to off-peak   |\n+------------------------------------------------+\n| RESOURCE FORECASTS                              |\n| ├─ orko disk: 89% -> 95% in 2 days             |\n| │   └─ ALERT: Will hit critical in 48h          |\n| └─ sydneymc memory: stable at 68%               |\n+------------------------------------------------+\n\nNAVIGATION:\n- Tab between forecast types\n- Enter to see detailed forecast breakdown\n- a to apply recommended action\n- d to dismiss/acknowledge\n\nDATA SOURCES:\n- predictions table\n- account_usage_snapshots (for rate forecasting)\n- sys_samples (for resource forecasting)\n- Oracle engine calculations\n\nIMPLEMENTATION:\n- Create oracle.rs in vc_tui/src/screens/\n- Show confidence intervals for predictions\n- Color-code by risk level\n- Highlight actionable recommendations\n- Support threshold customization\n\nTESTS:\n- Snapshot tests for rendering\n- Forecast display formatting\n- Risk level coloring tests\n\n## E2E & Logging\n- Add/extend `tests/e2e/tui/test_oracle_screen.sh` with expect/pexpect.\n- Capture logs and screenshots on failure; emit JSON summary in tests/logs/.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:49:36.676119159Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:17.293244606Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2cz","depends_on_id":"bd-2mv","type":"parent-child","created_at":"2026-01-27T16:55:43.307101625Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2cz","depends_on_id":"bd-2mv.5","type":"blocks","created_at":"2026-01-27T16:54:29.114092872Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2cz","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:17.292472812Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2cz","depends_on_id":"bd-33h.3","type":"blocks","created_at":"2026-01-27T16:54:28.016463105Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2df","title":"Implement TUI Beads screen","description":"Implement the Beads TUI screen showing bv triage output, blockers, and next picks.\n\nSCREEN CONTENT:\n- Quick reference: Ready tasks, blocked tasks, epics\n- Recommendations: What to work on next\n- Blockers: Tasks blocking progress\n- Graph health: Cycles, critical path\n\nLAYOUT:\n+------------------------------------------------+\n| BEADS TRIAGE                   [f]ilter [/]search |\n+------------------------------------------------+\n| QUICK REFERENCE                                 |\n| ├─ Ready: 12 tasks (3 P0, 5 P1, 4 P2)          |\n| ├─ Blocked: 34 tasks                            |\n| ├─ In Progress: 5 tasks                         |\n| └─ Epics: 7 total, 2 with ready work           |\n+------------------------------------------------+\n| RECOMMENDED NEXT                                |\n| 1. bd-2a9.1 [P0] Create Cargo workspace         |\n| 2. bd-2yb [P0] Implement fallback system probe  |\n| 3. bd-33h.1 [P1] Implement sysmoni collector    |\n+------------------------------------------------+\n| BLOCKERS                                        |\n| ├─ bd-2a9.1 blocks 5 tasks                      |\n| ├─ bd-2a9.3 blocks 4 tasks                      |\n| └─ bd-33h.1 blocks 3 tasks                      |\n+------------------------------------------------+\n| GRAPH HEALTH                                    |\n| ├─ Cycles: None detected                        |\n| ├─ Critical path: 12 tasks, 6 phases            |\n| └─ Bottleneck: Phase 0 Foundation               |\n+------------------------------------------------+\n\nNAVIGATION:\n- Tab between sections\n- Enter on task to show details\n- j/k to navigate task list\n- o to open task in editor\n- m to mark task in progress\n\nDATA SOURCES:\n- beads_triage_snapshots table\n- beads_issues table\n- beads_graph_metrics table\n\nIMPLEMENTATION:\n- Create beads.rs in vc_tui/src/screens/\n- Show cross-repo aggregation (all repos with .beads)\n- Support filtering by repo, priority, type\n- Color-code by priority (P0=red, P1=orange, etc.)\n\nTESTS:\n- Snapshot tests for rendering\n- Empty state handling\n- Multi-repo aggregation tests\n\n## E2E & Logging\n- Add/extend `tests/e2e/tui/test_beads_screen.sh` with expect/pexpect.\n- Capture logs and screenshots on failure; emit JSON summary in tests/logs/.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:49:20.974105924Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:17.991513334Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2df","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:17.990917712Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2df","depends_on_id":"bd-33h","type":"parent-child","created_at":"2026-01-27T16:55:41.837971600Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2df","depends_on_id":"bd-33h.3","type":"blocks","created_at":"2026-01-27T16:54:25.820461013Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2df","depends_on_id":"bd-rf7","type":"blocks","created_at":"2026-01-27T16:54:26.936236839Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2mv","title":"Phase 2: Accounts + Sessions + Mail","description":"# Phase 2: Accounts + Sessions + Mail\n\n## Purpose\nIntegrate account management (caut/caam), session indexing (cass), and inter-agent coordination (mcp_agent_mail). This phase enables the critical rate limit forecasting feature - the Predictive Oracle MVP.\n\n## Key Deliverables\n1. **caut Collector**: Per-account usage tracking via `caut usage --json`\n2. **caam Collector**: Profile limits and status via `caam limits --format json`, `caam status --json`\n3. **cass Collector**: Session statistics and timeline via `cass stats/timeline/health --json`\n4. **mcp_agent_mail Collector**: Message/reservation tracking via SQLite incremental queries\n5. **TUI Screens**: Accounts, Sessions, Mail views\n6. **Rate Limit Forecasting**: Oracle MVP predicting time-to-limit and suggesting swaps\n\n## Success Criteria\n- Account usage visible across all providers (Claude, OpenAI, Gemini)\n- Rate limit forecasts accurate within 10 minutes\n- Mail backlog surfaced with urgency indicators\n- Session volume trends visible\n\n## Technical Context\n\n### Account System Architecture\ncaut tracks usage windows, caam manages profiles:\n- caut: \"How much have I used?\" (usage percentage, reset times)\n- caam: \"Which account should I use?\" (profile selection, health scores)\n\nTogether they enable intelligent account rotation before hitting limits.\n\n### mcp_agent_mail SQLite Integration\nThe SQLite incremental pattern:\n- Keep cursor of last-seen message_id\n- Query `WHERE id > cursor ORDER BY id LIMIT N`\n- Update cursor on success\n\nThis avoids rescanning the entire message history each poll.\n\n### Oracle MVP - Rate Limit Forecasting\nThe first prediction capability:\n```rust\npub struct RateLimitForecast {\n    pub time_to_limit: Duration,\n    pub confidence: f64,\n    pub recommended_action: RateLimitAction, // Continue, SlowDown, PrepareSwap, SwapNow\n    pub optimal_swap_time: Option<Instant>,\n    pub alternative_accounts: Vec<(AccountId, f64)>,\n}\n```\n\nCalculated from: current_usage_pct + velocity (consumption rate) + historical patterns.\n\n## Dependencies\n- Requires Phase 1 (basic collectors working, TUI framework in place)\n\n## Estimated Scope\n- ~2 weeks\n- Enables proactive rate limit management - high user value\n\n## Testing & E2E Expectations\n- Every deliverable in this phase must add unit tests per bd-1x9.\n- Extend bd-30z with phase-specific e2e scenarios and structured logs.\n- Phase completion requires `cargo test --workspace` + phase e2e suite with JSON summaries in tests/logs/.\n","status":"in_progress","priority":0,"issue_type":"epic","created_at":"2026-01-27T16:17:36.425461846Z","created_by":"ubuntu","updated_at":"2026-01-28T17:03:27.280899444Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2mv","depends_on_id":"bd-2a9","type":"blocks","created_at":"2026-01-27T18:22:32.210634876Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2mv.1","title":"Implement caut (usage tracker) collector","description":"# Task: Implement caut (usage tracker) collector\n\n## What to Build\nA collector for per-account usage tracking via the caut tool. Uses CLI Snapshot pattern.\n\n## Integration Method\n```bash\ncaut usage --json\n```\n\n## Expected JSON Schema\n```json\n{\n  \"accounts\": [\n    {\n      \"provider\": \"claude\",\n      \"account\": \"jeff@email.com\",\n      \"window\": \"5_hour\",\n      \"used_percent\": 45.0,\n      \"remaining_percent\": 55.0,\n      \"resets_at\": \"2026-01-27T05:00:00Z\",\n      \"credits_remaining\": null,\n      \"status\": \"healthy\"\n    },\n    {\n      \"provider\": \"openai\",\n      \"account\": \"dev@company.com\",\n      \"window\": \"daily\",\n      \"used_percent\": 78.0,\n      \"remaining_percent\": 22.0,\n      \"resets_at\": \"2026-01-28T00:00:00Z\",\n      \"credits_remaining\": 85.50,\n      \"status\": \"warning\"\n    }\n  ]\n}\n```\n\n## Collector Implementation\n```rust\n// crates/vc_collect/src/collectors/caut.rs\n\npub struct CautCollector;\n\n#[async_trait]\nimpl Collector for CautCollector {\n    fn name(&self) -> &'static str { \"caut\" }\n    fn schema_version(&self) -> u32 { 1 }\n    fn required_tool(&self) -> Option<&'static str> { Some(\"caut\") }\n    \n    async fn collect(&self, ctx: &CollectContext) -> CollectResult {\n        let output = ctx.executor.run_timeout(\"caut usage --json\", ctx.timeout).await?;\n        let data: CautUsageOutput = serde_json::from_str(&output)?;\n        let collected_at = Utc::now();\n        \n        let rows: Vec<_> = data.accounts.iter().map(|a| {\n            serde_json::json!({\n                \"machine_id\": ctx.machine_id.as_str(),\n                \"collected_at\": collected_at.to_rfc3339(),\n                \"provider\": a.provider,\n                \"account\": a.account,\n                \"window\": a.window,\n                \"used_percent\": a.used_percent,\n                \"resets_at\": a.resets_at,\n                \"credits_remaining\": a.credits_remaining,\n                \"status\": a.status,\n                \"raw_json\": serde_json::to_string(a).unwrap_or_default(),\n            })\n        }).collect();\n        \n        CollectResult {\n            rows: vec![RowBatch { table: \"account_usage_snapshots\", rows }],\n            new_cursor: None,\n            ..Default::default()\n        }\n    }\n}\n```\n\n## DuckDB Table\n```sql\nCREATE TABLE IF NOT EXISTS account_usage_snapshots(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    provider TEXT,\n    account TEXT,\n    window TEXT,\n    used_percent REAL,\n    resets_at TIMESTAMP,\n    credits_remaining REAL,\n    status TEXT,\n    raw_json TEXT\n);\n\nCREATE INDEX IF NOT EXISTS idx_account_usage_time \nON account_usage_snapshots(machine_id, provider, account, collected_at DESC);\n```\n\n## Key Use Cases\n1. **Rate limit monitoring**: Show usage % approaching limits\n2. **Reset time tracking**: Know when accounts refresh\n3. **Cross-account comparison**: Which account has most headroom?\n4. **Historical trends**: Usage patterns over time\n\n## Acceptance Criteria\n- [ ] Collector parses caut JSON correctly\n- [ ] account_usage_snapshots table populated\n- [ ] All providers tracked (claude, openai, gemini)\n- [ ] Graceful handling when caut unavailable\n\n## Notes for Future Self\n- caut has both token-accounts.json (static) and usage-history.sqlite (historical)\n- Consider reading SQLite directly for historical charts\n- This feeds directly into Oracle rate limit forecasting\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:24:09.834662734Z","created_by":"ubuntu","updated_at":"2026-01-28T08:34:09.447827398Z","closed_at":"2026-01-28T08:34:09.447783605Z","close_reason":"Implemented caut collector with CLI Snapshot pattern - parses usage JSON with accounts, providers, usage percentages, and reset times","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2mv.1","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T16:59:33.978363393Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.1","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:59:34.919835110Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.1","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:59:35.845576628Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.1","depends_on_id":"bd-2mv","type":"parent-child","created_at":"2026-01-27T16:24:09.834662734Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2mv.2","title":"Implement caam (account manager) collector","description":"# Task: Implement caam (account manager) collector\n\n## What to Build\nA collector for account profile and limit information via caam. Complements caut with profile selection and health scoring.\n\n## Integration Method\n```bash\ncaam limits --format json    # Detailed limit info per profile\ncaam status --json           # Active profile and health status\n```\n\n## Expected JSON Schemas\n\n### caam limits --format json\n```json\n{\n  \"providers\": [\n    {\n      \"provider\": \"claude\",\n      \"profiles\": [\n        {\n          \"name\": \"jeff@email.com\",\n          \"window\": \"5_hour\",\n          \"utilization_percent\": 45.0,\n          \"resets_at\": \"2026-01-27T05:00:00Z\",\n          \"is_active\": true\n        }\n      ]\n    }\n  ]\n}\n```\n\n### caam status --json\n```json\n{\n  \"tools\": [\n    {\n      \"tool\": \"claude-code\",\n      \"active_profile\": \"jeff@email.com\",\n      \"health_score\": 0.85,\n      \"health_expires_at\": \"2026-01-27T01:00:00Z\"\n    }\n  ]\n}\n```\n\n## Collector Implementation\n```rust\npub struct CaamCollector;\n\n#[async_trait]\nimpl Collector for CaamCollector {\n    fn name(&self) -> &'static str { \"caam\" }\n    fn schema_version(&self) -> u32 { 1 }\n    fn required_tool(&self) -> Option<&'static str> { Some(\"caam\") }\n    \n    async fn collect(&self, ctx: &CollectContext) -> CollectResult {\n        let collected_at = Utc::now();\n        let mut rows = vec![];\n        let mut warnings = vec![];\n        \n        // Get limits\n        if let Ok(output) = ctx.executor.run_timeout(\"caam limits --format json\", ctx.timeout).await {\n            if let Ok(limits) = serde_json::from_str::<CaamLimitsOutput>(&output) {\n                for provider in &limits.providers {\n                    for profile in &provider.profiles {\n                        rows.push(serde_json::json!({\n                            \"machine_id\": ctx.machine_id.as_str(),\n                            \"collected_at\": collected_at.to_rfc3339(),\n                            \"provider\": provider.provider,\n                            \"profile\": profile.name,\n                            \"window\": profile.window,\n                            \"utilization_pct\": profile.utilization_percent,\n                            \"resets_at\": profile.resets_at,\n                            \"raw_json\": serde_json::to_string(profile).unwrap_or_default(),\n                        }));\n                    }\n                }\n            }\n        }\n        \n        // Get status\n        if let Ok(output) = ctx.executor.run_timeout(\"caam status --json\", ctx.timeout).await {\n            if let Ok(status) = serde_json::from_str::<CaamStatusOutput>(&output) {\n                for tool in &status.tools {\n                    rows.push(serde_json::json!({\n                        \"table\": \"account_profile_snapshots\",\n                        \"machine_id\": ctx.machine_id.as_str(),\n                        \"collected_at\": collected_at.to_rfc3339(),\n                        \"tool\": tool.tool,\n                        \"active_profile\": tool.active_profile,\n                        \"health_expires_at\": tool.health_expires_at,\n                        \"raw_json\": serde_json::to_string(tool).unwrap_or_default(),\n                    }));\n                }\n            }\n        }\n        \n        CollectResult {\n            rows: vec![\n                RowBatch { table: \"account_limits_snapshots\", rows: rows.clone() },\n                RowBatch { table: \"account_profile_snapshots\", rows },\n            ],\n            ..Default::default()\n        }\n    }\n}\n```\n\n## DuckDB Tables\n```sql\nCREATE TABLE IF NOT EXISTS account_limits_snapshots(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    provider TEXT,\n    profile TEXT,\n    window TEXT,\n    utilization_pct REAL,\n    resets_at TIMESTAMP,\n    raw_json TEXT\n);\n\nCREATE TABLE IF NOT EXISTS account_profile_snapshots(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    tool TEXT,\n    active_profile TEXT,\n    health_expires_at TIMESTAMP,\n    raw_json TEXT\n);\n```\n\n## Key Use Cases\n1. **Profile recommendation**: \"Which account should this agent use?\"\n2. **Health tracking**: Know when profiles need rotation\n3. **Cross-tool coordination**: Which tool is using which profile?\n\n## Acceptance Criteria\n- [ ] Collector runs both caam commands\n- [ ] Both tables populated correctly\n- [ ] Active profile tracked per tool\n- [ ] Health expiration captured\n\n## Notes for Future Self\n- caam and caut overlap in functionality; normalize in queries\n- Consider recommending account swaps based on combined data\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:24:29.409262208Z","created_by":"ubuntu","updated_at":"2026-01-28T08:36:03.151988796Z","closed_at":"2026-01-28T08:36:03.151494705Z","close_reason":"Implemented caam collector - collects profile limits and status from caam limits/status commands","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2mv.2","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T16:59:36.577790247Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.2","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:59:37.498839797Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.2","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:59:38.458213685Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.2","depends_on_id":"bd-2mv","type":"parent-child","created_at":"2026-01-27T16:24:29.409262208Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2mv.3","title":"Implement cass (session search) collector","description":"# Task: Implement cass (session search) collector\n\n## What to Build\nA collector for session index statistics via cass. Provides session volume metrics and index health.\n\n## Integration Method\n```bash\ncass stats --json     # Aggregate statistics\ncass timeline --json  # Time-bucketed data\ncass health --json    # Index status\n```\n\n## Expected JSON Schemas\n\n### cass stats --json\n```json\n{\n  \"total_conversations\": 1500,\n  \"total_messages\": 45000,\n  \"by_agent\": {\n    \"claude-code\": 800,\n    \"codex-cli\": 500,\n    \"gemini-cli\": 200\n  },\n  \"by_workspace\": {\n    \"/data/projects/vibe_cockpit\": 150,\n    \"/data/projects/dcg\": 120\n  }\n}\n```\n\n### cass health --json\n```json\n{\n  \"state\": \"ready\",\n  \"total_sessions\": 1500,\n  \"last_index_at\": \"2026-01-27T00:00:00Z\",\n  \"index_size_bytes\": 52428800,\n  \"freshness_seconds\": 120\n}\n```\n\n## Collector Implementation\n```rust\npub struct CassCollector;\n\n#[async_trait]\nimpl Collector for CassCollector {\n    fn name(&self) -> &'static str { \"cass\" }\n    fn schema_version(&self) -> u32 { 1 }\n    fn required_tool(&self) -> Option<&'static str> { Some(\"cass\") }\n    \n    async fn collect(&self, ctx: &CollectContext) -> CollectResult {\n        let collected_at = Utc::now();\n        let mut rows = vec![];\n        \n        // Health status\n        if let Ok(output) = ctx.executor.run_timeout(\"cass health --json\", ctx.timeout).await {\n            if let Ok(health) = serde_json::from_str::<CassHealthOutput>(&output) {\n                rows.push(RowBatch {\n                    table: \"cass_index_status\",\n                    rows: vec![serde_json::json!({\n                        \"machine_id\": ctx.machine_id.as_str(),\n                        \"collected_at\": collected_at.to_rfc3339(),\n                        \"state\": health.state,\n                        \"total_sessions\": health.total_sessions,\n                        \"last_index_at\": health.last_index_at,\n                        \"raw_json\": output,\n                    })],\n                });\n            }\n        }\n        \n        // Stats\n        if let Ok(output) = ctx.executor.run_timeout(\"cass stats --json\", ctx.timeout).await {\n            if let Ok(stats) = serde_json::from_str::<CassStatsOutput>(&output) {\n                // Store overall metrics\n                rows.push(RowBatch {\n                    table: \"cass_stats_snapshots\",\n                    rows: vec![\n                        serde_json::json!({\n                            \"machine_id\": ctx.machine_id.as_str(),\n                            \"collected_at\": collected_at.to_rfc3339(),\n                            \"metric_name\": \"total_conversations\",\n                            \"metric_value\": stats.total_conversations,\n                            \"dimensions_json\": \"{}\",\n                        }),\n                        serde_json::json!({\n                            \"machine_id\": ctx.machine_id.as_str(),\n                            \"collected_at\": collected_at.to_rfc3339(),\n                            \"metric_name\": \"total_messages\",\n                            \"metric_value\": stats.total_messages,\n                            \"dimensions_json\": \"{}\",\n                        }),\n                    ],\n                });\n                \n                // Store per-agent breakdown\n                for (agent, count) in &stats.by_agent {\n                    rows.push(RowBatch {\n                        table: \"cass_stats_snapshots\",\n                        rows: vec![serde_json::json!({\n                            \"machine_id\": ctx.machine_id.as_str(),\n                            \"collected_at\": collected_at.to_rfc3339(),\n                            \"metric_name\": \"sessions_by_agent\",\n                            \"metric_value\": count,\n                            \"dimensions_json\": format!(r#\"{{\"agent\":\"{}\"}}\"#, agent),\n                        })],\n                    });\n                }\n            }\n        }\n        \n        CollectResult { rows, ..Default::default() }\n    }\n}\n```\n\n## DuckDB Tables\n```sql\nCREATE TABLE IF NOT EXISTS cass_index_status(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    state TEXT,\n    total_sessions INTEGER,\n    last_index_at TIMESTAMP,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, collected_at)\n);\n\nCREATE TABLE IF NOT EXISTS cass_stats_snapshots(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    metric_name TEXT,\n    metric_value REAL,\n    dimensions_json TEXT,\n    raw_json TEXT\n);\n```\n\n## Alert Rule: Stale Index\n```rust\n// Alert if index not refreshed in 24 hours\n// This indicates cass indexer may be stuck\n```\n\n## Acceptance Criteria\n- [ ] Collector runs all three cass commands\n- [ ] Index health status tracked\n- [ ] Session counts by agent type captured\n- [ ] Stale index detectable (> 24h)\n\n## Notes for Future Self\n- cass has a vector index for semantic search - not collected here\n- Consider adding cass search integration for knowledge mining\n- Timeline data useful for activity heatmaps\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:24:50.809111745Z","created_by":"ubuntu","updated_at":"2026-01-28T08:37:04.659252603Z","closed_at":"2026-01-28T08:37:04.659230361Z","close_reason":"CassCollector implemented with health and stats collection, 8 unit tests, registered in CollectorRegistry, DB tables added","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2mv.3","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T16:59:39.327220407Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.3","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:59:40.139319688Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.3","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:59:41.045668879Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.3","depends_on_id":"bd-2mv","type":"parent-child","created_at":"2026-01-27T16:24:50.809111745Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2mv.4","title":"Implement mcp_agent_mail SQLite collector","description":"# Task: Implement mcp_agent_mail SQLite collector\n\n## What to Build\nA collector that reads from mcp_agent_mail's SQLite database using the incremental primary key pattern.\n\n## Integration Method\nDirect SQLite query on `storage.sqlite3`:\n```\n~/.mcp_agent_mail_git_mailbox_repo/storage.sqlite3\n```\n\n## SQLite Incremental Pattern\nThis collector uses the \"SQLite Incremental\" pattern:\n1. Keep cursor of last-seen message_id\n2. Query `WHERE id > cursor ORDER BY id LIMIT N`\n3. Update cursor on success\n\n```rust\npub struct AgentMailCollector {\n    db_path: PathBuf,\n}\n\n#[async_trait]\nimpl Collector for AgentMailCollector {\n    fn name(&self) -> &'static str { \"mcp_agent_mail\" }\n    fn schema_version(&self) -> u32 { 1 }\n    \n    async fn collect(&self, ctx: &CollectContext) -> CollectResult {\n        let last_id = match &ctx.last_cursor {\n            Some(Cursor::PrimaryKey(id)) => *id,\n            _ => 0,\n        };\n        \n        let collected_at = Utc::now();\n        let mut rows = vec![];\n        let mut max_id = last_id;\n        \n        // Query messages\n        let query = format!(\n            r#\"\n            SELECT id, project_id, thread_id, sender, \n                   importance, ack_required, created_ts, subject\n            FROM messages \n            WHERE id > {} \n            ORDER BY id \n            LIMIT {}\n            \"#,\n            last_id, ctx.max_rows\n        );\n        \n        let messages = ctx.executor.sqlite_query(&self.db_path, &query).await?;\n        \n        for msg in &messages {\n            let id: i64 = msg[\"id\"].as_i64().unwrap_or(0);\n            max_id = max_id.max(id);\n            \n            rows.push(serde_json::json!({\n                \"collected_at\": collected_at.to_rfc3339(),\n                \"project_id\": msg[\"project_id\"],\n                \"message_id\": id,\n                \"thread_id\": msg[\"thread_id\"],\n                \"sender\": msg[\"sender\"],\n                \"importance\": msg[\"importance\"],\n                \"ack_required\": msg[\"ack_required\"],\n                \"created_ts\": msg[\"created_ts\"],\n                \"subject\": msg[\"subject\"],\n                \"raw_json\": serde_json::to_string(msg).unwrap_or_default(),\n            }));\n        }\n        \n        // Query file reservations (snapshot, not incremental)\n        let reservation_query = r#\"\n            SELECT id, project_id, path_pattern, holder, expires_ts, exclusive\n            FROM file_reservations\n            WHERE expires_ts > datetime('now')\n        \"#;\n        \n        let reservations = ctx.executor.sqlite_query(&self.db_path, reservation_query).await?;\n        \n        let reservation_rows: Vec<_> = reservations.iter().map(|r| {\n            serde_json::json!({\n                \"collected_at\": collected_at.to_rfc3339(),\n                \"project_id\": r[\"project_id\"],\n                \"reservation_id\": r[\"id\"],\n                \"path_pattern\": r[\"path_pattern\"],\n                \"holder\": r[\"holder\"],\n                \"expires_ts\": r[\"expires_ts\"],\n                \"exclusive\": r[\"exclusive\"],\n                \"raw_json\": serde_json::to_string(r).unwrap_or_default(),\n            })\n        }).collect();\n        \n        CollectResult {\n            rows: vec![\n                RowBatch { table: \"mail_messages\", rows },\n                RowBatch { table: \"mail_file_reservations\", rows: reservation_rows },\n            ],\n            new_cursor: Some(Cursor::PrimaryKey(max_id)),\n            ..Default::default()\n        }\n    }\n}\n```\n\n## DuckDB Tables\n```sql\nCREATE TABLE IF NOT EXISTS mail_messages(\n    collected_at TIMESTAMP,\n    project_id TEXT,\n    message_id INTEGER,\n    thread_id TEXT,\n    sender TEXT,\n    importance TEXT,\n    ack_required BOOLEAN,\n    created_ts TIMESTAMP,\n    subject TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (project_id, message_id)\n);\n\nCREATE TABLE IF NOT EXISTS mail_recipients(\n    collected_at TIMESTAMP,\n    message_id INTEGER,\n    recipient TEXT,\n    read_ts TIMESTAMP,\n    ack_ts TIMESTAMP,\n    raw_json TEXT\n);\n\nCREATE TABLE IF NOT EXISTS mail_file_reservations(\n    collected_at TIMESTAMP,\n    project_id TEXT,\n    reservation_id INTEGER,\n    path_pattern TEXT,\n    holder TEXT,\n    expires_ts TIMESTAMP,\n    exclusive BOOLEAN,\n    raw_json TEXT\n);\n```\n\n## Key Queries\n```sql\n-- Urgent unread messages\nSELECT * FROM mail_messages\nWHERE importance IN ('high', 'urgent')\n  AND ack_required = true\n  AND message_id NOT IN (\n    SELECT message_id FROM mail_recipients WHERE ack_ts IS NOT NULL\n  );\n\n-- Active file reservations\nSELECT * FROM mail_file_reservations\nWHERE expires_ts > CURRENT_TIMESTAMP;\n\n-- Message volume by thread\nSELECT thread_id, COUNT(*) as msg_count\nFROM mail_messages\nGROUP BY thread_id\nORDER BY msg_count DESC;\n```\n\n## Alert Rules\n- Mail backlog: > 5 urgent unread\n- Mail overdue: > 3 ack_required unacked > 1 hour\n\n## Acceptance Criteria\n- [ ] Incremental message collection works\n- [ ] Cursor persisted across poll cycles\n- [ ] File reservations captured\n- [ ] Handles missing SQLite file gracefully\n\n## Notes for Future Self\n- SQLite path may vary per machine - make configurable\n- Consider recipients table for read/ack tracking\n- This is critical for agent coordination visibility\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:25:15.651458163Z","created_by":"ubuntu","updated_at":"2026-01-28T08:32:27.001839209Z","closed_at":"2026-01-28T08:32:27.001736275Z","close_reason":"Implemented mcp_agent_mail SQLite collector with incremental message collection and file reservation snapshots","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2mv.4","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T16:59:41.985004202Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.4","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:59:42.903426863Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.4","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:59:43.678049794Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.4","depends_on_id":"bd-2mv","type":"parent-child","created_at":"2026-01-27T16:25:15.651458163Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2mv.5","title":"Implement Oracle MVP - Rate Limit Forecasting","description":"# Task: Implement Oracle MVP - Rate Limit Forecasting\n\n## What to Build\nThe first predictive capability: forecasting when accounts will hit rate limits and recommending preemptive swaps.\n\n## Core Data Structure\n```rust\n// crates/vc_oracle/src/rate_limit.rs\n\n#[derive(Debug, Serialize)]\npub struct RateLimitForecast {\n    pub provider: String,\n    pub account: String,\n    \n    // Current state\n    pub current_usage_pct: f64,\n    pub current_velocity: f64,  // consumption rate (% per minute)\n    \n    // Predictions\n    pub time_to_limit: Duration,\n    pub confidence: f64,\n    pub recommended_action: RateLimitAction,\n    \n    // Optimization\n    pub optimal_swap_time: Option<DateTime<Utc>>,\n    pub alternative_accounts: Vec<(String, f64)>,  // (account, headroom_pct)\n}\n\n#[derive(Debug, Serialize)]\npub enum RateLimitAction {\n    Continue,                              // All good\n    SlowDown { target_velocity: f64 },     // Pace yourself\n    PrepareSwap { in_minutes: u32 },       // Get ready\n    SwapNow { to_account: String },        // Do it now\n    EmergencyPause,                        // Stop everything\n}\n```\n\n## Forecasting Algorithm\n```rust\nimpl Oracle {\n    pub async fn forecast_rate_limits(&self, store: &Store) -> Vec<RateLimitForecast> {\n        let mut forecasts = vec![];\n        \n        // Get recent usage snapshots (last 2 hours)\n        let query = r#\"\n            SELECT \n                provider, account, used_percent, collected_at, resets_at\n            FROM account_usage_snapshots\n            WHERE collected_at > datetime('now', '-2 hours')\n            ORDER BY provider, account, collected_at\n        \"#;\n        \n        let samples = store.query(query).await?;\n        \n        // Group by provider+account\n        let grouped = group_by_account(samples);\n        \n        for (key, samples) in grouped {\n            let forecast = self.forecast_single(&key, &samples).await;\n            forecasts.push(forecast);\n        }\n        \n        // Sort by urgency (smallest time_to_limit first)\n        forecasts.sort_by_key(|f| f.time_to_limit);\n        \n        forecasts\n    }\n    \n    async fn forecast_single(&self, key: &AccountKey, samples: &[UsageSample]) -> RateLimitForecast {\n        // Calculate velocity (rate of usage increase)\n        let velocity = calculate_velocity(samples);\n        \n        // Get current usage\n        let current = samples.last().unwrap();\n        \n        // Time to 100% at current velocity\n        let remaining = 100.0 - current.used_percent;\n        let time_to_limit = if velocity > 0.0 {\n            Duration::from_secs_f64(remaining / velocity * 60.0)\n        } else {\n            Duration::MAX\n        };\n        \n        // Confidence based on sample count and velocity stability\n        let confidence = calculate_confidence(samples, velocity);\n        \n        // Find alternatives with headroom\n        let alternatives = self.find_alternative_accounts(key.provider, store).await;\n        \n        // Determine recommended action\n        let action = match time_to_limit.as_secs() {\n            0..=300 => RateLimitAction::SwapNow { \n                to_account: alternatives.first()\n                    .map(|(a, _)| a.clone())\n                    .unwrap_or_default() \n            },\n            301..=600 => RateLimitAction::PrepareSwap { in_minutes: (time_to_limit.as_secs() / 60) as u32 },\n            601..=1800 if velocity > 1.0 => RateLimitAction::SlowDown { target_velocity: velocity * 0.7 },\n            _ => RateLimitAction::Continue,\n        };\n        \n        RateLimitForecast {\n            provider: key.provider.clone(),\n            account: key.account.clone(),\n            current_usage_pct: current.used_percent,\n            current_velocity: velocity,\n            time_to_limit,\n            confidence,\n            recommended_action: action,\n            optimal_swap_time: calculate_optimal_swap_time(time_to_limit, &alternatives),\n            alternative_accounts: alternatives,\n        }\n    }\n}\n\nfn calculate_velocity(samples: &[UsageSample]) -> f64 {\n    if samples.len() < 2 {\n        return 0.0;\n    }\n    \n    // Linear regression on usage over time\n    // Return slope (% per minute)\n    let n = samples.len() as f64;\n    let sum_x: f64 = samples.iter().enumerate().map(|(i, _)| i as f64).sum();\n    let sum_y: f64 = samples.iter().map(|s| s.used_percent).sum();\n    let sum_xy: f64 = samples.iter().enumerate().map(|(i, s)| i as f64 * s.used_percent).sum();\n    let sum_xx: f64 = samples.iter().enumerate().map(|(i, _)| (i * i) as f64).sum();\n    \n    let slope = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x);\n    \n    // Convert to per-minute (assuming samples are ~2 min apart)\n    slope / 2.0\n}\n\nfn calculate_confidence(samples: &[UsageSample], velocity: f64) -> f64 {\n    // Higher confidence with:\n    // - More samples\n    // - More consistent velocity\n    // - Recent data\n    \n    let sample_factor = (samples.len() as f64 / 10.0).min(1.0);\n    \n    // Calculate velocity variance\n    let velocities: Vec<f64> = samples.windows(2)\n        .map(|w| w[1].used_percent - w[0].used_percent)\n        .collect();\n    let variance = calculate_variance(&velocities);\n    let consistency_factor = 1.0 / (1.0 + variance);\n    \n    // Recency factor\n    let last_sample = samples.last().unwrap();\n    let age_minutes = (Utc::now() - last_sample.collected_at).num_minutes() as f64;\n    let recency_factor = 1.0 / (1.0 + age_minutes / 10.0);\n    \n    (sample_factor * consistency_factor * recency_factor).clamp(0.1, 0.99)\n}\n```\n\n## DuckDB Table for Predictions\n```sql\nCREATE TABLE IF NOT EXISTS predictions(\n    machine_id TEXT,\n    generated_at TIMESTAMP,\n    prediction_type TEXT,\n    horizon_minutes INTEGER,\n    confidence REAL,\n    details_json TEXT\n);\n```\n\n## Robot Output\n```json\n{\n  \"schema_version\": \"vc.robot.oracle.v1\",\n  \"forecasts\": [\n    {\n      \"provider\": \"claude\",\n      \"account\": \"jeff@email.com\",\n      \"current_usage_pct\": 85.0,\n      \"velocity\": 2.5,\n      \"time_to_limit_minutes\": 6,\n      \"confidence\": 0.82,\n      \"action\": \"swap_now\",\n      \"alternative\": \"backup@email.com\"\n    }\n  ]\n}\n```\n\n## Acceptance Criteria\n- [ ] Velocity calculation from usage history\n- [ ] Time-to-limit prediction\n- [ ] Alternative account ranking\n- [ ] Confidence scoring\n- [ ] Robot oracle command returns forecasts\n\n## Notes for Future Self\n- Consider exponential smoothing for velocity\n- Add hysteresis to prevent action flapping\n- Historical pattern matching could improve accuracy\n- This is the foundation for autopilot account swaps\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:25:45.883254519Z","created_by":"ubuntu","updated_at":"2026-01-28T08:39:50.914889395Z","closed_at":"2026-01-28T08:39:50.914866902Z","close_reason":"Implemented rate limit forecasting module with velocity calculation, time-to-limit prediction, confidence scoring, and recommended actions","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2mv.5","depends_on_id":"bd-2mv","type":"parent-child","created_at":"2026-01-27T16:25:45.883254519Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.5","depends_on_id":"bd-2mv.1","type":"blocks","created_at":"2026-01-27T16:26:53.737171492Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.5","depends_on_id":"bd-2mv.2","type":"blocks","created_at":"2026-01-27T16:26:53.874751248Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2mv.6","title":"Implement TUI Accounts & Sessions screens","description":"## Overview\nAdd TUI screens for accounts, sessions, and mail data visualization.\n\n## Background\nWith caut, caam, cass, and mcp_agent_mail collectors in place, users need visual interfaces to monitor account usage, active sessions, and agent mail traffic.\n\n## Screens to Implement\n\n### 1. Accounts Screen (vc_tui/src/screens/accounts.rs)\n- Table with columns: Program | Account | Usage | Limit | % | Status | Last Switch\n- Color-coded by rate_status (green/yellow/red)\n- Sparkline showing 24h usage trend\n\n### 2. Sessions Screen (vc_tui/src/screens/sessions.rs)\n- Hierarchical tree view when grouped by project/model\n- Table view when ungrouped\n- Session detail pane showing tokens and cost\n\n### 3. Agent Mail Screen (vc_tui/src/screens/mail.rs)\n- Left pane: thread list with unacked counts\n- Right pane: message timeline for selected thread\n- Agent activity heatmap at bottom\n\n## Key Interactions\n- Tab to switch screens, j/k for nav, Enter for details\n- / for search, s for sort, g for grouping\n\n## Acceptance Criteria\n- [ ] Accounts screen shows all tracked accounts with usage stats\n- [ ] Sessions screen supports grouping by project or model\n- [ ] Mail screen shows thread summaries with unacked counts\n- [ ] All screens support vim-style navigation\n\n## Testing & Logging\n- View-model unit tests for filtering, sorting, and state transitions.\n- Render snapshot tests using ratatui TestBackend for key layouts.\n- Navigation/keybinding tests (tab/enter/filter/search).\n- E2E: tests/e2e/tui/test_<screen>.sh with expect/pexpect; capture logs and screenshots on failure.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:27:29.140336829Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:18.706346634Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2mv.6","depends_on_id":"bd-2mv","type":"parent-child","created_at":"2026-01-27T16:27:29.140336829Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.6","depends_on_id":"bd-2mv.1","type":"blocks","created_at":"2026-01-27T16:27:34.820799738Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.6","depends_on_id":"bd-2mv.2","type":"blocks","created_at":"2026-01-27T16:27:34.940551124Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.6","depends_on_id":"bd-2mv.3","type":"blocks","created_at":"2026-01-27T16:27:35.063284357Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.6","depends_on_id":"bd-2mv.4","type":"blocks","created_at":"2026-01-27T16:27:35.190647260Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.6","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:18.705909500Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2mv.6","depends_on_id":"bd-33h.3","type":"blocks","created_at":"2026-01-27T16:27:34.681264519Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2v3","title":"Implement incident management system","description":"Implement incident tracking and management for correlating events and enabling post-incident review.\n\nINCIDENT MODEL:\nAn incident is a bounded time range with a root cause hypothesis and a reconstructed timeline. Used for post-mortem analysis and pattern learning.\n\nDUCKDB TABLES (already defined in schema):\n- incidents: Core incident record with status, severity, title\n- incident_timeline_events: Correlated events from multiple sources\n- incident_notes: Human/agent annotations during incident\n\nINCIDENT LIFECYCLE:\n1. AUTO-DETECTED: Created when multiple alerts correlate\n2. OPEN: Active investigation\n3. MITIGATED: Immediate issue resolved\n4. CLOSED: Root cause identified, postmortem complete\n\nINCIDENT CREATION:\n- Auto-create when multiple related alerts fire within time window\n- Manual creation via TUI or CLI\n- Guardian can create incidents during healing failures\n\nCLI COMMANDS:\nvc incident list                    # List recent incidents\nvc incident show <id>               # Show incident details + timeline\nvc incident create --title \"...\"   # Create manual incident\nvc incident note <id> \"...\"        # Add note to incident\nvc incident close <id> --reason \"...\"  # Close with resolution\nvc incident timeline <id>           # Show correlated events\n\nTIMELINE RECONSTRUCTION:\n- Collect all events within incident time window\n- Correlate across sources: alerts, dcg, rano, pt, collectors\n- Order by timestamp\n- Identify probable root cause\n- Store in incident_timeline_events\n\nINCIDENT AUTO-DETECTION RULES:\n- 3+ alerts on same machine within 5 minutes\n- Guardian healing failure\n- Rate limit hit across multiple accounts\n- System crash or OOM detected\n\nTUI INTEGRATION:\n- Add incident indicator to alert view\n- Link alerts to their incidents\n- Quick-create incident from alert context\n\nKNOWLEDGE INTEGRATION:\n- Failed incidents inform gotcha database\n- Successful resolutions inform playbooks\n- Pattern matching against past incidents\n\nTESTS:\n- Incident creation tests\n- Timeline correlation tests\n- Auto-detection rule tests\n- Lifecycle transition tests\n\n## E2E & Logging\n- Add `tests/e2e/system/test_incident_flow.sh`: inject alerts, auto-create incident, add notes, close, verify timeline.\n- Capture DuckDB snapshots and JSON summaries in tests/logs/.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:51:05.891062444Z","created_by":"ubuntu","updated_at":"2026-01-27T22:17:53.116170314Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2v3","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T22:17:53.115690630Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2v3","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T16:54:36.085287612Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2v3","depends_on_id":"bd-2xm.6","type":"blocks","created_at":"2026-01-27T17:03:58.607470663Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2v3","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:19.263042704Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2v3","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T16:55:50.908732096Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2v3","depends_on_id":"bd-3e2.4","type":"blocks","created_at":"2026-01-27T17:03:59.419047360Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xm","title":"Phase 4: rch + rano + dcg + pt + Alerts + Guardian","description":"# Phase 4: rch + rano + dcg + pt + Alerts + Guardian MVP\n\n## Purpose\nComplete the collector suite with specialized tools (rch, rano, dcg, pt) and implement the alert and self-healing systems. This phase adds the reactive and proactive response capabilities.\n\n## Key Deliverables\n1. **rch Collector**: Remote compilation status via JSON or Prometheus metrics scrape\n2. **rano Collector**: Network activity via `rano export --format jsonl --since <duration>`\n3. **dcg Collector**: Blocked commands via JSONL tail of history file\n4. **pt Collector**: Process triage findings via `pt robot plan --format json`\n5. **Alerts MVP**: Threshold-based rules with persistence and state management\n6. **Guardian MVP**: Self-healing protocols for common issues\n\n## Success Criteria\n- All 15 source systems have working collectors\n- Alert rules fire correctly on thresholds\n- Guardian protocols can suggest (and optionally execute) remediation\n- Audit log captures all automated actions\n\n## Technical Context\n\n### rch Integration Options\nTwo approaches based on rch deployment:\n1. Prometheus scrape: `curl localhost:9100/metrics` then parse metric lines\n2. JSON status: `rch status --json` if available\n\nPrometheus is preferred for dashboards (continuous samples), JSON for simpler polling.\n\n### dcg JSONL Tail Pattern\ndcg writes to `~/.config/dcg/history.jsonl`:\n- Maintain cursor: (inode, offset)\n- On rotation (inode changed): reset to 0 or scan last N minutes\n- Parse new lines as dcg events\n\nThis is the \"JSONL Tail\" ingestion pattern from the collector architecture.\n\n### Alert Rules MVP\nConcrete thresholds from the plan:\n\n**System:**\n| Rule | Threshold |\n|------|-----------|\n| Disk critical | < 10% free |\n| Load high | load5 > cores × 1.5 for > 10 min |\n| Swap pressure | > 0 and rising |\n\n**Tooling:**\n| Rule | Threshold |\n|------|-----------|\n| DCG denies spike | > 10/hour or new critical rules |\n| Mail backlog | > 5 urgent unread |\n| Mail overdue | > 3 ack_required unacked > 1 hour |\n| RCH queue saturated | queue > 10, workers = 0 |\n\n### Guardian MVP - Healing Protocols\nEach protocol follows the structured pattern:\n```rust\nHealingProtocol {\n    name: \"rate_limit_recovery\",\n    trigger: |state| state.usage_pct > 85,\n    diagnosis: vec![check_velocity, forecast_limit, find_alternatives],\n    treatment: vec![throttle, prepare_swap, execute_swap_if_urgent],\n    verification: confirm_new_account_healthy,\n    rollback: Some(revert_to_previous),\n    escalation: after_3_failures(alert_human),\n}\n```\n\nMVP protocols:\n1. Rate limit recovery\n2. Stuck agent recovery\n3. Disk space recovery\n4. High load mitigation\n\n## Dependencies\n- Requires Phase 3 (remote execution working across fleet)\n\n## Estimated Scope\n- ~2-3 weeks\n- Major milestone: vc becomes autonomous (with guardrails)\n\n## Testing & E2E Expectations\n- Every deliverable in this phase must add unit tests per bd-1x9.\n- Extend bd-30z with phase-specific e2e scenarios and structured logs.\n- Phase completion requires `cargo test --workspace` + phase e2e suite with JSON summaries in tests/logs/.\n","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-27T16:18:16.039396433Z","created_by":"ubuntu","updated_at":"2026-01-27T17:02:18.382718832Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xm","depends_on_id":"bd-3nb","type":"blocks","created_at":"2026-01-27T16:19:04.327866041Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xm.1","title":"Implement rch (remote compilation helper) collector","description":"## Overview\nImplement collector for rch (Remote Compilation Helper) - tracks Rust compilation offloading to powerful remote machines.\n\n## Background\nrch offloads cargo builds to Mac Minis or cloud VMs. Tracking compilation metrics helps identify slow crates, cache efficiency, and optimal worker allocation.\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE rch_compilations (\n    id INTEGER PRIMARY KEY,\n    machine_id VARCHAR NOT NULL,          -- Which vc machine ran this\n    worker_host VARCHAR,                  -- Which rch worker compiled\n    crate_name VARCHAR NOT NULL,\n    crate_version VARCHAR,\n    profile VARCHAR,                      -- debug, release\n    target_triple VARCHAR,\n    started_at TIMESTAMP NOT NULL,\n    completed_at TIMESTAMP,\n    duration_ms INTEGER,\n    cache_hit BOOLEAN DEFAULT FALSE,\n    cache_key VARCHAR,\n    exit_code INTEGER,\n    error_msg VARCHAR,\n    cpu_time_ms INTEGER,\n    peak_memory_mb INTEGER,\n    collected_at TIMESTAMP DEFAULT now()\n);\n\nCREATE TABLE rch_workers (\n    worker_id VARCHAR PRIMARY KEY,\n    hostname VARCHAR,\n    status VARCHAR,                       -- idle, busy, offline\n    current_job VARCHAR,\n    jobs_completed INTEGER,\n    total_cpu_time_ms BIGINT,\n    last_heartbeat TIMESTAMP,\n    collected_at TIMESTAMP DEFAULT now()\n);\n\n-- Indexes for common queries\nCREATE INDEX idx_rch_crate ON rch_compilations(crate_name);\nCREATE INDEX idx_rch_started ON rch_compilations(started_at);\nCREATE INDEX idx_rch_worker ON rch_compilations(worker_host);\n```\n\n## Ingestion Pattern: JSONL Tail\nrch writes to `~/.rch/compilations.jsonl`:\n```json\n{\"ts\":\"2025-01-27T10:00:00Z\",\"crate\":\"serde\",\"version\":\"1.0.200\",\"profile\":\"release\",\"duration_ms\":12340,\"cache_hit\":false,\"worker\":\"mini-1\"}\n```\n\n## Collector Implementation\n```rust\npub struct RchCollector {\n    jsonl_path: PathBuf,\n}\n\nimpl Collector for RchCollector {\n    const NAME: &str = \"rch\";\n    const TOOL_NAME: &str = \"rch\";\n    type Cursor = FileOffset;\n    \n    async fn collect(&self, ctx: &CollectContext) -> Result<CollectResult> {\n        let offset = ctx.cursor.unwrap_or(0);\n        \n        let file = File::open(&self.jsonl_path)?;\n        file.seek(SeekFrom::Start(offset))?;\n        \n        let mut records = Vec::new();\n        let mut new_offset = offset;\n        \n        for line in BufReader::new(file).lines() {\n            let line = line?;\n            new_offset += line.len() as u64 + 1;  // +1 for newline\n            \n            let record: RchCompilation = serde_json::from_str(&line)?;\n            records.push(record);\n        }\n        \n        Ok(CollectResult {\n            table: \"rch_compilations\",\n            records: records.into_iter().map(|r| r.to_row()).collect(),\n            new_cursor: Some(Cursor::FileOffset(new_offset)),\n            machine_id: ctx.machine_id.clone(),\n        })\n    }\n}\n\n#[derive(Deserialize)]\nstruct RchCompilation {\n    ts: DateTime<Utc>,\n    crate_name: String,\n    version: Option<String>,\n    profile: String,\n    duration_ms: u64,\n    cache_hit: bool,\n    worker: String,\n    // ... more fields\n}\n```\n\n## Key Metrics & Queries\n```sql\n-- Average compile time by crate (last 24h)\nSELECT crate_name, \n       AVG(duration_ms) as avg_ms,\n       COUNT(*) as compiles,\n       SUM(CASE WHEN cache_hit THEN 1 ELSE 0 END)::FLOAT / COUNT(*) as cache_rate\nFROM rch_compilations\nWHERE started_at > now() - INTERVAL 1 DAY\nGROUP BY crate_name\nORDER BY avg_ms DESC\nLIMIT 20;\n\n-- Worker utilization\nSELECT worker_host,\n       COUNT(*) as jobs,\n       SUM(duration_ms) / 1000.0 as total_seconds,\n       AVG(duration_ms) as avg_ms\nFROM rch_compilations\nWHERE started_at > now() - INTERVAL 1 HOUR\nGROUP BY worker_host;\n\n-- Cache hit rate over time\nSELECT date_trunc(\"hour\", started_at) as hour,\n       SUM(CASE WHEN cache_hit THEN 1 ELSE 0 END)::FLOAT / COUNT(*) as cache_rate\nFROM rch_compilations\nWHERE started_at > now() - INTERVAL 7 DAY\nGROUP BY 1 ORDER BY 1;\n```\n\n## Acceptance Criteria\n- [ ] JSONL tail collector reads rch compilation logs\n- [ ] Compilations stored with machine_id context\n- [ ] Worker status table maintained\n- [ ] Cache hit rate metrics available\n- [ ] Per-crate build time tracking works\n- [ ] Works on local and remote machines\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n\n## Remote Compatibility\n- Collector must run locally and via SSH runner once bd-3nb.4 exists.\n- Route execution through the CollectContext executor abstraction (local or remote).\n- Add a remote-path test (localhost or mock) and hook into `tests/e2e/remote/test_remote_collectors.sh` (bd-30z).\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:30:01.879433379Z","created_by":"ubuntu","updated_at":"2026-01-27T18:23:11.232923036Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xm.1","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T18:22:21.375397632Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.1","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T18:22:22.004569797Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.1","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T18:22:22.605244019Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.1","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:30:01.879433379Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xm.2","title":"Implement rano (network observer) collector","description":"## Overview\nImplement collector for rano - network activity observer for coding agents. This captures connections, domains, processes, and bandwidth so vc can surface anomalous traffic, auth loops, and provider-level issues.\n\n## Integration Methods\n- Primary: `rano export --format jsonl --since <duration>`\n- Optional fallback: read `observer.sqlite` directly if export is unavailable\n- Incremental cursor: track last event timestamp (and event_id if present) to avoid re-ingest\n\n## DuckDB Tables\n```sql\nCREATE TABLE net_events (\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    event_ts TIMESTAMP,\n    provider TEXT,\n    process_name TEXT,\n    pid INTEGER,\n    direction TEXT,          -- inbound | outbound\n    protocol TEXT,           -- tcp | udp | https | ...\n    remote_host TEXT,\n    remote_ip TEXT,\n    remote_port INTEGER,\n    bytes_sent BIGINT,\n    bytes_received BIGINT,\n    tags_json TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, event_ts, process_name, remote_host)\n);\n\nCREATE TABLE net_session_summary (\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    provider TEXT,\n    process_name TEXT,\n    session_count INTEGER,\n    bytes_sent BIGINT,\n    bytes_received BIGINT,\n    error_count INTEGER,\n    top_remote_hosts_json TEXT,\n    PRIMARY KEY (machine_id, collected_at, provider, process_name)\n);\n\nCREATE TABLE net_anomalies (\n    machine_id TEXT,\n    detected_at TIMESTAMP,\n    anomaly_type TEXT,       -- unknown_provider | spike | auth_loop | burst\n    severity TEXT,           -- info | warning | critical\n    description TEXT,\n    evidence_json TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, detected_at, anomaly_type)\n);\n```\n\n## Collector Implementation\n- Run export with a bounded window (default 10m) and parse JSONL line-by-line.\n- Normalize provider/domain/process fields for stable aggregation.\n- Store raw_json for forward compatibility.\n- Fail-soft when rano is missing; mark collector_available=false and continue.\n\n## Alert + UI Value\n- Populate Events screen network section.\n- Alert rules: unknown provider spike, auth-loop detection, bandwidth anomalies.\n\n## Testing & Logging\nUnit tests:\n- Parse representative JSONL fixtures (happy path + malformed/partial lines).\n- Normalize provider/domain/process fields.\n- Error-path tests: missing binary, empty export, invalid JSONL.\n\nIntegration tests:\n- Insert into DuckDB and query back net_events/net_session_summary.\n- Cursor behavior: no duplicates across consecutive runs.\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_rano_collect.sh` (see bd-30z).\n- Script emits structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- JSON summary per run stored in `tests/logs/`.\n\n## Remote Compatibility\n- Collector must run locally and via SSH runner once bd-3nb.4 exists.\n- Route execution through the CollectContext executor abstraction (local or remote).\n- Add a remote-path test (localhost or mock) and hook into `tests/e2e/remote/test_remote_collectors.sh` (bd-30z).\n","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-01-27T16:30:23.255269137Z","created_by":"ubuntu","updated_at":"2026-01-28T17:02:15.375958647Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xm.2","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T18:22:23.236880018Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.2","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T18:22:23.865795349Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.2","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T18:22:24.564636017Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.2","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:30:23.255269137Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xm.3","title":"Implement dcg (dangerous command guard) collector","description":"## Overview\nImplement collector for dcg (Dangerous Command Guard) - tracks blocked dangerous commands and security events.\n\n## Background\ndcg intercepts and blocks dangerous shell commands (rm -rf /, git push --force, etc.). Tracking blocked commands helps understand agent behavior patterns and identify potential issues.\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE dcg_events (\n    id INTEGER PRIMARY KEY,\n    machine_id VARCHAR NOT NULL,\n    timestamp TIMESTAMP NOT NULL,\n    event_type VARCHAR NOT NULL,          -- blocked, allowed_override, config_change\n    command VARCHAR NOT NULL,\n    working_dir VARCHAR,\n    user VARCHAR,\n    shell_pid INTEGER,\n    parent_process VARCHAR,               -- claude-code, codex, bash, etc.\n    rule_matched VARCHAR,                 -- Which dcg rule triggered\n    severity VARCHAR,                     -- low, medium, high, critical\n    override_reason VARCHAR,              -- If allowed after prompt\n    session_id VARCHAR,                   -- Link to cass session if available\n    collected_at TIMESTAMP DEFAULT now()\n);\n\nCREATE TABLE dcg_rules (\n    rule_id VARCHAR PRIMARY KEY,\n    machine_id VARCHAR NOT NULL,\n    pattern VARCHAR NOT NULL,             -- Regex or glob pattern\n    action VARCHAR NOT NULL,              -- block, warn, log\n    severity VARCHAR,\n    description VARCHAR,\n    enabled BOOLEAN DEFAULT TRUE,\n    created_at TIMESTAMP,\n    collected_at TIMESTAMP DEFAULT now()\n);\n\n-- Indexes\nCREATE INDEX idx_dcg_ts ON dcg_events(timestamp);\nCREATE INDEX idx_dcg_type ON dcg_events(event_type);\nCREATE INDEX idx_dcg_severity ON dcg_events(severity);\n```\n\n## Ingestion Pattern: SQLite Incremental\ndcg stores events in `~/.dcg/events.db`:\n```sql\n-- dcg native schema\nCREATE TABLE events (\n    id INTEGER PRIMARY KEY,\n    ts TEXT,\n    type TEXT,\n    cmd TEXT,\n    cwd TEXT,\n    rule TEXT,\n    severity TEXT,\n    ...\n);\n```\n\n## Collector Implementation\n```rust\npub struct DcgCollector {\n    db_path: PathBuf,\n}\n\nimpl Collector for DcgCollector {\n    const NAME: &str = \"dcg\";\n    const TOOL_NAME: &str = \"dcg\";\n    type Cursor = i64;  // Last event ID\n    \n    async fn collect(&self, ctx: &CollectContext) -> Result<CollectResult> {\n        let conn = rusqlite::Connection::open(&self.db_path)?;\n        let last_id = ctx.cursor.unwrap_or(0);\n        \n        let mut stmt = conn.prepare(\n            \"SELECT id, ts, type, cmd, cwd, rule, severity, override_reason\n             FROM events \n             WHERE id > ?\n             ORDER BY id\"\n        )?;\n        \n        let events: Vec<DcgEvent> = stmt.query_map([last_id], |row| {\n            Ok(DcgEvent {\n                id: row.get(0)?,\n                timestamp: row.get(1)?,\n                event_type: row.get(2)?,\n                command: row.get(3)?,\n                working_dir: row.get(4)?,\n                rule_matched: row.get(5)?,\n                severity: row.get(6)?,\n                override_reason: row.get(7)?,\n            })\n        })?.collect::<Result<Vec<_>, _>>()?;\n        \n        let new_cursor = events.last().map(|e| e.id).unwrap_or(last_id);\n        \n        Ok(CollectResult {\n            table: \"dcg_events\",\n            records: events.into_iter().map(|e| e.to_row()).collect(),\n            new_cursor: Some(Cursor::Integer(new_cursor)),\n            machine_id: ctx.machine_id.clone(),\n        })\n    }\n}\n```\n\n## Key Metrics & Queries\n```sql\n-- Blocked commands by severity (last 24h)\nSELECT severity, \n       COUNT(*) as count,\n       array_agg(DISTINCT rule_matched) as rules\nFROM dcg_events\nWHERE event_type = \"blocked\"\n  AND timestamp > now() - INTERVAL 1 DAY\nGROUP BY severity\nORDER BY \n    CASE severity WHEN \"critical\" THEN 1 WHEN \"high\" THEN 2 WHEN \"medium\" THEN 3 ELSE 4 END;\n\n-- Most frequently blocked commands\nSELECT command, \n       COUNT(*) as block_count,\n       MIN(timestamp) as first_blocked,\n       MAX(timestamp) as last_blocked\nFROM dcg_events\nWHERE event_type = \"blocked\"\nGROUP BY command\nORDER BY block_count DESC\nLIMIT 20;\n\n-- Blocks by parent process (agent detection)\nSELECT parent_process,\n       COUNT(*) as blocks,\n       COUNT(DISTINCT session_id) as sessions\nFROM dcg_events\nWHERE event_type = \"blocked\"\n  AND timestamp > now() - INTERVAL 7 DAY\nGROUP BY parent_process\nORDER BY blocks DESC;\n\n-- Override patterns (which dangerous commands were allowed)\nSELECT command, override_reason, COUNT(*) as times\nFROM dcg_events\nWHERE event_type = \"allowed_override\"\nGROUP BY command, override_reason\nORDER BY times DESC;\n```\n\n## Guardian Integration\ndcg events can trigger Guardian alerts:\n```rust\n// In vc_guardian\nimpl GuardianRule for DcgCriticalRule {\n    fn check(&self, store: &VcStore) -> Option<Alert> {\n        let critical_count = store.query(\n            \"SELECT COUNT(*) FROM dcg_events \n             WHERE severity = \\\"critical\\\" \n             AND timestamp > now() - INTERVAL 5 MINUTE\"\n        )?;\n        \n        if critical_count > 0 {\n            Some(Alert::new(\n                AlertLevel::Critical,\n                \"Critical command blocked by dcg\",\n                \"Check dcg_events for details\"\n            ))\n        } else {\n            None\n        }\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] SQLite incremental collector reads dcg events\n- [ ] Events stored with machine_id and session linkage\n- [ ] Severity classification preserved\n- [ ] Override reasons captured for allowed commands\n- [ ] Parent process tracking for agent attribution\n- [ ] Works on local and remote machines\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n\n## Remote Compatibility\n- Collector must run locally and via SSH runner once bd-3nb.4 exists.\n- Route execution through the CollectContext executor abstraction (local or remote).\n- Add a remote-path test (localhost or mock) and hook into `tests/e2e/remote/test_remote_collectors.sh` (bd-30z).\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:30:46.183958190Z","created_by":"ubuntu","updated_at":"2026-01-28T18:09:54.681679701Z","closed_at":"2026-01-28T18:09:54.680869705Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xm.3","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T18:22:25.146674904Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.3","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T18:22:25.749468932Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.3","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T18:22:26.368733363Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.3","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:30:46.183958190Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xm.4","title":"Implement pt (process tracker) collector","description":"## Overview\nImplement collector for pt (Process Tracker) - tracks long-running processes and their resource usage.\n\n## Background\npt monitors running processes, especially agent sessions and build processes. It provides process lifecycle events, resource usage over time, and parent-child relationships.\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE pt_processes (\n    id INTEGER PRIMARY KEY,\n    machine_id VARCHAR NOT NULL,\n    pid INTEGER NOT NULL,\n    ppid INTEGER,                         -- Parent PID\n    name VARCHAR NOT NULL,\n    cmdline VARCHAR,\n    user VARCHAR,\n    started_at TIMESTAMP,\n    ended_at TIMESTAMP,\n    exit_code INTEGER,\n    cpu_time_ms BIGINT,\n    peak_memory_mb INTEGER,\n    status VARCHAR,                       -- running, stopped, zombie\n    category VARCHAR,                     -- agent, build, system, other\n    session_id VARCHAR,                   -- Link to cass session\n    collected_at TIMESTAMP DEFAULT now(),\n    UNIQUE (machine_id, pid, started_at)\n);\n\nCREATE TABLE pt_snapshots (\n    id INTEGER PRIMARY KEY,\n    machine_id VARCHAR NOT NULL,\n    pid INTEGER NOT NULL,\n    snapshot_at TIMESTAMP NOT NULL,\n    cpu_percent FLOAT,\n    memory_mb FLOAT,\n    memory_percent FLOAT,\n    threads INTEGER,\n    open_files INTEGER,\n    io_read_bytes BIGINT,\n    io_write_bytes BIGINT,\n    collected_at TIMESTAMP DEFAULT now()\n);\n\n-- Process tree relationships\nCREATE TABLE pt_tree (\n    machine_id VARCHAR NOT NULL,\n    parent_pid INTEGER NOT NULL,\n    child_pid INTEGER NOT NULL,\n    depth INTEGER,                        -- Distance from root\n    collected_at TIMESTAMP DEFAULT now(),\n    PRIMARY KEY (machine_id, parent_pid, child_pid)\n);\n\n-- Indexes\nCREATE INDEX idx_pt_proc_machine ON pt_processes(machine_id, started_at);\nCREATE INDEX idx_pt_proc_category ON pt_processes(category);\nCREATE INDEX idx_pt_snap_ts ON pt_snapshots(snapshot_at);\n```\n\n## Ingestion Pattern: CLI Incremental Window\npt provides snapshots via `pt list --robot --json --since <timestamp>`:\n```json\n{\n  \"processes\": [\n    {\n      \"pid\": 12345,\n      \"ppid\": 1234,\n      \"name\": \"claude-code\",\n      \"cmdline\": \"claude-code --project /data/projects/vc\",\n      \"started_at\": \"2025-01-27T10:00:00Z\",\n      \"cpu_percent\": 45.2,\n      \"memory_mb\": 512.3,\n      \"category\": \"agent\"\n    }\n  ],\n  \"ended\": [\n    {\"pid\": 12340, \"exit_code\": 0, \"ended_at\": \"2025-01-27T10:05:00Z\"}\n  ]\n}\n```\n\n## Collector Implementation\n```rust\npub struct PtCollector;\n\nimpl Collector for PtCollector {\n    const NAME: &str = \"pt\";\n    const TOOL_NAME: &str = \"pt\";\n    type Cursor = DateTime<Utc>;\n    \n    async fn collect(&self, ctx: &CollectContext) -> Result<CollectResult> {\n        let since = ctx.cursor\n            .unwrap_or_else(|| Utc::now() - chrono::Duration::hours(1));\n        \n        let output = Command::new(\"pt\")\n            .args([\"list\", \"--robot\", \"--json\", \"--since\", &since.to_rfc3339()])\n            .output()\n            .await?;\n        \n        let data: PtOutput = serde_json::from_slice(&output.stdout)?;\n        \n        let mut result = CollectResult::new();\n        \n        // Active processes - upsert\n        for proc in data.processes {\n            result.add_rows(\"pt_processes\", vec![proc.to_row()]);\n            \n            // Snapshot current resource usage\n            result.add_rows(\"pt_snapshots\", vec![\n                row!(\n                    machine_id = ctx.machine_id,\n                    pid = proc.pid,\n                    snapshot_at = Utc::now(),\n                    cpu_percent = proc.cpu_percent,\n                    memory_mb = proc.memory_mb,\n                    // ...\n                )\n            ]);\n        }\n        \n        // Ended processes - update\n        for ended in data.ended {\n            result.add_update(\"pt_processes\",\n                Update::set(\"ended_at\", ended.ended_at)\n                    .set(\"exit_code\", ended.exit_code)\n                    .where_eq(\"machine_id\", ctx.machine_id)\n                    .where_eq(\"pid\", ended.pid)\n            );\n        }\n        \n        result.new_cursor = Some(Cursor::Timestamp(Utc::now()));\n        Ok(result)\n    }\n}\n```\n\n## Process Categorization\n```rust\nfn categorize_process(name: &str, cmdline: &str) -> &str {\n    if name.contains(\"claude\") || name.contains(\"codex\") || name.contains(\"gemini\") {\n        \"agent\"\n    } else if name.contains(\"cargo\") || name.contains(\"rustc\") || name.contains(\"npm\") {\n        \"build\"\n    } else if name.contains(\"node\") || name.contains(\"python\") || name.contains(\"ruby\") {\n        \"runtime\"\n    } else {\n        \"other\"\n    }\n}\n```\n\n## Key Queries\n```sql\n-- Active agent processes\nSELECT machine_id, pid, name, cmdline, \n       started_at,\n       age(now(), started_at) as runtime\nFROM pt_processes\nWHERE category = \"agent\"\n  AND ended_at IS NULL\nORDER BY started_at DESC;\n\n-- Resource usage over time for a process\nSELECT snapshot_at, cpu_percent, memory_mb\nFROM pt_snapshots\nWHERE machine_id = ? AND pid = ?\nORDER BY snapshot_at;\n\n-- Process tree for an agent\nWITH RECURSIVE tree AS (\n    SELECT pid, name, 0 as depth\n    FROM pt_processes WHERE pid = ?\n    UNION ALL\n    SELECT p.pid, p.name, t.depth + 1\n    FROM pt_processes p\n    JOIN tree t ON p.ppid = t.pid\n)\nSELECT * FROM tree ORDER BY depth, pid;\n\n-- Processes by resource usage\nSELECT machine_id, pid, name, \n       MAX(cpu_percent) as peak_cpu,\n       MAX(memory_mb) as peak_mem\nFROM pt_snapshots\nWHERE snapshot_at > now() - INTERVAL 1 HOUR\nGROUP BY machine_id, pid, name\nORDER BY peak_cpu DESC\nLIMIT 20;\n```\n\n## Acceptance Criteria\n- [ ] CLI incremental collector polls pt list\n- [ ] Process lifecycle (start/end) tracked\n- [ ] Resource snapshots stored periodically\n- [ ] Process tree relationships captured\n- [ ] Category auto-detection for agents/builds\n- [ ] Session linkage when available\n- [ ] Works on local and remote machines\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n\n## Remote Compatibility\n- Collector must run locally and via SSH runner once bd-3nb.4 exists.\n- Route execution through the CollectContext executor abstraction (local or remote).\n- Add a remote-path test (localhost or mock) and hook into `tests/e2e/remote/test_remote_collectors.sh` (bd-30z).\n","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-01-27T16:31:10.725493763Z","created_by":"ubuntu","updated_at":"2026-01-28T17:30:10.269680179Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xm.4","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T18:22:26.982820175Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.4","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T18:22:27.573105335Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.4","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T18:22:28.131872289Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.4","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:31:10.725493763Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xm.5","title":"Implement Alerts MVP","description":"## Overview\nBuild the alerting system that monitors collected data and triggers notifications when conditions are met.\n\n## Background\nWith multiple collectors gathering data, we need an alerting system to notify users of important events: rate limits approaching, critical commands blocked, agents stuck, etc.\n\n## Architecture\n```\n┌─────────────────────────────────────────────────────┐\n│                   AlertEngine                        │\n├─────────────────────────────────────────────────────┤\n│  Rules[]     → Check conditions against store        │\n│  History[]   → Track fired alerts (dedup)           │\n│  Channels[]  → Deliver via TUI/webhook/desktop      │\n└─────────────────────────────────────────────────────┘\n```\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE alert_rules (\n    rule_id VARCHAR PRIMARY KEY,\n    name VARCHAR NOT NULL,\n    description VARCHAR,\n    severity VARCHAR NOT NULL,            -- info, warning, critical\n    enabled BOOLEAN DEFAULT TRUE,\n    check_interval_seconds INTEGER DEFAULT 60,\n    condition_type VARCHAR NOT NULL,      -- threshold, pattern, absence\n    condition_config JSON NOT NULL,       -- Rule-specific params\n    cooldown_seconds INTEGER DEFAULT 300, -- Dedup window\n    channels VARCHAR[],                   -- Where to send\n    created_at TIMESTAMP DEFAULT now(),\n    updated_at TIMESTAMP\n);\n\nCREATE TABLE alert_history (\n    id INTEGER PRIMARY KEY,\n    rule_id VARCHAR REFERENCES alert_rules(rule_id),\n    fired_at TIMESTAMP NOT NULL,\n    resolved_at TIMESTAMP,\n    severity VARCHAR NOT NULL,\n    title VARCHAR NOT NULL,\n    message TEXT,\n    context JSON,                         -- Additional data\n    machine_id VARCHAR,\n    acknowledged BOOLEAN DEFAULT FALSE,\n    acknowledged_by VARCHAR,\n    acknowledged_at TIMESTAMP\n);\n\nCREATE TABLE alert_channels (\n    channel_id VARCHAR PRIMARY KEY,\n    channel_type VARCHAR NOT NULL,        -- tui, webhook, desktop, email\n    config JSON NOT NULL,                 -- Type-specific config\n    enabled BOOLEAN DEFAULT TRUE\n);\n```\n\n## Alert Rule Types\n```rust\n#[derive(Debug, Serialize, Deserialize)]\npub enum AlertCondition {\n    /// Fire when metric exceeds threshold\n    Threshold {\n        query: String,           // SQL returning single value\n        operator: ThresholdOp,   // gt, gte, lt, lte, eq\n        value: f64,\n    },\n    \n    /// Fire when pattern appears in data\n    Pattern {\n        table: String,\n        column: String,\n        regex: String,\n    },\n    \n    /// Fire when expected data is missing\n    Absence {\n        table: String,\n        max_age_seconds: u64,\n    },\n    \n    /// Fire on rate of change\n    RateOfChange {\n        query: String,\n        window_seconds: u64,\n        threshold_per_second: f64,\n    },\n}\n```\n\n## Built-in Rules\n```rust\npub fn default_rules() -> Vec<AlertRule> {\n    vec![\n        AlertRule {\n            rule_id: \"rate-limit-warning\".into(),\n            name: \"Rate Limit Warning\".into(),\n            severity: Severity::Warning,\n            condition: AlertCondition::Threshold {\n                query: \"SELECT MAX(usage_pct) FROM caam_accounts\".into(),\n                operator: ThresholdOp::Gte,\n                value: 80.0,\n            },\n            cooldown_seconds: 900,\n            ..Default::default()\n        },\n        AlertRule {\n            rule_id: \"dcg-critical-block\".into(),\n            name: \"Critical Command Blocked\".into(),\n            severity: Severity::Critical,\n            condition: AlertCondition::Pattern {\n                table: \"dcg_events\".into(),\n                column: \"severity\".into(),\n                regex: \"critical\".into(),\n            },\n            cooldown_seconds: 60,\n            ..Default::default()\n        },\n        AlertRule {\n            rule_id: \"agent-stuck\".into(),\n            name: \"Agent Appears Stuck\".into(),\n            severity: Severity::Warning,\n            condition: AlertCondition::Absence {\n                table: \"caut_usage\",\n                max_age_seconds: 600,  // No activity for 10 min\n            },\n            cooldown_seconds: 600,\n            ..Default::default()\n        },\n    ]\n}\n```\n\n## Alert Engine Implementation\n```rust\npub struct AlertEngine {\n    store: Arc<VcStore>,\n    rules: Vec<AlertRule>,\n    history: DashMap<String, Instant>,  // Cooldown tracking\n}\n\nimpl AlertEngine {\n    pub async fn check_all(&self) -> Vec<Alert> {\n        let mut alerts = Vec::new();\n        \n        for rule in &self.rules {\n            if !rule.enabled {\n                continue;\n            }\n            \n            // Check cooldown\n            if let Some(last_fired) = self.history.get(&rule.rule_id) {\n                if last_fired.elapsed() < Duration::from_secs(rule.cooldown_seconds) {\n                    continue;\n                }\n            }\n            \n            // Evaluate condition\n            if let Some(alert) = self.evaluate_rule(rule).await? {\n                self.history.insert(rule.rule_id.clone(), Instant::now());\n                self.store.record_alert(&alert).await?;\n                alerts.push(alert);\n            }\n        }\n        \n        alerts\n    }\n    \n    async fn evaluate_rule(&self, rule: &AlertRule) -> Result<Option<Alert>> {\n        match &rule.condition {\n            AlertCondition::Threshold { query, operator, value } => {\n                let result: f64 = self.store.query_scalar(query).await?;\n                if operator.check(result, *value) {\n                    Some(Alert::from_rule(rule, format!(\"Value {} {} threshold {}\", result, operator, value)))\n                } else {\n                    None\n                }\n            }\n            // ... other condition types\n        }\n    }\n}\n```\n\n## Delivery Channels\n```rust\npub trait AlertChannel: Send + Sync {\n    async fn deliver(&self, alert: &Alert) -> Result<()>;\n}\n\npub struct TuiChannel {\n    tx: mpsc::Sender<Alert>,\n}\n\npub struct WebhookChannel {\n    url: String,\n    client: reqwest::Client,\n}\n\npub struct DesktopChannel;  // Uses notify-rust\n\nimpl AlertChannel for WebhookChannel {\n    async fn deliver(&self, alert: &Alert) -> Result<()> {\n        self.client.post(&self.url)\n            .json(alert)\n            .send()\n            .await?;\n        Ok(())\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] AlertRule definitions stored in DuckDB\n- [ ] Threshold, Pattern, and Absence conditions work\n- [ ] Cooldown prevents alert spam\n- [ ] TUI channel shows alerts in overlay/status bar\n- [ ] Webhook channel delivers JSON to configured URL\n- [ ] Alert history queryable for dashboards\n- [ ] Built-in rules for common scenarios\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n\n## Alert Sources (Explicit)\n- Account usage + limits: caut/caam snapshots (rate-limit proximity).\n- Repo hygiene: ru snapshots (dirty repos / ahead-behind anomalies).\n- System health: sysmoni + fallback probe for disk/CPU/memory thresholds.\n- Tooling signals: dcg denies, cass stale, mail backlog/overdue, rch queue pressure.\n","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-01-27T16:31:36.504517926Z","created_by":"ubuntu","updated_at":"2026-01-28T17:47:04.535993841Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xm.5","depends_on_id":"bd-2mv.1","type":"blocks","created_at":"2026-01-27T18:22:29.329652908Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-2mv.2","type":"blocks","created_at":"2026-01-27T18:22:29.907463867Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-2mv.3","type":"blocks","created_at":"2026-01-27T17:00:00.963144861Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-2mv.4","type":"blocks","created_at":"2026-01-27T16:59:59.976785553Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:31:36.504517926Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-2xm.1","type":"blocks","created_at":"2026-01-27T16:32:41.341378898Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-2xm.2","type":"blocks","created_at":"2026-01-27T16:59:58.252216372Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-2xm.3","type":"blocks","created_at":"2026-01-27T16:32:41.499472461Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-2xm.4","type":"blocks","created_at":"2026-01-27T16:32:41.657084407Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-33h.1","type":"blocks","created_at":"2026-01-27T16:59:59.167830504Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.5","depends_on_id":"bd-33h.2","type":"blocks","created_at":"2026-01-27T18:22:28.729377508Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xm.6","title":"Implement Guardian MVP - Self-Healing Protocols","description":"## Overview\nBuild the Guardian system that detects problems and executes automated remediation playbooks.\n\n## Background\nGuardian extends alerting with automated response. When certain conditions are detected (rate limits, stuck agents, resource exhaustion), Guardian can execute predefined playbooks to resolve issues without human intervention.\n\n## Architecture\n```\n┌──────────────────────────────────────────────────────────┐\n│                       Guardian                            │\n├──────────────────────────────────────────────────────────┤\n│  Detectors[]  → Monitor for specific conditions          │\n│  Playbooks[]  → Predefined remediation sequences         │\n│  Executor     → Safe command execution with rollback     │\n│  Audit Log    → Full history of actions taken            │\n└──────────────────────────────────────────────────────────┘\n```\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE guardian_playbooks (\n    playbook_id VARCHAR PRIMARY KEY,\n    name VARCHAR NOT NULL,\n    description TEXT,\n    trigger_condition JSON NOT NULL,      -- When to activate\n    steps JSON NOT NULL,                  -- Array of steps\n    enabled BOOLEAN DEFAULT TRUE,\n    requires_approval BOOLEAN DEFAULT FALSE,\n    max_runs_per_hour INTEGER DEFAULT 3,\n    created_at TIMESTAMP DEFAULT now()\n);\n\nCREATE TABLE guardian_runs (\n    id INTEGER PRIMARY KEY,\n    playbook_id VARCHAR REFERENCES guardian_playbooks(playbook_id),\n    started_at TIMESTAMP NOT NULL,\n    completed_at TIMESTAMP,\n    status VARCHAR NOT NULL,              -- running, success, failed, aborted\n    trigger_context JSON,                 -- What triggered this run\n    steps_completed INTEGER DEFAULT 0,\n    steps_total INTEGER,\n    error_message TEXT,\n    rollback_performed BOOLEAN DEFAULT FALSE\n);\n\nCREATE TABLE guardian_step_log (\n    id INTEGER PRIMARY KEY,\n    run_id INTEGER REFERENCES guardian_runs(id),\n    step_index INTEGER NOT NULL,\n    step_type VARCHAR NOT NULL,\n    step_config JSON,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    status VARCHAR,                       -- pending, running, success, failed, skipped\n    output TEXT,\n    error_message TEXT\n);\n```\n\n## Playbook Definition\n```rust\n#[derive(Debug, Serialize, Deserialize)]\npub struct Playbook {\n    pub playbook_id: String,\n    pub name: String,\n    pub description: String,\n    pub trigger: PlaybookTrigger,\n    pub steps: Vec<PlaybookStep>,\n    pub requires_approval: bool,\n    pub max_runs_per_hour: u32,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub enum PlaybookTrigger {\n    /// Trigger on alert\n    OnAlert { rule_id: String },\n    \n    /// Trigger on metric threshold\n    OnThreshold { \n        query: String,\n        operator: ThresholdOp,\n        value: f64,\n    },\n    \n    /// Manual trigger only\n    Manual,\n}\n\n#[derive(Debug, Serialize, Deserialize)]\npub enum PlaybookStep {\n    /// Log a message\n    Log { message: String },\n    \n    /// Execute a command\n    Command {\n        cmd: String,\n        args: Vec<String>,\n        timeout_seconds: u64,\n        allow_failure: bool,\n    },\n    \n    /// Switch account\n    SwitchAccount {\n        program: String,\n        strategy: SwitchStrategy,  // next_available, specific, least_used\n    },\n    \n    /// Send notification\n    Notify {\n        channel: String,\n        message: String,\n    },\n    \n    /// Wait/delay\n    Wait { seconds: u64 },\n    \n    /// Conditional branch\n    Condition {\n        query: String,\n        then_steps: Vec<PlaybookStep>,\n        else_steps: Vec<PlaybookStep>,\n    },\n}\n```\n\n## Built-in Playbooks\n```rust\npub fn builtin_playbooks() -> Vec<Playbook> {\n    vec![\n        Playbook {\n            playbook_id: \"rate-limit-switch\".into(),\n            name: \"Rate Limit Account Switch\".into(),\n            description: \"Switch to backup account when rate limit approaches\".into(),\n            trigger: PlaybookTrigger::OnAlert { \n                rule_id: \"rate-limit-warning\".into() \n            },\n            steps: vec![\n                PlaybookStep::Log { \n                    message: \"Rate limit warning detected, switching account\".into() \n                },\n                PlaybookStep::SwitchAccount {\n                    program: \"claude-code\".into(),\n                    strategy: SwitchStrategy::LeastUsed,\n                },\n                PlaybookStep::Notify {\n                    channel: \"tui\".into(),\n                    message: \"Switched to backup account due to rate limit\".into(),\n                },\n            ],\n            requires_approval: false,\n            max_runs_per_hour: 3,\n        },\n        \n        Playbook {\n            playbook_id: \"stuck-agent-restart\".into(),\n            name: \"Restart Stuck Agent\".into(),\n            description: \"Restart agent that appears stuck\".into(),\n            trigger: PlaybookTrigger::OnAlert {\n                rule_id: \"agent-stuck\".into()\n            },\n            steps: vec![\n                PlaybookStep::Log {\n                    message: \"Agent appears stuck, attempting restart\".into()\n                },\n                PlaybookStep::Command {\n                    cmd: \"pkill\".into(),\n                    args: vec![\"-f\".into(), \"claude-code\".into()],\n                    timeout_seconds: 10,\n                    allow_failure: true,\n                },\n                PlaybookStep::Wait { seconds: 5 },\n                PlaybookStep::Notify {\n                    channel: \"tui\".into(),\n                    message: \"Stuck agent terminated, ready for restart\".into(),\n                },\n            ],\n            requires_approval: true,  // Destructive action\n            max_runs_per_hour: 2,\n        },\n    ]\n}\n```\n\n## Guardian Executor\n```rust\npub struct Guardian {\n    store: Arc<VcStore>,\n    playbooks: Vec<Playbook>,\n    alert_engine: Arc<AlertEngine>,\n}\n\nimpl Guardian {\n    pub async fn run_loop(&self) {\n        let mut interval = tokio::time::interval(Duration::from_secs(30));\n        \n        loop {\n            interval.tick().await;\n            \n            // Check alerts\n            let alerts = self.alert_engine.check_all().await;\n            \n            // Find triggered playbooks\n            for alert in alerts {\n                for playbook in &self.playbooks {\n                    if playbook.should_trigger(&alert) {\n                        if let Err(e) = self.execute_playbook(playbook, &alert).await {\n                            tracing::error!(\"Playbook {} failed: {}\", playbook.playbook_id, e);\n                        }\n                    }\n                }\n            }\n        }\n    }\n    \n    async fn execute_playbook(&self, playbook: &Playbook, context: &Alert) -> Result<()> {\n        // Check rate limit\n        let recent_runs = self.store.count_recent_runs(\n            &playbook.playbook_id,\n            Duration::from_secs(3600)\n        ).await?;\n        \n        if recent_runs >= playbook.max_runs_per_hour {\n            tracing::warn!(\"Playbook {} rate limited\", playbook.playbook_id);\n            return Ok(());\n        }\n        \n        // Check approval requirement\n        if playbook.requires_approval {\n            // Queue for approval, dont execute immediately\n            self.store.queue_pending_playbook(playbook, context).await?;\n            return Ok(());\n        }\n        \n        // Execute steps\n        let run_id = self.store.create_run(playbook, context).await?;\n        \n        for (i, step) in playbook.steps.iter().enumerate() {\n            let result = self.execute_step(step, context).await;\n            self.store.log_step(run_id, i, &result).await?;\n            \n            if result.is_err() && !step.allow_failure() {\n                self.store.mark_run_failed(run_id, &result).await?;\n                return result;\n            }\n        }\n        \n        self.store.mark_run_success(run_id).await?;\n        Ok(())\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] Playbook definitions stored and loaded\n- [ ] Trigger conditions evaluated correctly\n- [ ] Step execution with proper logging\n- [ ] Rate limiting prevents runaway execution\n- [ ] Approval workflow for destructive playbooks\n- [ ] Built-in playbooks for rate limit and stuck agent\n- [ ] Full audit trail of all Guardian actions\n- [ ] TUI shows pending approvals and recent runs\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n\n## Oracle-Driven Actions\n- Consume bd-2mv.5 forecasts for proactive account swaps and rate-limit avoidance.\n- Record decision context (forecast inputs + thresholds) in audit logs for explainability.\n","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-01-27T16:32:05.502914133Z","created_by":"ubuntu","updated_at":"2026-01-28T18:10:56.470527902Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xm.6","depends_on_id":"bd-2mv.5","type":"blocks","created_at":"2026-01-27T18:22:30.586432710Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.6","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:32:05.502914133Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.6","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T16:32:41.814378083Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xm.7","title":"Implement comprehensive Phase 4 TUI screens (RCH, Alerts, Guardian, Events)","description":"## Overview\nComprehensive TUI screens for Phase 4 collectors and systems: RCH builds, Alerts dashboard, Guardian self-healing, and Events (dcg/rano/pt). This is the canonical bead for ALL Phase 4 TUI work.\n\n## Background\nPhase 4 introduces several interconnected monitoring systems that need dedicated TUI screens:\n- **RCH**: Remote compilation monitoring with worker status and cache metrics\n- **Alerts**: Real-time alert display with acknowledgment and rule management\n- **Guardian**: Self-healing protocol status, pending approvals, and execution history\n- **Events**: DCG denies, RANO network anomalies, and PT process findings\n\nThese screens share navigation patterns and integrate with the global health overview.\n\n---\n\n## Screen 1: RCH/Build Screen\n\n### Layout\n```\n╭─ Remote Compilations ─────────────────────────────────────╮\n│ Worker Status:                                            │\n│   mini-1 🟢 idle      mini-2 🟢 building serde           │\n│   mini-3 🟢 idle      hetzner-1 🔴 offline               │\n├───────────────────────────────────────────────────────────┤\n│ Recent Builds:                                            │\n│ Time    Crate           Worker   Duration  Cache         │\n│ 10:05   serde           mini-2   12.3s     MISS          │\n│ 10:04   tokio-runtime   mini-1    8.7s     HIT           │\n│ 10:03   regex           mini-3    4.2s     HIT           │\n├───────────────────────────────────────────────────────────┤\n│ Slowest Crates (24h):                                     │\n│ rustc_codegen  █████████████████████ 45.2s              │\n│ llvm-sys       ████████████████ 32.1s                    │\n│ syn            ████████ 15.3s                            │\n├───────────────────────────────────────────────────────────┤\n│ Cache Hit Rate: ████████████░░░░ 67%  (last 24h)        │\n╰───────────────────────────────────────────────────────────╯\n```\n\n### Implementation\n```rust\npub struct RchScreen {\n    workers: Vec<WorkerStatus>,\n    recent_builds: Vec<RchBuild>,\n    slowest_crates: Vec<CrateStats>,\n    cache_hit_rate: f64,\n    selected_section: RchSection,\n}\n\npub enum RchSection { Workers, Builds, Crates, Cache }\n```\n\n### Navigation\n- `w` focus workers section, `b` focus builds, `c` focus crates\n- `r` refresh data, `Enter` on worker for detailed stats\n\n### Data Sources\n- rch_compilations table for builds\n- rch_workers table for worker status (from Prometheus scrape)\n\n---\n\n## Screen 2: Alerts Screen\n\n### Layout\n```\n╭─ Alerts ──────────────────────────────────────────────────╮\n│ Active (3)                                                │\n│ 🔴 CRITICAL  2m ago  Critical command blocked by dcg     │\n│    └─ Machine: orko | Rule: dcg_critical_block           │\n│ 🟡 WARNING   5m ago  Rate limit at 85% for claude-max    │\n│ 🟡 WARNING  15m ago  Agent stuck (no activity 10 min)    │\n├───────────────────────────────────────────────────────────┤\n│ Recent (Resolved)                                         │\n│ ✓ 1h ago  Rate limit warning - resolved by account switch│\n│ ✓ 3h ago  Agent stuck - manually resolved                │\n├───────────────────────────────────────────────────────────┤\n│ Rules: 12 enabled | 3 muted | 2 custom                   │\n│ [r]ules  [a]cknowledge  [m]ute  [d]ismiss  [h]istory     │\n╰───────────────────────────────────────────────────────────╯\n```\n\n### Implementation\n```rust\npub struct AlertsScreen {\n    active_alerts: Vec<Alert>,\n    recent_alerts: Vec<Alert>,\n    rules: Vec<AlertRule>,\n    selected: usize,\n    view_mode: AlertViewMode,  // Active, History, Rules\n    filter: AlertFilter,\n}\n```\n\n### Navigation\n- `j/k` navigate alerts, `a` acknowledge, `m` mute rule, `d` dismiss\n- `r` rules view, `h` history view, `/` search/filter\n\n### Data Sources\n- alert_history table, alert_rules table\n\n---\n\n## Screen 3: Guardian Screen (Self-Healing)\n\n### Layout\n```\n+------------------------------------------------+\n| GUARDIAN - SELF-HEALING            [t]oggle mode |\n+------------------------------------------------+\n| STATUS                                          |\n| ├─ Mode: execute-safe (allowlisted commands)    |\n| ├─ Patterns detected: 3 active                  |\n| ├─ Last action: 2 min ago                       |\n| └─ Success rate: 94% (47/50 last week)         |\n+------------------------------------------------+\n| ACTIVE PROTOCOLS                                |\n| ├─ rate_limit_recovery on orko                  |\n| │   └─ Step 2/4: Preparing account swap         |\n| │   └─ Started: 45 sec ago                      |\n| └─ disk_space_recovery on mac-mini              |\n|     └─ Step 1/4: Analyzing disk consumers       |\n+------------------------------------------------+\n| PENDING INTERVENTIONS                           |\n| ├─ cc_5 stuck_agent_recovery                    |\n| │   └─ Waiting for: manual approval             |\n| │   └─ Actions: send interrupt, force compact   |\n+------------------------------------------------+\n| HISTORY (last 24h)                              |\n| ├─ [OK] rate_limit_recovery (orko) - 2min ago  |\n| │   └─ Swapped jeff@email.com -> backup@email   |\n| ├─ [FAIL] stuck_agent (orko) - 3hr ago         |\n| │   └─ Agent did not recover, escalated         |\n+------------------------------------------------+\n```\n\n### Implementation\n```rust\npub struct GuardianScreen {\n    status: GuardianStatus,\n    active_protocols: Vec<ActiveProtocol>,\n    pending_approvals: Vec<PendingPlaybook>,\n    recent_runs: Vec<GuardianRun>,\n    selected_section: GuardianSection,\n}\n\npub struct GuardianStatus {\n    mode: GuardianMode,  // Off, Suggest, ExecuteSafe, WithApproval\n    enabled: bool,\n    active_patterns: usize,\n    last_action_at: Option<DateTime<Utc>>,\n    success_rate_7d: f64,\n}\n```\n\n### Navigation\n- `Tab` between sections (Status, Active, Pending, History)\n- `y` approve pending, `n` reject, `p` pause guardian, `t` toggle mode\n- `Enter` on history for run details, `h` full history\n\n### Data Sources\n- guardian_status, guardian_runs, guardian_pending tables\n\n---\n\n## Screen 4: Events Screen (DCG/RANO/PT)\n\n### Layout\n```\n+------------------------------------------------+\n| EVENTS                          [f]ilter [/]search |\n+------------------------------------------------+\n| DCG DENIES (last 24h)                           |\n| ├─ orko: 12 denies (3 critical)                 |\n| │   └─ rm -rf / (critical) [12:34]              |\n| │   └─ git reset --hard (high) [11:22]          |\n| ├─ sydneymc: 5 denies (0 critical)              |\n+------------------------------------------------+\n| NETWORK ANOMALIES                               |\n| ├─ Unknown provider: suspicious.domain.com      |\n| │   └─ PID 1234 (node) -> 12 connections        |\n| ├─ Auth loop detected: claude.api (rate limit?) |\n+------------------------------------------------+\n| PROCESS ISSUES                                  |\n| ├─ Zombie processes: 3 on orko                  |\n| ├─ Stuck agents: cc_5 (velocity: 0 for 15min)   |\n| ├─ Runaway: cargo build (CPU 95% for 30min)     |\n+------------------------------------------------+\n```\n\n### Implementation\n```rust\npub struct EventsScreen {\n    dcg_events: Vec<DcgEvent>,\n    rano_events: Vec<RanoEvent>,\n    pt_findings: Vec<PtFinding>,\n    selected_section: EventSection,  // Dcg, Rano, Pt\n    filter: EventFilter,\n    time_range: TimeRange,\n}\n```\n\n### Navigation\n- `Tab` switch sections (DCG, Network, Processes)\n- `j/k` navigate, `Enter` drill into event details\n- `f` filter by machine/severity, `/` search, `t` time range\n\n### Data Sources\n- dcg_events, rano_events, process_triage_findings tables\n\n---\n\n## Shared Components\n\n### Color Coding\n```rust\nfn severity_color(severity: &Severity) -> Color {\n    match severity {\n        Severity::Critical => Color::Red,\n        Severity::High => Color::LightRed,\n        Severity::Warning => Color::Yellow,\n        Severity::Info => Color::Blue,\n        Severity::Low => Color::Gray,\n    }\n}\n```\n\n### Screen Manager\n```rust\npub enum Phase4Screen { Rch, Alerts, Guardian, Events }\n\n// Keys 1-4 switch screens, shared keybindings apply\n```\n\n---\n\n## Acceptance Criteria\n- [ ] RCH screen shows worker status with online/offline indicators\n- [ ] RCH screen shows recent builds with timing and cache status\n- [ ] RCH slowest crates with bar chart visualization\n- [ ] RCH cache hit rate with progress bar/sparkline\n- [ ] Alerts screen shows active alerts with severity colors\n- [ ] Alert acknowledgment and muting works from TUI\n- [ ] Guardian screen shows current mode and status\n- [ ] Guardian shows active healing protocols with step progress\n- [ ] Guardian pending approvals with y/n keyboard action\n- [ ] Guardian history with success/failure indicators\n- [ ] Events screen shows DCG denies grouped by machine\n- [ ] Events screen shows RANO network anomalies\n- [ ] Events screen shows PT process issues\n- [ ] All screens support machine filtering and time range\n- [ ] Screens refresh on poll cycle completion\n- [ ] Navigation via 1-4 number keys\n- [ ] Consistent color coding across screens\n\n---\n\n## Testing & Logging\n\n### Unit Tests\n- Screen state initialization and defaults\n- Filter/color mapping functions\n- Navigation state transitions\n\n### Snapshot Tests\n- Each screen layout at different terminal sizes\n- Color rendering for severities\n- Empty/loading states\n\n### E2E Tests (tests/e2e/tui/)\n- `test_phase4_tui.sh`: Full Phase 4 screen suite\n- `test_alerts_ack.sh`: Acknowledge flow\n- `test_guardian_approve.sh`: Approval workflow\n\nAll tests emit JSON summary in tests/logs/ with test_name, status, duration_ms, assertions.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:32:33.751981079Z","created_by":"ubuntu","updated_at":"2026-01-27T21:37:23.941040607Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xm.7","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:32:33.751981079Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.7","depends_on_id":"bd-2xm.1","type":"blocks","created_at":"2026-01-27T16:32:41.985878867Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.7","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T16:32:42.153397812Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.7","depends_on_id":"bd-2xm.6","type":"blocks","created_at":"2026-01-27T16:32:42.315347087Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.7","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:19.825282590Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xm.7","depends_on_id":"bd-33h.3","type":"blocks","created_at":"2026-01-27T16:32:42.476977301Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2xy","title":"Implement vc watch streaming mode","description":"Implement vc watch command for real-time JSONL event streaming.\n\nPURPOSE:\nEnable guardian agents to monitor without polling full snapshots. Events are emitted on change or at fixed intervals.\n\nCOMMAND USAGE:\nvc watch --format jsonl --interval 30\nvc watch --format jsonl --changes-only\nvc watch --format toon --interval 60\n\nEVENT TYPES:\n- alert: New or escalated alerts\n  {\"type\":\"alert\",\"ts\":\"...\",\"alert_id\":\"...\",\"severity\":\"critical\",\"machine\":\"orko\",\"message\":\"...\"}\n\n- prediction: Forecast risk crossing threshold\n  {\"type\":\"prediction\",\"ts\":\"...\",\"prediction_type\":\"rate_limit\",\"machine\":\"orko\",\"confidence\":0.85,\"action\":\"swap_now\"}\n\n- opportunity: Optimization hints\n  {\"type\":\"opportunity\",\"ts\":\"...\",\"opportunity_type\":\"cost_saving\",\"estimated_savings\":12.50,\"action\":\"...\"}\n\n- health_change: Machine/fleet health state change\n  {\"type\":\"health_change\",\"ts\":\"...\",\"machine\":\"orko\",\"old_score\":0.85,\"new_score\":0.62,\"factor\":\"cpu_high\"}\n\n- collector_status: Collector success/failure\n  {\"type\":\"collector_status\",\"ts\":\"...\",\"collector\":\"sysmoni\",\"machine\":\"orko\",\"status\":\"ok\",\"duration_ms\":234}\n\nIMPLEMENTATION:\n- Create vc_cli/src/watch.rs\n- Use tokio broadcast channel for event distribution\n- Support filtering by event type: --events alert,prediction\n- Support filtering by machine: --machines orko,sydneymc\n- Support severity threshold: --min-severity high\n\nOUTPUT OPTIONS:\n- --format jsonl: One JSON object per line (default)\n- --format toon: Token-optimized format\n- --interval N: Emit summary every N seconds even if no changes\n- --changes-only: Only emit when something changes\n- --buffer N: Buffer up to N events before emitting (batch mode)\n\nGRACEFUL HANDLING:\n- Reconnect to daemon if connection lost\n- Emit heartbeat events periodically\n- Clean shutdown on SIGINT/SIGTERM\n\nTESTS:\n- Event emission tests\n- Filter application tests\n- Format output tests\n- Reconnection tests\n\n## E2E & Logging\n- Add `tests/e2e/cli/test_watch_stream.sh` to validate JSONL output, filters, and heartbeat handling.\n- Capture stdout/stderr and emit JSON summary in tests/logs/.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:50:36.300307098Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:20.681755222Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2xy","depends_on_id":"bd-1h6","type":"blocks","created_at":"2026-01-27T19:26:01.166294996Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xy","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T16:54:33.690906113Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xy","depends_on_id":"bd-2mv.5","type":"blocks","created_at":"2026-01-27T17:03:57.752768365Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xy","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T16:54:34.823968119Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xy","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:20.681576856Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2xy","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T16:55:49.672918646Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2yb","title":"Implement fallback system probe collector","description":"Implement the fallback system probe collector - ALWAYS ENABLED baseline health check that works even when no tools are installed.\n\nINTEGRATION METHOD:\n- Raw shell commands that exist on all Linux/macOS systems\n- No external tool dependencies\n- Always succeeds (or machine is unreachable)\n\nCOMMANDS TO RUN:\nuptime                      # System uptime and load averages\ndf -P                       # Disk usage (POSIX format)\nfree -b                     # Memory usage in bytes\ncat /proc/loadavg           # Load averages (Linux)\ncat /proc/meminfo           # Detailed memory info (Linux)\ncat /proc/stat              # CPU stats (Linux)\nsysctl hw.memsize           # Memory (macOS)\nvm_stat                     # Memory stats (macOS)\n\nDUCKDB TABLE:\nCREATE TABLE sys_fallback_samples(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    uptime_seconds BIGINT,\n    load1 REAL,\n    load5 REAL,\n    load15 REAL,\n    mem_total_bytes BIGINT,\n    mem_available_bytes BIGINT,\n    mem_used_bytes BIGINT,\n    swap_total_bytes BIGINT,\n    swap_used_bytes BIGINT,\n    disk_usage_json TEXT,  -- [{mount, total, used, avail, pct}]\n    raw_output TEXT,\n    PRIMARY KEY (machine_id, collected_at)\n);\n\nCOLLECTOR IMPLEMENTATION:\n- Implement FallbackProbeCollector with Collector trait\n- Detect platform (Linux vs macOS) and use appropriate commands\n- Parse uptime output for load averages\n- Parse df -P output for disk usage\n- Parse free -b or /proc/meminfo for memory\n- Store all raw output for debugging\n- NEVER FAIL - if individual commands fail, store partial data\n- Set collector_available = true always\n\nPRIORITY: This collector runs FIRST and ALWAYS\n- Other collectors (sysmoni, etc.) may add more detail\n- Fallback ensures we always have basic health data\n- Critical for new machines or machines with minimal tooling\n\nUI VALUE:\n- Baseline health for ALL machines\n- Works immediately on any SSH-accessible system\n- Foundation for health scoring when other collectors missing\n\nTESTS:\n- Test Linux parsing\n- Test macOS parsing\n- Test partial failure handling\n- Verify always-succeeds behavior\n\n## E2E & Logging\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-27T16:48:49.845518180Z","created_by":"ubuntu","updated_at":"2026-01-28T05:40:39.968624601Z","closed_at":"2026-01-28T05:40:39.968534791Z","close_reason":"FallbackProbeCollector implemented: platform detection, uptime/memory/disk collection, NEVER FAIL design, 7 tests","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2yb","depends_on_id":"bd-2a9","type":"parent-child","created_at":"2026-01-27T16:55:33.136167292Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yb","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T16:59:28.638341872Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yb","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:52:52.302850404Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2yb","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:52:51.097580446Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-30z","title":"Create E2E test scripts with detailed logging","description":"Implement comprehensive end-to-end test scripts for vibe_cockpit:\n\nE2E TEST SCENARIOS:\n\n1. COLLECTOR E2E TESTS (tests/e2e/collectors/):\n- test_sysmoni_collect.sh: Run sysmoni collector, verify DuckDB rows, check data integrity\n- test_ru_collect.sh: Create temp git repos, run ru collector, verify repo_status_snapshots\n- test_caut_collect.sh: Mock caut CLI output, verify account_usage_snapshots\n- test_caam_collect.sh: Mock caam CLI output, verify account_profile_snapshots\n- test_cass_collect.sh: Create test sessions, run cass collector, verify session tables\n- test_mcp_mail_collect.sh: Create test SQLite mail DB, run collector, verify mail tables\n- test_dcg_collect.sh: Create history.jsonl, run collector, verify dcg_events\n- test_rch_collect.sh: Mock prometheus endpoint, run collector, verify rch tables\n- test_rano_collect.sh: Create rano export, run collector, verify net_events\n- test_pt_collect.sh: Mock pt CLI output, verify process_triage tables\n- test_ntm_collect.sh: Mock ntm robot output, verify ntm_sessions\n- test_bv_br_collect.sh: Create .beads DB, run collector, verify beads tables\n- test_afsc_collect.sh: Mock afsc output, verify afsc tables\n- test_cloud_bench_collect.sh: Mock benchmarker output, verify cloud_bench tables\n- test_fallback_probe.sh: Test baseline system probe (always works)\n\n2. REMOTE EXECUTION E2E (tests/e2e/remote/):\n- test_ssh_runner.sh: SSH to localhost, verify command execution\n- test_tool_probing.sh: Probe for tool availability, verify caching\n- test_remote_collectors.sh: Run collectors over SSH, verify results\n\n3. TUI E2E TESTS (tests/e2e/tui/):\n- Use expect/pexpect for terminal automation\n- test_tui_navigation.sh: Navigate all screens, verify no crashes\n- test_tui_filters.sh: Apply filters, verify data updates\n- test_tui_drill_down.sh: Drill into machines/repos/alerts\n\n4. WEB API E2E (tests/e2e/web/):\n- test_api_health.sh: Verify /api/health endpoint\n- test_api_machines.sh: Verify machine CRUD\n- test_api_alerts.sh: Verify alert listing/ack\n\n5. FULL SYSTEM E2E (tests/e2e/system/):\n- test_full_poll_cycle.sh: Start vc daemon, run full poll, verify all tables populated\n- test_alert_flow.sh: Inject conditions, verify alert creation and delivery\n- test_autopilot_suggest.sh: Trigger conditions, verify suggestions generated\n- test_guardian_healing.sh: Trigger healing protocol, verify execution\n\nLOGGING REQUIREMENTS:\n- All scripts use set -x for command tracing\n- Structured logging to tests/logs/ with timestamps\n- Each test outputs JSON summary: {test_name, status, duration_ms, assertions, failures}\n- Aggregate test report generated at end\n- On failure: capture DuckDB state, collector outputs, stderr\n\nTEST RUNNER:\n- tests/e2e/run_all.sh: Run all E2E tests in sequence\n- Support --parallel for concurrent execution\n- Support --filter for running subset\n- Exit codes: 0=pass, 1=fail, 2=skip\n\nADDITIONAL E2E SCENARIOS (incremental):\n- test_tui_oracle.sh: Oracle screen navigation + forecast rendering\n- test_tui_beads.sh: Beads screen triage + filter behavior\n- test_tui_guardian.sh: Guardian screen state + history view\n- test_watch_stream.sh: vc watch JSONL streaming + filters\n- test_toon_output.sh: TOON output for robot commands\n- test_incident_flow.sh: incident create/auto-detect + timeline verification\n\nCOVERAGE OWNERSHIP (IMPORTANT):\n- Each feature bead owns its corresponding E2E script(s); bd-30z provides the shared runner, logging spec, and coverage manifest.\n- The script list above is a coverage checklist. Implement scripts alongside their feature beads and ensure they are registered in `tests/e2e/run_all.sh`.\n\nPHASE 7 E2E SCENARIOS (incremental):\n- test_data_quality.sh: freshness + drift detection\n- test_adaptive_polling.sh: adaptive intervals + backoff\n- test_profile_burst.sh: on-demand profiling capture\n- test_incident_replay.sh: time-travel replay + export\n- test_digest_reports.sh: daily/weekly report generation\n- test_vc_node_push.sh: vc-node bundle push + ingest\n- test_prometheus_export.sh: /metrics scrape + cardinality limits\n- test_github_collect.sh: issues/PRs collector\n- test_cost_attribution.sh: cost analytics summary\n- test_robot_schemas.sh: schema validation\n- test_config_lint.sh: config lint + wizard\n- test_query_guardrails.sh: safe template enforcement\n- test_redaction.sh: secret redaction pipeline\n- test_export_backup.sh: export/import round-trip\n- test_alert_routing.sh: routing/escalation policies\n- test_api_auth.sh: auth enforcement","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:47:49.176811382Z","created_by":"ubuntu","updated_at":"2026-01-28T17:42:21.127835687Z","closed_at":"2026-01-28T17:42:21.127445923Z","close_reason":"E2E infrastructure complete - verified by DustyPrairie: run_all.sh, lib/test_helpers.sh, JSON summaries, 15+ test scripts","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-30z","depends_on_id":"bd-1x9","type":"blocks","created_at":"2026-01-27T16:52:49.964123916Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-30z","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T18:22:36.972842503Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-30z","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:59:22.227576667Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-30z","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T18:22:37.858625693Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-30z","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T16:59:23.302964529Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33h","title":"Phase 1: System + Repo Basics","description":"# Phase 1: System + Repo Basics\n\n## Purpose\nImplement the first real data collectors (sysmoni, ru) and the foundation TUI. This phase proves out the collector architecture and gives users something tangible to interact with.\n\n## Key Deliverables\n1. **sysmoni Collector**: System metrics collection (CPU, memory, load, disk, network)\n2. **ru Collector**: Repository status tracking via `ru list --json` and `ru status --json`\n3. **DuckDB Tables**: sys_samples, sys_top_processes, repos, repo_status_snapshots\n4. **TUI Overview Screen**: ratatui-based interface showing machines, repos, basic health\n5. **Robot Status Command**: `vc robot status --json` with stable schema\n\n## Success Criteria\n- System metrics stored in DuckDB at configured intervals\n- Repo status tracked across machines\n- TUI launches and shows overview\n- Robot output parseable by agents\n\n## Technical Context\n\n### sysmoni Integration\nsysmoni provides one-shot (`--json`) or streaming (`--json-stream`) system metrics:\n- CPU utilization per core\n- Memory/swap usage\n- Load averages (1, 5, 15 min)\n- Disk I/O rates\n- Network I/O rates\n- Top processes by resource usage\n\nThis is the \"CLI Snapshot\" ingestion pattern - run command, parse JSON, store with collected_at timestamp.\n\n### ru Integration\nru (repo updater) tracks git repositories:\n- `ru list --json` enumerates repo paths\n- `ru status --no-fetch --json` shows dirty/ahead/behind without network\n\nThis enables the \"dirty repos\" alert rule and provides context for agent work locations.\n\n### TUI Framework Choice\nratatui + crossterm because:\n- Pure Rust (no ncurses dependency)\n- Cross-platform (Linux, macOS, Windows)\n- Active community and good documentation\n- Async-friendly with tokio\n\n## Dependencies\n- Requires Phase 0 (Cargo workspace, DuckDB, Collector trait, CLI foundation)\n\n## Estimated Scope\n- ~1-2 weeks\n- First \"real\" feedback loop - users can see data flowing\n\n## Testing & E2E Expectations\n- Every deliverable in this phase must add unit tests per bd-1x9.\n- Extend bd-30z with phase-specific e2e scenarios and structured logs.\n- Phase completion requires `cargo test --workspace` + phase e2e suite with JSON summaries in tests/logs/.\n\n## Fallback Baseline (Always-On)\n- Include the fallback system probe (bd-2yb) as a Phase 1 deliverable so every machine has baseline health, even without sysmoni.\n- Surface fallback metrics in the Overview + Robot Status when sysmoni data is missing or stale.\n","status":"in_progress","priority":0,"issue_type":"epic","created_at":"2026-01-27T16:17:20.015690877Z","created_by":"ubuntu","updated_at":"2026-01-28T16:57:09.826621303Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-33h","depends_on_id":"bd-2a9","type":"blocks","created_at":"2026-01-27T16:19:04.087502737Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h","depends_on_id":"bd-2yb","type":"blocks","created_at":"2026-01-27T18:22:36.371914462Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33h.1","title":"Implement sysmoni collector","description":"# Task: Implement sysmoni collector\n\n## What to Build\nA collector for system metrics via the sysmoni tool. This is the first \"real\" collector that uses the CLI Snapshot ingestion pattern.\n\n## Integration Method\nsysmoni provides JSON output:\n```bash\nsysmoni --json           # One-shot snapshot\nsysmoni --json-stream    # Continuous NDJSON (for high-frequency)\n```\n\nFor MVP, use one-shot `--json` mode.\n\n## sysmoni JSON Schema (expected)\n```json\n{\n  \"timestamp\": \"2026-01-27T00:00:00Z\",\n  \"cpu\": {\n    \"total_percent\": 45.2,\n    \"per_core\": [42.1, 48.3, 44.0, 46.4],\n    \"load_1\": 2.1,\n    \"load_5\": 1.8,\n    \"load_15\": 1.5\n  },\n  \"memory\": {\n    \"total_bytes\": 34359738368,\n    \"used_bytes\": 23622320128,\n    \"available_bytes\": 10737418240,\n    \"swap_total_bytes\": 8589934592,\n    \"swap_used_bytes\": 0\n  },\n  \"disk\": {\n    \"read_bytes_per_sec\": 1048576,\n    \"write_bytes_per_sec\": 2097152,\n    \"filesystems\": [\n      {\"mount\": \"/\", \"total_bytes\": 500000000000, \"used_bytes\": 350000000000}\n    ]\n  },\n  \"network\": {\n    \"rx_bytes_per_sec\": 10485760,\n    \"tx_bytes_per_sec\": 5242880\n  },\n  \"processes\": [\n    {\"pid\": 1234, \"name\": \"cargo\", \"cpu_percent\": 45.0, \"memory_bytes\": 1073741824}\n  ]\n}\n```\n\n## Collector Implementation\n```rust\n// crates/vc_collect/src/collectors/sysmoni.rs\n\npub struct SysmoniCollector;\n\n#[async_trait]\nimpl Collector for SysmoniCollector {\n    fn name(&self) -> &'static str { \"sysmoni\" }\n    fn schema_version(&self) -> u32 { 1 }\n    \n    fn required_tool(&self) -> Option<&'static str> {\n        Some(\"sysmoni\")\n    }\n    \n    async fn collect(&self, ctx: &CollectContext) -> CollectResult {\n        let output = ctx.executor.run_timeout(\"sysmoni --json\", ctx.timeout).await?;\n        \n        let data: SysmoniOutput = serde_json::from_str(&output)\n            .map_err(|e| CollectError::ParseError(e.to_string()))?;\n        \n        let collected_at = Utc::now();\n        \n        // Build sys_samples row\n        let sys_row = serde_json::json!({\n            \"machine_id\": ctx.machine_id.as_str(),\n            \"collected_at\": collected_at.to_rfc3339(),\n            \"cpu_total\": data.cpu.total_percent,\n            \"load1\": data.cpu.load_1,\n            \"load5\": data.cpu.load_5,\n            \"load15\": data.cpu.load_15,\n            \"mem_used_bytes\": data.memory.used_bytes,\n            \"mem_total_bytes\": data.memory.total_bytes,\n            \"swap_used_bytes\": data.memory.swap_used_bytes,\n            \"disk_read_mbps\": data.disk.read_bytes_per_sec as f64 / 1_000_000.0,\n            \"disk_write_mbps\": data.disk.write_bytes_per_sec as f64 / 1_000_000.0,\n            \"net_rx_mbps\": data.network.rx_bytes_per_sec as f64 / 1_000_000.0,\n            \"net_tx_mbps\": data.network.tx_bytes_per_sec as f64 / 1_000_000.0,\n            \"raw_json\": output,\n        });\n        \n        // Build top processes rows\n        let proc_rows: Vec<_> = data.processes.iter().take(10).map(|p| {\n            serde_json::json!({\n                \"machine_id\": ctx.machine_id.as_str(),\n                \"collected_at\": collected_at.to_rfc3339(),\n                \"pid\": p.pid,\n                \"comm\": p.name,\n                \"cpu_pct\": p.cpu_percent,\n                \"mem_bytes\": p.memory_bytes,\n            })\n        }).collect();\n        \n        CollectResult {\n            rows: vec![\n                RowBatch { table: \"sys_samples\", rows: vec![sys_row] },\n                RowBatch { table: \"sys_top_processes\", rows: proc_rows },\n            ],\n            new_cursor: None,  // Stateless - each poll is fresh snapshot\n            raw_artifacts: vec![],\n            warnings: vec![],\n            duration: ctx.timeout,  // Will be set by framework\n            success: true,\n            error: None,\n        }\n    }\n}\n```\n\n## DuckDB Tables (add to migrations)\n```sql\n-- migrations/002_sys_metrics.sql\n\nCREATE TABLE IF NOT EXISTS sys_samples(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    cpu_total REAL,\n    load1 REAL,\n    load5 REAL,\n    load15 REAL,\n    mem_used_bytes BIGINT,\n    mem_total_bytes BIGINT,\n    swap_used_bytes BIGINT,\n    disk_read_mbps REAL,\n    disk_write_mbps REAL,\n    net_rx_mbps REAL,\n    net_tx_mbps REAL,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, collected_at)\n);\n\nCREATE TABLE IF NOT EXISTS sys_top_processes(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    pid INTEGER,\n    comm TEXT,\n    cpu_pct REAL,\n    mem_bytes BIGINT,\n    fd_count INTEGER,\n    io_read_bytes BIGINT,\n    io_write_bytes BIGINT,\n    raw_json TEXT\n);\n```\n\n## Fallback Probe\nIf sysmoni is not available, implement a fallback using basic system commands:\n```bash\nuptime         # load averages\nfree -b        # memory\ndf -P          # disk space\ncat /proc/loadavg\n```\n\nThis ensures basic system health even without tooling.\n\n## Acceptance Criteria\n- [ ] Collector parses sysmoni JSON correctly\n- [ ] sys_samples and sys_top_processes tables populated\n- [ ] Fallback works when sysmoni unavailable\n- [ ] Collection respects timeout\n- [ ] Invalid JSON handled gracefully\n\n## Notes for Future Self\n- Consider streaming mode for high-frequency monitoring\n- Watch for sysmoni output schema changes\n- Fallback should degrade gracefully, not fail entirely\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","notes":"MaroonIsland: Removed duplicate SysmoniCollector from collectors/mod.rs (lines 910-1117). Kept sysmoni.rs module import. VERIFIED: File structure is correct. PENDING: Cargo build verification blocked by lock. Next agent should run cargo check -p vc_collect to verify and close.","status":"closed","priority":1,"issue_type":"task","owner":"maroonisland","created_at":"2026-01-27T16:22:12.157762069Z","created_by":"ubuntu","updated_at":"2026-01-28T16:57:33.765913060Z","closed_at":"2026-01-28T16:57:33.765840914Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-33h.1","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T16:59:24.277796423Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.1","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:59:25.346385539Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.1","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:23:47.760876816Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.1","depends_on_id":"bd-2yb","type":"blocks","created_at":"2026-01-27T16:53:12.515857319Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.1","depends_on_id":"bd-33h","type":"parent-child","created_at":"2026-01-27T16:22:12.157762069Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33h.2","title":"Implement ru (repo updater) collector","description":"# Task: Implement ru (repo updater) collector\n\n## What to Build\nA collector for repository status via the ru tool. Uses CLI Snapshot pattern with two commands: list and status.\n\n## Integration Method\n```bash\nru list --json              # List all tracked repos\nru status --no-fetch --json # Status without network (important for speed)\n```\n\nThe `--no-fetch` flag is critical to avoid network latency in tight poll loops.\n\n## Expected JSON Schema\n\n### ru list --json\n```json\n{\n  \"repos\": [\n    {\n      \"path\": \"/data/projects/vibe_cockpit\",\n      \"url\": \"git@github.com:Dicklesworthstone/vibe_cockpit.git\",\n      \"name\": \"vibe_cockpit\"\n    }\n  ]\n}\n```\n\n### ru status --no-fetch --json\n```json\n{\n  \"repos\": [\n    {\n      \"path\": \"/data/projects/vibe_cockpit\",\n      \"branch\": \"main\",\n      \"dirty\": false,\n      \"ahead\": 0,\n      \"behind\": 0,\n      \"last_commit\": {\n        \"sha\": \"abc123\",\n        \"timestamp\": \"2026-01-27T00:00:00Z\",\n        \"message\": \"feat: add collector trait\"\n      },\n      \"modified_files\": [],\n      \"untracked_files\": []\n    }\n  ]\n}\n```\n\n## Collector Implementation\n```rust\n// crates/vc_collect/src/collectors/ru.rs\n\npub struct RuCollector;\n\n#[async_trait]\nimpl Collector for RuCollector {\n    fn name(&self) -> &'static str { \"ru\" }\n    fn schema_version(&self) -> u32 { 1 }\n    \n    fn required_tool(&self) -> Option<&'static str> {\n        Some(\"ru\")\n    }\n    \n    async fn collect(&self, ctx: &CollectContext) -> CollectResult {\n        let collected_at = Utc::now();\n        let mut rows = vec![];\n        let mut warnings = vec![];\n        \n        // First, get repo list (for populating the repos table)\n        let list_output = ctx.executor.run_timeout(\"ru list --json\", ctx.timeout).await;\n        \n        if let Ok(output) = list_output {\n            if let Ok(list) = serde_json::from_str::<RuListOutput>(&output) {\n                for repo in &list.repos {\n                    rows.push(RowBatch {\n                        table: \"repos\",\n                        rows: vec![serde_json::json!({\n                            \"machine_id\": ctx.machine_id.as_str(),\n                            \"repo_id\": hash_repo(&repo.url.as_ref().unwrap_or(&repo.path)),\n                            \"path\": repo.path,\n                            \"url\": repo.url,\n                            \"name\": repo.name,\n                        })],\n                    });\n                }\n            }\n        }\n        \n        // Then, get status (no-fetch to avoid network)\n        let status_output = ctx.executor.run_timeout(\"ru status --no-fetch --json\", ctx.timeout).await;\n        \n        match status_output {\n            Ok(output) => {\n                match serde_json::from_str::<RuStatusOutput>(&output) {\n                    Ok(status) => {\n                        let status_rows: Vec<_> = status.repos.iter().map(|r| {\n                            serde_json::json!({\n                                \"machine_id\": ctx.machine_id.as_str(),\n                                \"collected_at\": collected_at.to_rfc3339(),\n                                \"repo_id\": hash_repo(&r.url.as_ref().unwrap_or(&r.path)),\n                                \"branch\": r.branch,\n                                \"dirty\": r.dirty,\n                                \"ahead\": r.ahead,\n                                \"behind\": r.behind,\n                                \"raw_json\": serde_json::to_string(&r).unwrap_or_default(),\n                            })\n                        }).collect();\n                        \n                        rows.push(RowBatch {\n                            table: \"repo_status_snapshots\",\n                            rows: status_rows,\n                        });\n                    }\n                    Err(e) => {\n                        warnings.push(Warning {\n                            level: WarningLevel::Error,\n                            message: format!(\"Failed to parse ru status: {}\", e),\n                            context: Some(output),\n                        });\n                    }\n                }\n            }\n            Err(e) => {\n                warnings.push(Warning {\n                    level: WarningLevel::Error,\n                    message: format!(\"Failed to run ru status: {}\", e),\n                    context: None,\n                });\n            }\n        }\n        \n        CollectResult {\n            rows,\n            new_cursor: None,  // Stateless\n            raw_artifacts: vec![],\n            warnings,\n            duration: Duration::ZERO,\n            success: warnings.is_empty(),\n            error: if warnings.is_empty() { None } else { Some(\"Partial failure\".to_string()) },\n        }\n    }\n}\n\nfn hash_repo(identifier: &str) -> String {\n    // Create stable hash of repo URL or path\n    use std::hash::{Hash, Hasher};\n    use std::collections::hash_map::DefaultHasher;\n    let mut hasher = DefaultHasher::new();\n    identifier.hash(&mut hasher);\n    format!(\"repo_{:016x}\", hasher.finish())\n}\n```\n\n## DuckDB Tables\n```sql\n-- migrations/003_repos.sql\n\nCREATE TABLE IF NOT EXISTS repos(\n    machine_id TEXT,\n    repo_id TEXT,\n    path TEXT,\n    url TEXT,\n    name TEXT,\n    PRIMARY KEY (machine_id, repo_id)\n);\n\nCREATE TABLE IF NOT EXISTS repo_status_snapshots(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    repo_id TEXT,\n    branch TEXT,\n    dirty BOOLEAN,\n    ahead INTEGER,\n    behind INTEGER,\n    raw_json TEXT\n);\n\nCREATE INDEX IF NOT EXISTS idx_repo_status_time \nON repo_status_snapshots(machine_id, repo_id, collected_at DESC);\n```\n\n## Alert Rule: Dirty Repos Spike\n```rust\n// This becomes an alert rule in Phase 4\n// If > 10 dirty repos on a machine, something is stuck\n```\n\n## Acceptance Criteria\n- [ ] Collector runs ru list and ru status\n- [ ] repos table populated with unique repo_id\n- [ ] repo_status_snapshots captures dirty/ahead/behind\n- [ ] Stable repo_id hash allows cross-machine correlation\n- [ ] Graceful handling when ru not available\n\n## Notes for Future Self\n- repo_id must be stable across runs\n- Consider commit stats (commits_7d, commits_30d) for productivity metrics\n- ru may need updates for new JSON schema\n\n## Testing & Logging\nUnit tests:\n- Parse representative fixtures (happy path + partial/malformed).\n- Validate normalized fields and schema_version handling.\n- Error-path tests: missing tool, non-zero exit, timeout, invalid JSON/JSONL.\n\nIntegration tests:\n- Insert rows into DuckDB and query back expected values (including raw_json/raw_output).\n- Cursor/incremental window idempotency (no duplicates on re-run).\n\nE2E:\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:22:37.394332082Z","created_by":"ubuntu","updated_at":"2026-01-28T05:51:37.123186019Z","closed_at":"2026-01-28T05:51:37.122475460Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-33h.2","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T16:59:26.563600316Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.2","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:59:27.552102741Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.2","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:23:47.879089625Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.2","depends_on_id":"bd-33h","type":"parent-child","created_at":"2026-01-27T16:22:37.394332082Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33h.3","title":"Implement TUI overview screen with ratatui","description":"# Task: Implement TUI overview screen with ratatui\n\n## What to Build\nThe primary TUI interface using ratatui + crossterm, starting with the Overview screen that shows machines, basic health, and navigation.\n\n## Framework Choice Rationale\n- **ratatui**: Active Rust TUI framework, well-documented\n- **crossterm**: Cross-platform terminal handling (Linux, macOS, Windows)\n- Both are pure Rust (no ncurses dependency)\n- Async-friendly with tokio\n\n## TUI Architecture\n```rust\n// crates/vc_tui/src/lib.rs\n\npub struct App {\n    pub state: AppState,\n    pub store: Arc<Store>,\n    pub config: VcConfig,\n}\n\npub struct AppState {\n    pub current_screen: Screen,\n    pub selected_machine: Option<String>,\n    pub filter: FilterState,\n    pub last_refresh: DateTime<Utc>,\n}\n\npub enum Screen {\n    Overview,\n    MachineDetail(String),\n    Repos,\n    Accounts,\n    Alerts,\n    // More screens added later\n}\n\npub struct FilterState {\n    pub machine_filter: Option<String>,\n    pub time_window: Duration,\n}\n```\n\n## Overview Screen Layout\n```\n╔══════════════════════════════════════════════════════════════════════════════╗\n║  V I B E   C O C K P I T                    [Health: ●] [Refresh: 30s ago]   ║\n╠══════════════════════════════════════════════════════════════════════════════╣\n║                                                                              ║\n║  ┌─ MACHINES ─────────────────────────┐  ┌─ ALERTS ─────────────────────────┐║\n║  │  ● orko          CPU 45% MEM 68%  │  │  No active alerts               │║\n║  │    sydneymc      CPU 32% MEM 42%  │  │                                  │║\n║  │    mac-mini      CPU 12% MEM 55%  │  │                                  │║\n║  │  ○ gpu-box       [offline]        │  │                                  │║\n║  └────────────────────────────────────┘  └──────────────────────────────────┘║\n║                                                                              ║\n║  ┌─ REPOS ────────────────────────────────────────────────────────────────┐  ║\n║  │  vibe_cockpit    main  ✓ clean  0↑ 0↓                                  │  ║\n║  │  dcg             main  ! dirty   3 modified                            │  ║\n║  │  beads_rust      main  ✓ clean  2↑ 0↓                                  │  ║\n║  └────────────────────────────────────────────────────────────────────────┘  ║\n║                                                                              ║\n╠══════════════════════════════════════════════════════════════════════════════╣\n║ [?]Help [q]Quit [r]Refresh [Enter]Detail [a]Accounts [g]Repos [m]Mail [b]Beads║\n╚══════════════════════════════════════════════════════════════════════════════╝\n```\n\n## Key Bindings\n```rust\n// crates/vc_tui/src/input.rs\n\npub enum Action {\n    Quit,\n    Help,\n    Refresh,\n    DrillDown,\n    Back,\n    SelectNext,\n    SelectPrev,\n    GoToAccounts,\n    GoToRepos,\n    GoToMail,\n    GoToBeads,\n    GoToAlerts,\n    Filter,\n}\n\npub fn handle_key(key: KeyEvent, state: &AppState) -> Option<Action> {\n    match key.code {\n        KeyCode::Char('q') | KeyCode::Esc => Some(Action::Quit),\n        KeyCode::Char('?') => Some(Action::Help),\n        KeyCode::Char('r') => Some(Action::Refresh),\n        KeyCode::Enter => Some(Action::DrillDown),\n        KeyCode::Backspace => Some(Action::Back),\n        KeyCode::Up | KeyCode::Char('k') => Some(Action::SelectPrev),\n        KeyCode::Down | KeyCode::Char('j') => Some(Action::SelectNext),\n        KeyCode::Char('a') => Some(Action::GoToAccounts),\n        KeyCode::Char('g') => Some(Action::GoToRepos),\n        KeyCode::Char('m') => Some(Action::GoToMail),\n        KeyCode::Char('b') => Some(Action::GoToBeads),\n        KeyCode::Char('e') => Some(Action::GoToAlerts),\n        KeyCode::Char('/') => Some(Action::Filter),\n        _ => None,\n    }\n}\n```\n\n## Render Loop\n```rust\n// crates/vc_tui/src/render.rs\n\nuse ratatui::{\n    layout::{Constraint, Direction, Layout, Rect},\n    style::{Color, Modifier, Style},\n    text::{Line, Span},\n    widgets::{Block, Borders, List, ListItem, Paragraph},\n    Frame,\n};\n\npub fn render_overview(f: &mut Frame, app: &App) {\n    let chunks = Layout::default()\n        .direction(Direction::Vertical)\n        .constraints([\n            Constraint::Length(3),  // Header\n            Constraint::Min(10),    // Main content\n            Constraint::Length(3),  // Footer\n        ])\n        .split(f.size());\n    \n    render_header(f, chunks[0], app);\n    render_main_content(f, chunks[1], app);\n    render_footer(f, chunks[2]);\n}\n\nfn render_header(f: &mut Frame, area: Rect, app: &App) {\n    let health_indicator = match app.state.overall_health() {\n        h if h > 0.8 => Span::styled(\"●\", Style::default().fg(Color::Green)),\n        h if h > 0.5 => Span::styled(\"◐\", Style::default().fg(Color::Yellow)),\n        _ => Span::styled(\"○\", Style::default().fg(Color::Red)),\n    };\n    \n    let title = Line::from(vec![\n        Span::styled(\"VIBE COCKPIT\", Style::default().add_modifier(Modifier::BOLD)),\n        Span::raw(\"  \"),\n        health_indicator,\n    ]);\n    \n    let header = Paragraph::new(title)\n        .block(Block::default().borders(Borders::ALL));\n    f.render_widget(header, area);\n}\n```\n\n## Color Scheme (Neural Theme)\n```rust\n// crates/vc_tui/src/theme.rs\n\npub struct Theme {\n    pub bg_primary: Color,\n    pub bg_secondary: Color,\n    pub healthy: Color,\n    pub warning: Color,\n    pub critical: Color,\n    pub claude: Color,\n    pub codex: Color,\n    pub gemini: Color,\n}\n\nimpl Default for Theme {\n    fn default() -> Self {\n        Self {\n            bg_primary: Color::Rgb(13, 17, 23),\n            bg_secondary: Color::Rgb(22, 27, 34),\n            healthy: Color::Rgb(63, 185, 80),\n            warning: Color::Rgb(210, 153, 34),\n            critical: Color::Rgb(248, 81, 73),\n            claude: Color::Rgb(217, 119, 87),\n            codex: Color::Rgb(16, 163, 127),\n            gemini: Color::Rgb(66, 133, 244),\n        }\n    }\n}\n```\n\n## Data Queries for Overview\n```rust\n// crates/vc_query/src/overview.rs\n\npub async fn get_machine_summary(store: &Store) -> Vec<MachineSummary> {\n    let query = r#\"\n        SELECT \n            m.machine_id,\n            m.last_seen_at,\n            s.cpu_total,\n            s.mem_used_bytes * 100.0 / s.mem_total_bytes as mem_pct,\n            s.load5\n        FROM machines m\n        LEFT JOIN (\n            SELECT machine_id, cpu_total, mem_used_bytes, mem_total_bytes, load5\n            FROM sys_samples\n            WHERE (machine_id, collected_at) IN (\n                SELECT machine_id, MAX(collected_at)\n                FROM sys_samples\n                GROUP BY machine_id\n            )\n        ) s ON m.machine_id = s.machine_id\n        ORDER BY m.machine_id\n    \"#;\n    \n    // Execute and map to MachineSummary\n}\n```\n\n## Acceptance Criteria\n- [ ] TUI launches with `vc ui`\n- [ ] Overview screen shows machines with latest metrics\n- [ ] Health indicator updates based on data\n- [ ] Navigation works (arrow keys, vim keys)\n- [ ] Graceful exit with q/Esc\n- [ ] Colors render correctly\n\n## Notes for Future Self\n- Start simple, add charts later\n- Keep render functions pure (data in -> widgets out)\n- Consider refresh interval configuration\n- Test on different terminal sizes\n\n## E2E & Logging\n- Add/extend `tests/e2e/tui/test_<screen>.sh` with expect/pexpect.\n- Capture logs and screenshots on failure; emit JSON summary in tests/logs/.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:23:11.268745777Z","created_by":"ubuntu","updated_at":"2026-01-28T18:12:51.006308523Z","closed_at":"2026-01-28T18:12:51.005511321Z","close_reason":"done","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-33h.3","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:23:48.081797968Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.3","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T16:23:47.979812132Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.3","depends_on_id":"bd-2yb","type":"blocks","created_at":"2026-01-27T16:59:31.918343077Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.3","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:21.442271860Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.3","depends_on_id":"bd-33h","type":"parent-child","created_at":"2026-01-27T16:23:11.268745777Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.3","depends_on_id":"bd-33h.1","type":"blocks","created_at":"2026-01-27T16:59:29.870625093Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.3","depends_on_id":"bd-33h.2","type":"blocks","created_at":"2026-01-27T16:59:30.881390261Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-33h.4","title":"Implement vc robot status command","description":"# Task: Implement vc robot status command\n\n## What to Build\nThe `vc robot status` command that returns a machine-parseable JSON summary of system state for agent consumption.\n\n## Command Interface\n```bash\nvc robot status --json         # Full status\nvc robot status --format toon  # Token-optimized compact format\n```\n\n## JSON Output Schema\n```json\n{\n  \"schema_version\": \"vc.robot.status.v1\",\n  \"generated_at\": \"2026-01-27T00:00:00Z\",\n  \"data\": {\n    \"fleet\": {\n      \"total_machines\": 4,\n      \"online\": 3,\n      \"offline\": 1,\n      \"health_score\": 0.85\n    },\n    \"machines\": [\n      {\n        \"id\": \"orko\",\n        \"status\": \"online\",\n        \"last_seen\": \"2026-01-27T00:00:00Z\",\n        \"health_score\": 0.91,\n        \"metrics\": {\n          \"cpu_pct\": 45.2,\n          \"mem_pct\": 68.0,\n          \"load5\": 1.8,\n          \"disk_free_pct\": 35.0\n        },\n        \"top_issue\": null\n      },\n      {\n        \"id\": \"gpu-box\",\n        \"status\": \"offline\",\n        \"last_seen\": \"2026-01-26T18:00:00Z\",\n        \"health_score\": 0.0,\n        \"metrics\": null,\n        \"top_issue\": \"no_response\"\n      }\n    ],\n    \"repos\": {\n      \"total\": 15,\n      \"dirty\": 2,\n      \"ahead\": 3,\n      \"behind\": 1\n    },\n    \"alerts\": {\n      \"critical\": 0,\n      \"high\": 1,\n      \"medium\": 2,\n      \"low\": 0\n    }\n  },\n  \"staleness\": {\n    \"sysmoni\": 40,\n    \"ru\": 120\n  },\n  \"warnings\": []\n}\n```\n\n## Implementation\n```rust\n// crates/vc_cli/src/commands/robot_status.rs\n\nuse crate::robot::RobotEnvelope;\nuse vc_query::{get_fleet_summary, get_machine_summaries, get_repo_summary, get_alert_summary};\n\n#[derive(Serialize)]\npub struct StatusData {\n    pub fleet: FleetSummary,\n    pub machines: Vec<MachineStatus>,\n    pub repos: RepoSummary,\n    pub alerts: AlertSummary,\n}\n\n#[derive(Serialize)]\npub struct FleetSummary {\n    pub total_machines: u32,\n    pub online: u32,\n    pub offline: u32,\n    pub health_score: f64,\n}\n\n#[derive(Serialize)]\npub struct MachineStatus {\n    pub id: String,\n    pub status: String,  // \"online\" | \"offline\" | \"degraded\"\n    pub last_seen: DateTime<Utc>,\n    pub health_score: f64,\n    pub metrics: Option<MachineMetrics>,\n    pub top_issue: Option<String>,\n}\n\n#[derive(Serialize)]\npub struct MachineMetrics {\n    pub cpu_pct: f64,\n    pub mem_pct: f64,\n    pub load5: f64,\n    pub disk_free_pct: f64,\n}\n\n#[derive(Serialize)]\npub struct RepoSummary {\n    pub total: u32,\n    pub dirty: u32,\n    pub ahead: u32,\n    pub behind: u32,\n}\n\n#[derive(Serialize)]\npub struct AlertSummary {\n    pub critical: u32,\n    pub high: u32,\n    pub medium: u32,\n    pub low: u32,\n}\n\npub async fn robot_status(store: &Store) -> RobotEnvelope<StatusData> {\n    let fleet = get_fleet_summary(store).await.unwrap_or_default();\n    let machines = get_machine_summaries(store).await.unwrap_or_default();\n    let repos = get_repo_summary(store).await.unwrap_or_default();\n    let alerts = get_alert_summary(store).await.unwrap_or_default();\n    \n    let staleness = calculate_staleness(store).await;\n    \n    RobotEnvelope::new(\n        \"vc.robot.status.v1\",\n        StatusData { fleet, machines, repos, alerts },\n    )\n    .with_staleness(staleness)\n}\n\nasync fn calculate_staleness(store: &Store) -> HashMap<String, u64> {\n    // Query last collection time per source\n    let query = r#\"\n        SELECT source, MAX(updated_at) as last_update\n        FROM ingestion_cursors\n        GROUP BY source\n    \"#;\n    \n    // Calculate seconds since last update for each source\n    // Return as HashMap<source_name, seconds_since_update>\n}\n```\n\n## TOON Format\n```rust\n// Token-optimized output for constrained agents\n\nimpl ToToon for StatusData {\n    fn to_toon(&self) -> String {\n        // Format: F:4on1off|orko:on,cpu45,mem68|syd:on,cpu32|...\n        // AL:1h,2m|RP:15t,2d,3a,1b\n        \n        let mut parts = vec![];\n        \n        // Fleet summary\n        parts.push(format!(\n            \"F:{}on{}off\",\n            self.fleet.online,\n            self.fleet.offline\n        ));\n        \n        // Machine summaries (abbreviated)\n        let machines: Vec<String> = self.machines.iter().map(|m| {\n            let status = if m.status == \"online\" { \"on\" } else { \"off\" };\n            if let Some(metrics) = &m.metrics {\n                format!(\"{}:{},cpu{:.0},mem{:.0}\", \n                    abbreviate(&m.id), status, metrics.cpu_pct, metrics.mem_pct)\n            } else {\n                format!(\"{}:{}\", abbreviate(&m.id), status)\n            }\n        }).collect();\n        parts.push(machines.join(\"|\"));\n        \n        // Alerts\n        parts.push(format!(\n            \"AL:{}c,{}h,{}m,{}l\",\n            self.alerts.critical, self.alerts.high, \n            self.alerts.medium, self.alerts.low\n        ));\n        \n        // Repos\n        parts.push(format!(\n            \"RP:{}t,{}d,{}a,{}b\",\n            self.repos.total, self.repos.dirty,\n            self.repos.ahead, self.repos.behind\n        ));\n        \n        parts.join(\"\\n\")\n    }\n}\n\nfn abbreviate(name: &str) -> String {\n    // Shorten machine names: \"sydneymc\" -> \"syd\", \"mac-mini\" -> \"mac\"\n    name.chars().take(3).collect()\n}\n```\n\n## Acceptance Criteria\n- [ ] `vc robot status` returns valid JSON\n- [ ] Schema version included in output\n- [ ] Staleness calculated per source\n- [ ] TOON format produces ~60% token reduction\n- [ ] Handles empty database gracefully\n\n## Notes for Future Self\n- TOON format needs documentation for agent parsing\n- Consider adding `--since` flag for delta output\n- Staleness threshold should be configurable\n\n## Testing & Logging\n- Clap parsing tests for flags/subcommands and invalid input.\n- JSON schema compliance tests with golden outputs.\n- Error-path tests (missing config/db, read-only store).\n- E2E: tests/e2e/cli/test_<cmd>.sh with JSON summary and stderr capture.\n\n## Health Score Integration (Graceful)\n- Use health_score if available; otherwise compute a basic score from sysmoni + fallback probe only.\n- Mark missing factors in the output `warnings` field so agents know the score is partial.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:23:39.253087364Z","created_by":"ubuntu","updated_at":"2026-01-28T18:22:32.260950752Z","closed_at":"2026-01-28T18:22:32.260356422Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-33h.4","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T16:23:48.178474233Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.4","depends_on_id":"bd-2yb","type":"blocks","created_at":"2026-01-27T16:59:32.992763115Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.4","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:22.133939720Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.4","depends_on_id":"bd-33h","type":"parent-child","created_at":"2026-01-27T16:23:39.253087364Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.4","depends_on_id":"bd-33h.1","type":"blocks","created_at":"2026-01-27T16:23:48.277907592Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-33h.4","depends_on_id":"bd-33h.2","type":"blocks","created_at":"2026-01-27T16:23:48.373361466Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3e2","title":"Phase 5: Web Dashboard + Autopilot","description":"# Phase 5: Web Dashboard + Autopilot\n\n## Purpose\nBuild the web interface (Next.js static + Rust JSON API) and implement autopilot modes for automated remediation. This phase adds accessibility for users who prefer web over TUI.\n\n## Key Deliverables\n1. **Next.js Static Build**: React-based dashboard compiled to static assets\n2. **Rust Web Server**: axum serving static files + JSON API endpoints\n3. **JSON API**: Full API surface for health, machines, repos, accounts, alerts, oracle, guardian\n4. **Autopilot System**: Configurable modes (off, suggest, execute-safe, execute-with-approval)\n5. **Knowledge Base Storage**: gotchas table and basic CRUD\n6. **Agent DNA Profiling MVP**: Initial behavioral fingerprinting\n\n## Success Criteria\n- Web dashboard accessible at localhost:8080\n- All TUI data visible in web\n- Autopilot suggest mode generating actionable recommendations\n- Gotchas stored and retrievable\n- Agent DNA profiles building over time\n\n## Technical Context\n\n### Why Static Next.js + Rust API?\nThis architecture avoids running Node in production:\n- Next.js configured for static export (`output: 'export'`)\n- Rust serves static files from embedded assets or disk\n- API endpoints backed directly by DuckDB queries\n- Simplest deployment: single vc binary\n\nIf SSR needed later: run Node standalone as managed subprocess with reverse proxy.\n\n### API Endpoint Design\nAll endpoints return JSON with consistent envelope:\n```json\n{\n  \"schema_version\": \"vc.api.<resource>.v1\",\n  \"generated_at\": \"2026-01-27T00:00:00Z\",\n  \"data\": { ... },\n  \"warnings\": []\n}\n```\n\nKey endpoints:\n- `GET /api/health` - Global + per-machine health\n- `GET /api/machines/:id/overview` - System + alerts + processes\n- `GET /api/accounts` - caut/caam merged view + recommendations\n- `GET /api/oracle/forecasts` - Rate limit + failure predictions\n- `GET /api/guardian/status` - Healing protocol status\n\n### Autopilot Modes\n```toml\n[autopilot]\nmode = \"suggest\"  # off | suggest | execute-safe | execute-with-approval\n\n[autopilot.safe_commands]\nallow = [\n    \"caam recommend\",\n    \"caam limits\",\n    \"pt robot plan\",\n    \"vc account swap *\",\n]\n```\n\n- **suggest**: Show recommendations in UI, no execution\n- **execute-safe**: Run allowlisted commands automatically\n- **execute-with-approval**: Require confirmation step\n\n### Agent DNA Profiling MVP\nInitial metrics to track per agent:\n- Average velocity (tokens/min)\n- Task completion rate\n- Error rate\n- Recovery speed\n- Time-of-day performance patterns\n\nStored in DuckDB, queryable for \"best agent for task type X\" recommendations.\n\n## Dependencies\n- Requires Phase 4 (all collectors, alerts, guardian infrastructure)\n\n## Estimated Scope\n- ~2-3 weeks\n- Major milestone: web accessibility + automated operations\n\n## Testing & E2E Expectations\n- Every deliverable in this phase must add unit tests per bd-1x9.\n- Extend bd-30z with phase-specific e2e scenarios and structured logs.\n- Phase completion requires `cargo test --workspace` + phase e2e suite with JSON summaries in tests/logs/.\n","status":"open","priority":1,"issue_type":"epic","created_at":"2026-01-27T16:18:35.994542371Z","created_by":"ubuntu","updated_at":"2026-01-27T17:02:27.321136557Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3e2","depends_on_id":"bd-2xm","type":"blocks","created_at":"2026-01-27T16:19:04.400858640Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3e2.1","title":"Build Next.js web dashboard (static export first, SSR optional)","description":"## Overview\nBuild the web dashboard in Next.js with a static export as the primary deployment path. The Rust server (vc_web) serves the exported assets and provides JSON APIs. SSR is optional later for real-time views, but not required for MVP.\n\n## Core Requirements\n- Next.js app with pages: Overview, Machines, Repos, Accounts, Sessions, Mail, Alerts, Guardian, Oracle, Settings.\n- Static export (`output: 'export'`) with predictable asset paths.\n- Data fetching via vc_web JSON API with caching + staleness indicators.\n- No runtime Node dependency in the vc binary; static assets served by Rust.\n\n## Optional SSR Path (Later)\n- Document how to run a separate Node process for SSR if needed.\n- Gate via config (default off) and keep API surface identical.\n\n## UX Requirements\n- Fast first paint (static HTML + skeletons).\n- Clear stale-data badges and last-updated timestamps.\n- Severity color mapping aligned with TUI.\n\n## Testing & Logging\nUnit/Component tests:\n- React Testing Library for critical components (health cards, tables, filters).\n- Snapshot tests for layouts at common breakpoints.\n\nAPI contract tests:\n- Validate JSON envelopes from vc_web against schema fixtures.\n\nE2E:\n- Playwright tests for navigation, filters, and drill-downs.\n- Capture console errors, network failures, and screenshots on failure.\n- Emit JSON summary per run in `tests/logs/` (see bd-30z).","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:33:08.053789094Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:22.831652741Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3e2.1","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:22.831508048Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.1","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T16:33:08.053789094Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.1","depends_on_id":"bd-3e2.2","type":"blocks","created_at":"2026-01-27T17:00:03.920624142Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3e2.2","title":"Implement Rust axum web server (vc_web crate)","description":"## Overview\nBuild the Rust backend that serves the JSON API for the web dashboard, proxying data from DuckDB.\n\n## Background\nThe Next.js frontend needs a JSON API to fetch data. This Rust server provides that API, querying DuckDB and formatting responses. It runs alongside the collector daemon.\n\n## Project Structure\n```\nvc_web/\n├── Cargo.toml\n└── src/\n    ├── lib.rs\n    ├── server.rs         # Main server setup\n    ├── routes/\n    │   ├── mod.rs\n    │   ├── overview.rs   # /api/overview\n    │   ├── machines.rs   # /api/machines/*\n    │   ├── accounts.rs   # /api/accounts/*\n    │   ├── sessions.rs   # /api/sessions/*\n    │   ├── alerts.rs     # /api/alerts/*\n    │   ├── guardian.rs   # /api/guardian/*\n    │   └── websocket.rs  # /ws for live updates\n    ├── auth.rs           # Optional authentication\n    └── error.rs          # Error types\n```\n\n## Server Setup\n```rust\n// src/server.rs\nuse axum::{\n    routing::{get, post},\n    Router,\n    Extension,\n};\nuse tower_http::cors::CorsLayer;\n\npub struct WebServer {\n    store: Arc<VcStore>,\n    config: WebConfig,\n}\n\nimpl WebServer {\n    pub fn new(store: Arc<VcStore>, config: WebConfig) -> Self {\n        Self { store, config }\n    }\n    \n    pub async fn run(&self) -> Result<()> {\n        let app = Router::new()\n            // Overview\n            .route(\"/api/overview\", get(routes::overview::get_overview))\n            \n            // Machines\n            .route(\"/api/machines\", get(routes::machines::list))\n            .route(\"/api/machines/:id\", get(routes::machines::get))\n            .route(\"/api/machines/:id/collectors\", get(routes::machines::collectors))\n            \n            // Accounts\n            .route(\"/api/accounts\", get(routes::accounts::list))\n            .route(\"/api/accounts/:id/usage\", get(routes::accounts::usage))\n            \n            // Sessions\n            .route(\"/api/sessions\", get(routes::sessions::list))\n            .route(\"/api/sessions/:id\", get(routes::sessions::get))\n            \n            // Alerts\n            .route(\"/api/alerts\", get(routes::alerts::list))\n            .route(\"/api/alerts/:id/acknowledge\", post(routes::alerts::acknowledge))\n            .route(\"/api/alerts/rules\", get(routes::alerts::rules))\n            \n            // Guardian\n            .route(\"/api/guardian/status\", get(routes::guardian::status))\n            .route(\"/api/guardian/pending\", get(routes::guardian::pending))\n            .route(\"/api/guardian/approve/:id\", post(routes::guardian::approve))\n            .route(\"/api/guardian/runs\", get(routes::guardian::runs))\n            \n            // WebSocket for live updates\n            .route(\"/ws\", get(routes::websocket::handler))\n            \n            // Middleware\n            .layer(CorsLayer::permissive())\n            .layer(Extension(self.store.clone()));\n        \n        let addr = format\\!(\"{}:{}\", self.config.host, self.config.port);\n        tracing::info\\!(\"Starting web server on {}\", addr);\n        \n        axum::Server::bind(&addr.parse()?)\n            .serve(app.into_make_service())\n            .await?;\n        \n        Ok(())\n    }\n}\n```\n\n## Route Handlers\n```rust\n// src/routes/overview.rs\nuse axum::{Extension, Json};\n\n#[derive(Serialize)]\npub struct Overview {\n    active_agents: u32,\n    rate_limit_pct: f64,\n    active_alerts: u32,\n    machines_online: u32,\n    machines_total: u32,\n    recent_alerts: Vec<AlertSummary>,\n    usage_history: Vec<UsagePoint>,\n}\n\npub async fn get_overview(\n    Extension(store): Extension<Arc<VcStore>>,\n) -> Result<Json<Overview>, AppError> {\n    let active_agents = store.query_scalar::<u32>(\n        \"SELECT COUNT(*) FROM pt_processes WHERE category = \\\"agent\\\" AND ended_at IS NULL\"\n    ).await?;\n    \n    let rate_limit_pct = store.query_scalar::<f64>(\n        \"SELECT MAX(usage_pct) FROM caam_accounts\"\n    ).await?;\n    \n    let active_alerts = store.query_scalar::<u32>(\n        \"SELECT COUNT(*) FROM alert_history WHERE resolved_at IS NULL\"\n    ).await?;\n    \n    let machines = store.query::<(u32, u32)>(\n        \"SELECT COUNT(*) FILTER (WHERE status = \\\"online\\\"), COUNT(*) FROM machines\"\n    ).await?;\n    \n    let recent_alerts = store.query_vec::<AlertSummary>(\n        \"SELECT id, severity, title, fired_at FROM alert_history ORDER BY fired_at DESC LIMIT 5\"\n    ).await?;\n    \n    let usage_history = store.query_vec::<UsagePoint>(\n        \"SELECT date_trunc(\\\"hour\\\", ts) as time, AVG(usage_pct) as usage, MAX(limit_pct) as limit\n         FROM caut_usage WHERE ts > now() - INTERVAL 24 HOUR GROUP BY 1 ORDER BY 1\"\n    ).await?;\n    \n    Ok(Json(Overview {\n        active_agents,\n        rate_limit_pct,\n        active_alerts,\n        machines_online: machines.0,\n        machines_total: machines.1,\n        recent_alerts,\n        usage_history,\n    }))\n}\n```\n\n## WebSocket for Live Updates\n```rust\n// src/routes/websocket.rs\nuse axum::extract::ws::{WebSocket, WebSocketUpgrade};\nuse futures::{SinkExt, StreamExt};\n\npub async fn handler(\n    ws: WebSocketUpgrade,\n    Extension(store): Extension<Arc<VcStore>>,\n) -> impl IntoResponse {\n    ws.on_upgrade(|socket| handle_socket(socket, store))\n}\n\nasync fn handle_socket(mut socket: WebSocket, store: Arc<VcStore>) {\n    let mut interval = tokio::time::interval(Duration::from_secs(5));\n    \n    loop {\n        interval.tick().await;\n        \n        // Send updates\n        let update = serde_json::json\\!({\n            \"type\": \"update\",\n            \"alerts\": store.get_active_alerts().await.unwrap_or_default(),\n            \"machines\": store.get_machine_statuses().await.unwrap_or_default(),\n        });\n        \n        if socket.send(Message::Text(update.to_string())).await.is_err() {\n            break;\n        }\n    }\n}\n```\n\n## Error Handling\n```rust\n// src/error.rs\nuse axum::{response::{IntoResponse, Response}, http::StatusCode, Json};\n\npub struct AppError(anyhow::Error);\n\nimpl IntoResponse for AppError {\n    fn into_response(self) -> Response {\n        let body = Json(serde_json::json\\!({\n            \"error\": self.0.to_string()\n        }));\n        (StatusCode::INTERNAL_SERVER_ERROR, body).into_response()\n    }\n}\n\nimpl<E: Into<anyhow::Error>> From<E> for AppError {\n    fn from(err: E) -> Self {\n        Self(err.into())\n    }\n}\n```\n\n## Dependencies\n```toml\n[dependencies]\naxum = { version = \"0.7\", features = [\"ws\"] }\ntower-http = { version = \"0.5\", features = [\"cors\"] }\ntokio = { version = \"1\", features = [\"full\"] }\nserde = { version = \"1\", features = [\"derive\"] }\nserde_json = \"1\"\ntracing = \"0.1\"\n```\n\n## Acceptance Criteria\n- [ ] Axum server serves JSON API on configurable port\n- [ ] All route handlers query DuckDB via VcStore\n- [ ] WebSocket endpoint pushes live updates\n- [ ] CORS configured for frontend access\n- [ ] Proper error responses with JSON bodies\n- [ ] Request logging via tracing\n- [ ] Graceful shutdown handling\n\n## Testing & Logging\n- Unit tests for validation, defaults, or timeout behavior.\n- Integration tests with temporary DuckDB/fixtures.\n- Failure-mode tests (invalid config, corrupt db, partial output).\n- E2E: add scenario scripts under `tests/e2e/<area>/` with structured logs.\n","notes":"MaroonIsland: Enhanced vc_web with real data integration. Added VcStore/QueryBuilder integration, WebServer struct with config-driven bind, endpoints for overview/machines/accounts/sessions/guardian, proper error handling with IntoResponse, pagination support. Fixed axum 0.8 route syntax. ~30 tests. Awaiting cargo lock to verify tests pass.","status":"in_progress","priority":1,"issue_type":"task","owner":"maroonisland","created_at":"2026-01-27T16:33:36.128578886Z","created_by":"ubuntu","updated_at":"2026-01-28T18:24:47.581140394Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3e2.2","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T17:00:02.026694724Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.2","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T17:00:02.939064305Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.2","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T16:33:36.128578886Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3e2.3","title":"Implement Autopilot mode","description":"## Overview\nBuild Autopilot mode that enables fully autonomous agent fleet operation with intelligent decision-making.\n\n## Background\nAutopilot combines Oracle predictions, Guardian remediation, and collected telemetry to make autonomous decisions about agent operation: when to switch accounts, when to pause work, when to scale up/down.\n\n## Autopilot Capabilities\n\n### 1. Preemptive Account Switching\n```rust\npub struct AccountAutopilot {\n    oracle: Arc<Oracle>,\n    caam: Arc<CaamClient>,\n    config: AutopilotConfig,\n}\n\nimpl AccountAutopilot {\n    /// Check if we should preemptively switch accounts\n    pub async fn check_switch(&self) -> Option<SwitchRecommendation> {\n        // Get current account state\n        let current = self.caam.current_account().await?;\n        \n        // Get Oracle prediction\n        let forecast = self.oracle.forecast_rate_limit(\n            &current.program,\n            &current.account_id,\n            Duration::from_mins(30)\n        ).await?;\n        \n        // If predicted to hit limit soon, recommend switch\n        if forecast.will_hit_limit && forecast.time_to_limit < Duration::from_mins(15) {\n            let alternatives = self.caam.list_accounts(&current.program).await?\n                .into_iter()\n                .filter(|a| a.account_id != current.account_id)\n                .filter(|a| a.usage_pct < self.config.switch_threshold)\n                .collect::<Vec<_>>();\n            \n            if let Some(best) = alternatives.first() {\n                return Some(SwitchRecommendation {\n                    from: current.clone(),\n                    to: best.clone(),\n                    reason: format!(\n                        \"Predicted to hit rate limit in {} mins\",\n                        forecast.time_to_limit.as_mins()\n                    ),\n                    confidence: forecast.confidence,\n                });\n            }\n        }\n        None\n    }\n    \n    /// Execute account switch if autopilot is enabled\n    pub async fn maybe_switch(&self) -> Result<Option<SwitchEvent>> {\n        if !self.config.enabled {\n            return Ok(None);\n        }\n        \n        if let Some(rec) = self.check_switch().await {\n            if rec.confidence >= self.config.min_confidence {\n                self.caam.switch_account(&rec.to.account_id).await?;\n                return Ok(Some(SwitchEvent::from_recommendation(rec)));\n            }\n        }\n        Ok(None)\n    }\n}\n```\n\n### 2. Workload Management\n```rust\npub struct WorkloadAutopilot {\n    store: Arc<VcStore>,\n    ntm: Arc<NtmClient>,\n    config: AutopilotConfig,\n}\n\nimpl WorkloadAutopilot {\n    /// Balance workload across available agents/machines\n    pub async fn balance_workload(&self) -> Result<Vec<BalanceAction>> {\n        let mut actions = Vec::new();\n        \n        // Get current agent distribution\n        let agents = self.store.query_vec::<AgentStatus>(\n            \"SELECT machine_id, COUNT(*) as agent_count, AVG(cpu_percent) as avg_cpu\n             FROM pt_processes WHERE category = \\\"agent\\\" AND ended_at IS NULL\n             GROUP BY machine_id\"\n        ).await?;\n        \n        // Find overloaded machines\n        let overloaded = agents.iter()\n            .filter(|a| a.avg_cpu > self.config.cpu_threshold)\n            .collect::<Vec<_>>();\n        \n        // Find underutilized machines\n        let underutilized = agents.iter()\n            .filter(|a| a.avg_cpu < self.config.cpu_min_threshold)\n            .collect::<Vec<_>>();\n        \n        // Recommend migrations\n        for over in &overloaded {\n            if let Some(under) = underutilized.first() {\n                actions.push(BalanceAction::MigrateAgent {\n                    from_machine: over.machine_id.clone(),\n                    to_machine: under.machine_id.clone(),\n                    reason: \"Load balancing\".into(),\n                });\n            }\n        }\n        \n        Ok(actions)\n    }\n}\n```\n\n### 3. Cost Optimization\n```rust\npub struct CostAutopilot {\n    store: Arc<VcStore>,\n    config: AutopilotConfig,\n}\n\nimpl CostAutopilot {\n    /// Analyze spending and suggest optimizations\n    pub async fn analyze_costs(&self) -> Result<CostAnalysis> {\n        let daily_cost = self.store.query_scalar::<f64>(\n            \"SELECT SUM(cost_estimate) FROM caut_usage WHERE ts > current_date\"\n        ).await?;\n        \n        let by_model = self.store.query_vec::<ModelCost>(\n            \"SELECT model, SUM(cost_estimate) as cost, COUNT(*) as requests\n             FROM caut_usage WHERE ts > current_date - 7\n             GROUP BY model ORDER BY cost DESC\"\n        ).await?;\n        \n        let recommendations = Vec::new();\n        \n        // Check for expensive model usage that could use cheaper alternatives\n        for model_cost in &by_model {\n            if model_cost.model.contains(\"opus\") && model_cost.cost > self.config.opus_daily_limit {\n                recommendations.push(CostRecommendation {\n                    title: \"Consider using Sonnet for routine tasks\".into(),\n                    potential_savings: model_cost.cost * 0.5,\n                    severity: \"info\".into(),\n                });\n            }\n        }\n        \n        Ok(CostAnalysis {\n            daily_cost,\n            projected_monthly: daily_cost * 30.0,\n            by_model,\n            recommendations,\n        })\n    }\n}\n```\n\n## Autopilot Loop\n```rust\npub struct Autopilot {\n    account: AccountAutopilot,\n    workload: WorkloadAutopilot,\n    cost: CostAutopilot,\n    config: AutopilotConfig,\n}\n\nimpl Autopilot {\n    pub async fn run_loop(&self) {\n        let mut interval = tokio::time::interval(Duration::from_secs(60));\n        \n        loop {\n            interval.tick().await;\n            \n            if !self.config.enabled {\n                continue;\n            }\n            \n            // Account management\n            if let Ok(Some(switch)) = self.account.maybe_switch().await {\n                tracing::info!(\"Autopilot switched account: {:?}\", switch);\n            }\n            \n            // Workload balancing (less frequent)\n            if self.should_check_workload() {\n                if let Ok(actions) = self.workload.balance_workload().await {\n                    for action in actions {\n                        if self.config.auto_execute_balance {\n                            self.execute_balance_action(action).await;\n                        }\n                    }\n                }\n            }\n            \n            // Cost analysis (hourly)\n            if self.should_check_costs() {\n                if let Ok(analysis) = self.cost.analyze_costs().await {\n                    if analysis.daily_cost > self.config.daily_budget {\n                        self.trigger_cost_alert(analysis).await;\n                    }\n                }\n            }\n        }\n    }\n}\n```\n\n## Configuration\n```toml\n[autopilot]\nenabled = true\nmin_confidence = 0.8          # Minimum Oracle confidence for auto-actions\n\n[autopilot.accounts]\nswitch_threshold = 0.75       # Switch when usage exceeds this %\npreemptive_mins = 15          # Switch this many mins before predicted limit\n\n[autopilot.workload]\ncpu_threshold = 80.0          # Consider machine overloaded above this\ncpu_min_threshold = 20.0      # Consider machine underutilized below this\nauto_execute_balance = false  # Require approval for migrations\n\n[autopilot.cost]\ndaily_budget = 50.0           # Alert when daily cost exceeds\nopus_daily_limit = 20.0       # Recommend alternatives above this\n```\n\n## Acceptance Criteria\n- [ ] AccountAutopilot preemptively switches on predicted rate limits\n- [ ] WorkloadAutopilot identifies load imbalances\n- [ ] CostAutopilot tracks spending and recommends optimizations\n- [ ] Configurable thresholds and confidence levels\n- [ ] Manual override capability\n- [ ] Full audit trail of autopilot decisions\n- [ ] TUI/web indicators show autopilot status\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:34:07.344427902Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:23.408482438Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3e2.3","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T19:26:01.456922734Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.3","depends_on_id":"bd-2mv.5","type":"blocks","created_at":"2026-01-27T16:35:19.744732446Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.3","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T17:00:04.852497620Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.3","depends_on_id":"bd-2xm.6","type":"blocks","created_at":"2026-01-27T16:35:19.929419034Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.3","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:23.407846389Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.3","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T16:34:07.344427902Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3e2.4","title":"Implement Knowledge Base (vc_knowledge crate)","description":"## Overview\nBuild the knowledge base system that captures and indexes solutions, patterns, and learnings from agent sessions.\n\n## Background\nAs agents work on tasks, they discover solutions to problems, effective prompts, and useful patterns. The knowledge base captures this institutional knowledge for future reference.\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE knowledge_entries (\n    id INTEGER PRIMARY KEY,\n    entry_type VARCHAR NOT NULL,          -- solution, pattern, prompt, debug_log\n    title VARCHAR NOT NULL,\n    summary TEXT,\n    content TEXT NOT NULL,\n    source_session_id VARCHAR,            -- Link to cass session\n    source_file VARCHAR,\n    source_lines VARCHAR,                 -- \"10-25\" or NULL\n    tags VARCHAR[],\n    embedding FLOAT[1536],                -- For semantic search\n    created_at TIMESTAMP DEFAULT now(),\n    updated_at TIMESTAMP,\n    usefulness_score FLOAT DEFAULT 0.0,   -- Computed from feedback\n    view_count INTEGER DEFAULT 0,\n    applied_count INTEGER DEFAULT 0\n);\n\nCREATE TABLE knowledge_feedback (\n    id INTEGER PRIMARY KEY,\n    entry_id INTEGER REFERENCES knowledge_entries(id),\n    feedback_type VARCHAR,                -- helpful, not_helpful, outdated\n    session_id VARCHAR,\n    comment TEXT,\n    created_at TIMESTAMP DEFAULT now()\n);\n\n-- Full-text search index\nCREATE INDEX idx_knowledge_fts ON knowledge_entries \n    USING FTS(title, summary, content);\n```\n\n## Entry Types\n\n### 1. Solutions\n```json\n{\n  \"entry_type\": \"solution\",\n  \"title\": \"Fix DuckDB connection pool exhaustion\",\n  \"summary\": \"When DuckDB throws \\\"too many open connections\\\", wrap access in Arc<Mutex<>>\",\n  \"content\": \"## Problem\\nDuckDB was throwing connection errors...\\n\\n## Solution\\n```rust\\nlet conn = Arc::new(Mutex::new(Connection::open(path)?));\\n```\\n\\n## Why It Works\\nDuckDB embedded connections are not thread-safe...\",\n  \"tags\": [\"duckdb\", \"rust\", \"connection-pool\"],\n  \"source_session_id\": \"sess_abc123\"\n}\n```\n\n### 2. Patterns\n```json\n{\n  \"entry_type\": \"pattern\",\n  \"title\": \"Collector trait implementation pattern\",\n  \"summary\": \"Standard pattern for implementing new data collectors\",\n  \"content\": \"## Pattern\\n```rust\\nimpl Collector for FooCollector {\\n    const NAME: &str = \\\"foo\\\";\\n    ...\\n}\\n```\\n\\n## When to Use\\nWhen adding a new data source to vc...\",\n  \"tags\": [\"pattern\", \"collector\", \"rust\"]\n}\n```\n\n### 3. Debug Logs\n```json\n{\n  \"entry_type\": \"debug_log\",\n  \"title\": \"Debugging slow rch builds\",\n  \"summary\": \"Investigation of why rustc_codegen builds were taking 45+ seconds\",\n  \"content\": \"## Observation\\nBuilds suddenly slow...\\n\\n## Investigation\\n1. Checked CPU usage...\\n2. Found cache miss...\\n\\n## Root Cause\\nCache invalidation bug in rch v0.2.3\",\n  \"tags\": [\"debug\", \"rch\", \"performance\"]\n}\n```\n\n## Knowledge Capture\n```rust\npub struct KnowledgeCapture {\n    store: Arc<VcStore>,\n    embedder: Arc<Embedder>,\n}\n\nimpl KnowledgeCapture {\n    /// Capture a solution from a session\n    pub async fn capture_solution(\n        &self,\n        session_id: &str,\n        title: &str,\n        content: &str,\n        tags: Vec<String>,\n    ) -> Result<KnowledgeEntry> {\n        // Generate embedding for semantic search\n        let embedding = self.embedder.embed(content).await?;\n        \n        // Auto-generate summary\n        let summary = self.summarize(content).await?;\n        \n        let entry = KnowledgeEntry {\n            entry_type: EntryType::Solution,\n            title: title.to_string(),\n            summary,\n            content: content.to_string(),\n            source_session_id: Some(session_id.to_string()),\n            tags,\n            embedding,\n            ..Default::default()\n        };\n        \n        self.store.insert_knowledge_entry(&entry).await?;\n        Ok(entry)\n    }\n    \n    /// Auto-extract patterns from successful sessions\n    pub async fn extract_patterns(&self, session_id: &str) -> Result<Vec<KnowledgeEntry>> {\n        // Analyze session for reusable patterns\n        // This would use LLM analysis of the session transcript\n        todo!()\n    }\n}\n```\n\n## Knowledge Search\n```rust\npub struct KnowledgeSearch {\n    store: Arc<VcStore>,\n    embedder: Arc<Embedder>,\n}\n\nimpl KnowledgeSearch {\n    /// Semantic search for relevant knowledge\n    pub async fn search(&self, query: &str, limit: usize) -> Result<Vec<SearchResult>> {\n        let query_embedding = self.embedder.embed(query).await?;\n        \n        // Combine semantic and keyword search\n        let results = self.store.query_vec::<SearchResult>(r#\"\n            WITH semantic AS (\n                SELECT id, title, summary, entry_type,\n                       array_cosine_similarity(embedding, ?) as sim\n                FROM knowledge_entries\n                ORDER BY sim DESC\n                LIMIT ?\n            ),\n            keyword AS (\n                SELECT id, title, summary, entry_type,\n                       fts_main_knowledge_entries.match_bm25(id, ?) as score\n                FROM knowledge_entries\n                WHERE score > 0\n                LIMIT ?\n            )\n            SELECT DISTINCT id, title, summary, entry_type,\n                   COALESCE(s.sim, 0) * 0.6 + COALESCE(k.score, 0) * 0.4 as combined_score\n            FROM semantic s\n            FULL OUTER JOIN keyword k USING (id)\n            ORDER BY combined_score DESC\n            LIMIT ?\n        \"#).bind(&query_embedding).bind(limit * 2).bind(query).bind(limit * 2).bind(limit)\n           .await?;\n        \n        Ok(results)\n    }\n    \n    /// Search by tags\n    pub async fn search_by_tags(&self, tags: &[String]) -> Result<Vec<KnowledgeEntry>> {\n        self.store.query_vec::<KnowledgeEntry>(\n            \"SELECT * FROM knowledge_entries WHERE tags && ? ORDER BY usefulness_score DESC\"\n        ).bind(tags).await\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] Knowledge entries stored with embeddings\n- [ ] Solution, pattern, and debug_log types supported\n- [ ] Semantic search via embedding similarity\n- [ ] Keyword search via FTS\n- [ ] Combined ranking for best results\n- [ ] Feedback mechanism tracks usefulness\n- [ ] CLI commands for capture and search\n- [ ] Integration with session context\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:34:37.314830133Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:24.038898244Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3e2.4","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T17:00:05.700249847Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.4","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T17:00:06.716947540Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.4","depends_on_id":"bd-2mv.3","type":"blocks","created_at":"2026-01-27T16:35:20.114813204Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.4","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:24.038409673Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.4","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T16:34:37.314830133Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3e2.5","title":"Implement Agent DNA fingerprinting","description":"## Overview\nBuild Agent DNA - behavioral fingerprints that characterize agent performance patterns and preferences.\n\n## Background\nDifferent agents (and different configurations of the same agent) exhibit distinct behavioral patterns: token usage, error rates, preferred tools, coding styles. Agent DNA captures these patterns to enable comparison, optimization, and anomaly detection.\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE agent_dna (\n    dna_id VARCHAR PRIMARY KEY,\n    agent_program VARCHAR NOT NULL,       -- claude-code, codex, gemini\n    agent_model VARCHAR NOT NULL,         -- opus-4.5, gpt4, etc.\n    configuration_hash VARCHAR,           -- Hash of relevant config\n    computed_at TIMESTAMP DEFAULT now(),\n    \n    -- Token patterns\n    avg_tokens_per_turn FLOAT,\n    avg_input_output_ratio FLOAT,\n    token_variance FLOAT,\n    \n    -- Error patterns\n    error_rate FLOAT,\n    common_error_types JSON,              -- {\"parse_error\": 0.3, \"timeout\": 0.1}\n    recovery_rate FLOAT,                  -- How often errors are recovered\n    \n    -- Tool usage\n    tool_preferences JSON,                -- {\"Read\": 0.4, \"Edit\": 0.3, \"Bash\": 0.2}\n    tool_success_rates JSON,              -- Per-tool success rate\n    avg_tools_per_task FLOAT,\n    \n    -- Timing patterns\n    avg_response_time_ms FLOAT,\n    p95_response_time_ms FLOAT,\n    time_of_day_distribution JSON,        -- Hourly activity pattern\n    \n    -- Task patterns  \n    avg_task_completion_time_mins FLOAT,\n    task_success_rate FLOAT,\n    complexity_handling JSON,             -- Success by task complexity\n    \n    -- Session patterns\n    avg_session_duration_mins FLOAT,\n    avg_turns_per_session FLOAT,\n    session_abandonment_rate FLOAT,\n    \n    -- Embedding for similarity search\n    dna_embedding FLOAT[128]\n);\n\nCREATE TABLE dna_history (\n    id INTEGER PRIMARY KEY,\n    dna_id VARCHAR REFERENCES agent_dna(dna_id),\n    computed_at TIMESTAMP,\n    metrics JSON,                         -- Full metrics snapshot\n    change_summary VARCHAR                -- What changed\n);\n```\n\n## DNA Computation\n```rust\npub struct DnaComputer {\n    store: Arc<VcStore>,\n}\n\nimpl DnaComputer {\n    /// Compute DNA for an agent configuration\n    pub async fn compute_dna(\n        &self,\n        program: &str,\n        model: &str,\n        config_hash: Option<&str>,\n        time_range: TimeRange,\n    ) -> Result<AgentDna> {\n        let filter = format\\!(\n            \"program = \\\"{}\\\" AND model = \\\"{}\\\"\",\n            program, model\n        );\n        \n        // Token patterns\n        let token_stats = self.store.query_one::<TokenStats>(&format\\!(r#\"\n            SELECT \n                AVG(tokens_out) as avg_tokens,\n                AVG(tokens_in::FLOAT / NULLIF(tokens_out, 0)) as input_output_ratio,\n                STDDEV(tokens_out) as token_variance\n            FROM caut_usage\n            WHERE {}\n              AND ts BETWEEN ? AND ?\n        \"#, filter)).bind(time_range.start).bind(time_range.end).await?;\n        \n        // Error patterns\n        let error_stats = self.store.query_one::<ErrorStats>(&format\\!(r#\"\n            SELECT\n                COUNT(*) FILTER (WHERE error IS NOT NULL)::FLOAT / COUNT(*) as error_rate,\n                COUNT(*) FILTER (WHERE error IS NOT NULL AND next_success) / \n                    NULLIF(COUNT(*) FILTER (WHERE error IS NOT NULL), 0) as recovery_rate\n            FROM caut_usage\n            WHERE {}\n        \"#, filter)).await?;\n        \n        // Tool preferences (from session analysis)\n        let tool_prefs = self.store.query_vec::<ToolPref>(&format\\!(r#\"\n            SELECT tool_name, \n                   COUNT(*)::FLOAT / SUM(COUNT(*)) OVER () as preference,\n                   AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END) as success_rate\n            FROM session_tool_usage\n            WHERE {}\n            GROUP BY tool_name\n        \"#, filter)).await?;\n        \n        // Timing patterns\n        let timing = self.store.query_one::<TimingStats>(&format\\!(r#\"\n            SELECT\n                AVG(response_time_ms) as avg_response,\n                PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY response_time_ms) as p95\n            FROM caut_usage\n            WHERE {}\n        \"#, filter)).await?;\n        \n        // Combine into DNA\n        let dna = AgentDna {\n            dna_id: format\\!(\"{}:{}:{}\", program, model, config_hash.unwrap_or(\"default\")),\n            agent_program: program.to_string(),\n            agent_model: model.to_string(),\n            configuration_hash: config_hash.map(String::from),\n            avg_tokens_per_turn: token_stats.avg_tokens,\n            avg_input_output_ratio: token_stats.input_output_ratio,\n            error_rate: error_stats.error_rate,\n            recovery_rate: error_stats.recovery_rate,\n            tool_preferences: tool_prefs.into_iter()\n                .map(|t| (t.tool_name, t.preference))\n                .collect(),\n            avg_response_time_ms: timing.avg_response,\n            p95_response_time_ms: timing.p95,\n            // ... more fields\n            computed_at: Utc::now(),\n        };\n        \n        // Compute embedding for similarity\n        let dna = self.compute_embedding(dna).await?;\n        \n        // Store DNA\n        self.store.upsert_dna(&dna).await?;\n        \n        // Track history\n        self.store.insert_dna_history(&dna).await?;\n        \n        Ok(dna)\n    }\n}\n```\n\n## DNA Comparison\n```rust\nimpl DnaComputer {\n    /// Compare two agent DNAs\n    pub fn compare(&self, a: &AgentDna, b: &AgentDna) -> DnaComparison {\n        DnaComparison {\n            similarity: cosine_similarity(&a.dna_embedding, &b.dna_embedding),\n            differences: vec\\![\n                Difference::new(\"error_rate\", a.error_rate, b.error_rate),\n                Difference::new(\"avg_tokens\", a.avg_tokens_per_turn, b.avg_tokens_per_turn),\n                // ... more comparisons\n            ].into_iter().filter(|d| d.is_significant()).collect(),\n        }\n    }\n    \n    /// Find similar agents\n    pub async fn find_similar(&self, dna: &AgentDna, limit: usize) -> Result<Vec<(AgentDna, f64)>> {\n        let results = self.store.query_vec::<(AgentDna, f64)>(r#\"\n            SELECT *, array_cosine_similarity(dna_embedding, ?) as similarity\n            FROM agent_dna\n            WHERE dna_id \\!= ?\n            ORDER BY similarity DESC\n            LIMIT ?\n        \"#).bind(&dna.dna_embedding).bind(&dna.dna_id).bind(limit).await?;\n        \n        Ok(results)\n    }\n}\n```\n\n## Anomaly Detection\n```rust\nimpl DnaComputer {\n    /// Detect anomalies in recent agent behavior\n    pub async fn detect_anomalies(&self, dna_id: &str) -> Result<Vec<Anomaly>> {\n        let current = self.store.get_dna(dna_id).await?;\n        let history = self.store.get_dna_history(dna_id, 30).await?;  // Last 30 snapshots\n        \n        let mut anomalies = Vec::new();\n        \n        // Check each metric against historical baseline\n        let historical_error_rate: Vec<f64> = history.iter()\n            .map(|h| h.metrics.error_rate)\n            .collect();\n        let (mean, stddev) = stats::mean_stddev(&historical_error_rate);\n        \n        if (current.error_rate - mean).abs() > 2.0 * stddev {\n            anomalies.push(Anomaly {\n                metric: \"error_rate\".into(),\n                current: current.error_rate,\n                expected: mean,\n                deviation_sigmas: (current.error_rate - mean) / stddev,\n                severity: if (current.error_rate - mean) / stddev > 3.0 { \"high\" } else { \"medium\" }.into(),\n            });\n        }\n        \n        // Similar checks for other metrics...\n        \n        Ok(anomalies)\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] DNA computed from usage and session data\n- [ ] Token, error, tool, and timing patterns captured\n- [ ] DNA embedding enables similarity search\n- [ ] Comparison shows significant differences\n- [ ] Historical tracking detects drift\n- [ ] Anomaly detection finds unusual behavior\n- [ ] CLI and API for DNA queries\n- [ ] TUI/web visualization of DNA metrics\n\n## Testing & Logging\n- Deterministic unit tests with fixed seeds/time where applicable.\n- Regression tests against fixture datasets.\n- Safety tests: no side effects without allowlist/approval.\n- E2E: scenario scripts under tests/e2e/system with JSON summaries and log artifacts.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:35:11.257730343Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:24.740220094Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3e2.5","depends_on_id":"bd-2mv.1","type":"blocks","created_at":"2026-01-27T16:35:20.435771814Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.5","depends_on_id":"bd-2mv.3","type":"blocks","created_at":"2026-01-27T16:35:20.618540129Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.5","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:24.739393958Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3e2.5","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T16:35:11.257730343Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3hr","title":"Implement TOON (Token-Optimized Object Notation) output format","description":"Implement TOON output format for extremely token-constrained agent queries.\n\nMOTIVATION:\nTraditional JSON output can be 800+ tokens. TOON reduces this by 60-70% while preserving essential information for agent decision-making.\n\nTOON FORMAT SPECIFICATION:\n# Traditional JSON: 847 tokens\n{\"fleet\":{\"machines\":[{\"hostname\":\"orko\",\"status\":\"online\",\"agents\":{\"total\":15...\n\n# TOON: 312 tokens (63% reduction)\nF:4on1off|orko:on,15ag(14h),cpu45,mem68|sydneymc:on,12ag,cpu78|mac:on,8ag|gpu:on,10ag|bak:off\nAL:3[!ratelim@orko:95%,!zombie@syd:2,!disk@mac:89%]\nPR:[ratelim:swap@23m,fail:cc5@34%,cost:+$12@eod]\n\nFORMAT SECTIONS:\n- F: Fleet summary (machines online/offline, agent counts, key metrics)\n- AL: Alerts (severity!, type@location:value)\n- PR: Predictions (type:action@timeframe)\n- AC: Accounts (provider:pct%,ttl)\n- RP: Repos (name:dirty/clean,ahead/behind)\n- EV: Events (type@machine:count)\n\nIMPLEMENTATION:\n- Add --format toon option to all vc robot commands\n- Create vc_cli/src/toon.rs with serialization logic\n- Define abbreviation dictionary for common terms\n- Ensure format is parseable (delimiters: | for items, : for key-value, , for sub-items)\n- Include format version header: TOON1|\n\nCLI USAGE:\nvc robot health --format toon\nvc robot triage --format toon\nvc robot accounts --format toon\n\nPARSING NOTES (for agents):\n- Split on | for top-level sections\n- Split on , for items within section\n- Split on : for key-value pairs\n- ! prefix indicates alert/warning\n- @ indicates location/time relationship\n- % suffix indicates percentage\n\nTESTS:\n- Round-trip test: JSON -> TOON -> parsed -> verify values\n- Token count comparison tests\n- Format validation tests\n- Edge case handling (special characters, empty sections)\n\n## E2E & Logging\n- Add `tests/e2e/cli/test_toon_output.sh` covering key robot commands.\n- Log token counts and round-trip parse validation; emit JSON summary in tests/logs/.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:50:12.397779776Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:25.627432758Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3hr","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T16:54:32.489559204Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:25.627258831Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3hr","depends_on_id":"bd-3e2","type":"parent-child","created_at":"2026-01-27T16:55:48.549649555Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3nb","title":"Phase 3: Remote Machines","description":"# Phase 3: Remote Machines\n\n## Purpose\nExtend vc to monitor multiple machines via SSH. This phase transforms vc from a single-machine tool into a true fleet monitoring system.\n\n## Key Deliverables\n1. **Machine Inventory**: TOML-based machine configuration with SSH targets, tags, roles\n2. **SSH Runner**: Secure command execution with timeouts, BatchMode, connection pooling\n3. **Tool Probing**: Auto-detect which tools are available on each machine\n4. **Remote Collectors**: All existing collectors (sysmoni, ru, caut, caam) running remotely\n5. **TUI Per-Machine Drill-Down**: Navigate from fleet overview to individual machine details\n6. **machine_tool_capabilities Table**: Cache tool availability with TTL\n\n## Success Criteria\n- vc.toml can define multiple machines with SSH targets\n- Collectors run on remote machines via SSH\n- Tool availability auto-detected and cached\n- TUI shows fleet topology with machine health\n- SSH failures gracefully degraded (show \"stale data\" badge)\n\n## Technical Context\n\n### Machine Inventory Format\n```toml\n[[machines]]\nid = \"orko\"\nssh = \"local\"  # special-case for local execution\ntags = [\"primary\", \"claude\", \"codex\"]\n\n[[machines]]\nid = \"sydneymc\"\nssh = \"ubuntu@sydneymc.internal:22\"\ntags = [\"worker\", \"rch\", \"claude\"]\n```\n\n### SSH Execution Strategy\nMVP uses the `ssh` binary with strict options:\n- `BatchMode=yes` (no interactive prompts)\n- `StrictHostKeyChecking=accept-new` (TOFU for new hosts)\n- Configurable timeouts per command\n- Later: consider russh for native Rust SSH\n\n### Tool Probing Protocol\nOn first connect (and periodically after):\n1. Run `command -v <tool>` for each known tool\n2. If found, run `<tool> --version` to capture version\n3. Store in machine_tool_capabilities with 24-hour TTL\n4. Collectors check capabilities before attempting collection\n\n### Remote Acquisition Patterns\nFour patterns for data collection:\n1. Run remote command, read stdout JSON\n2. Read remote file (JSONL tail or SQLite query)\n3. Fetch export artifact via scp\n4. Fallback basic system probe (uptime, df, free)\n\n## Dependencies\n- Requires Phase 2 (all local collectors working, Oracle MVP functional)\n\n## Estimated Scope\n- ~2 weeks\n- Major milestone: vc becomes a multi-machine system\n\n## Testing & E2E Expectations\n- Every deliverable in this phase must add unit tests per bd-1x9.\n- Extend bd-30z with phase-specific e2e scenarios and structured logs.\n- Phase completion requires `cargo test --workspace` + phase e2e suite with JSON summaries in tests/logs/.\n","status":"open","priority":0,"issue_type":"epic","created_at":"2026-01-27T16:17:53.770875229Z","created_by":"ubuntu","updated_at":"2026-01-27T18:22:33.329952697Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3nb","depends_on_id":"bd-33h","type":"blocks","created_at":"2026-01-27T18:22:33.329797244Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3nb.1","title":"Implement machine inventory and registry","description":"## Overview\nCreate the machine registry system that tracks all known machines, their SSH credentials, installed tools, and health status.\n\n## Background\nvibe_cockpit needs to monitor multiple machines (local dev, Mac Minis, cloud VMs). Before we can collect data from remotes, we need a registry of machines and their configurations.\n\n## Data Model (DuckDB Schema)\n```sql\nCREATE TABLE machines (\n    machine_id VARCHAR PRIMARY KEY,      -- unique identifier\n    hostname VARCHAR NOT NULL,\n    display_name VARCHAR,\n    ssh_host VARCHAR,                    -- SSH connection string\n    ssh_user VARCHAR DEFAULT \"ubuntu\",\n    ssh_key_path VARCHAR,                -- path to SSH key\n    ssh_port INTEGER DEFAULT 22,\n    is_local BOOLEAN DEFAULT FALSE,\n    os_type VARCHAR,                     -- linux, darwin\n    arch VARCHAR,                        -- x86_64, aarch64\n    added_at TIMESTAMP DEFAULT now(),\n    last_seen_at TIMESTAMP,\n    last_probe_at TIMESTAMP,\n    status VARCHAR DEFAULT \"unknown\",    -- online, offline, unknown\n    tags VARCHAR[],                      -- for filtering/grouping\n    metadata JSON                        -- flexible extra data\n);\n\nCREATE TABLE machine_tools (\n    machine_id VARCHAR REFERENCES machines(machine_id),\n    tool_name VARCHAR,                   -- ntm, caut, rch, etc.\n    tool_path VARCHAR,                   -- /home/ubuntu/.cargo/bin/caut\n    tool_version VARCHAR,\n    is_available BOOLEAN DEFAULT TRUE,\n    probed_at TIMESTAMP,\n    PRIMARY KEY (machine_id, tool_name)\n);\n```\n\n## Configuration (vc_config)\n```toml\n# ~/.config/vc/config.toml\n[machines]\nlocal = { is_local = true, hostname = \"$(hostname)\" }\n\n[machines.mac-mini-1]\nssh_host = \"192.168.1.100\"\nssh_user = \"ubuntu\"\nssh_key_path = \"~/.ssh/id_ed25519\"\ntags = [\"mini\", \"builder\"]\n\n[machines.mac-mini-2]\nssh_host = \"192.168.1.101\"\nssh_user = \"ubuntu\"\ntags = [\"mini\", \"builder\"]\n\n[machines.hetzner-1]\nssh_host = \"hetzner1.example.com\"\ntags = [\"cloud\", \"worker\"]\n```\n\n## Implementation (vc_collect/src/machine.rs)\n```rust\npub struct MachineRegistry {\n    store: Arc<VcStore>,\n}\n\nimpl MachineRegistry {\n    pub async fn load_from_config(&self, config: &VcConfig) -> Result<()>;\n    pub async fn get_machine(&self, id: &str) -> Result<Option<Machine>>;\n    pub async fn list_machines(&self, filter: Option<MachineFilter>) -> Result<Vec<Machine>>;\n    pub async fn update_status(&self, id: &str, status: MachineStatus) -> Result<()>;\n    pub async fn record_tool(&self, id: &str, tool: ToolInfo) -> Result<()>;\n}\n\npub struct Machine {\n    pub machine_id: String,\n    pub hostname: String,\n    pub ssh_config: Option<SshConfig>,\n    pub is_local: bool,\n    pub status: MachineStatus,\n    pub tools: Vec<ToolInfo>,\n}\n```\n\n## CLI Commands\n```bash\n# List all machines\nvc machines list\nvc machines list --status online --tags builder\n\n# Add a new machine\nvc machines add mac-mini-3 --ssh ubuntu@192.168.1.102\n\n# Probe a machine for tools\nvc machines probe mac-mini-1\n\n# Show machine details\nvc machines show mac-mini-1\n\n# Robot mode output\nvc robot machines --format json\n```\n\n## Acceptance Criteria\n- [ ] Machines table created in DuckDB\n- [ ] Config parsing loads machine definitions\n- [ ] MachineRegistry provides CRUD operations\n- [ ] CLI commands work for list/add/show\n- [ ] Local machine auto-registered on startup\n- [ ] Machine status updated on successful connections\n\n## Testing & Logging\n- Unit tests for validation, defaults, or timeout behavior.\n- Integration tests with temporary DuckDB/fixtures.\n- Failure-mode tests (invalid config, corrupt db, partial output).\n- E2E: add scenario scripts under `tests/e2e/<area>/` with structured logs.\n","status":"in_progress","priority":1,"issue_type":"task","created_at":"2026-01-27T16:27:56.290055438Z","created_by":"ubuntu","updated_at":"2026-01-28T18:26:08.531060813Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3nb.1","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T18:22:34.515879313Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.1","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T18:22:35.141556982Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.1","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:26.534209015Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.1","depends_on_id":"bd-3nb","type":"parent-child","created_at":"2026-01-27T16:27:56.290055438Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3nb.2","title":"Implement SSH runner infrastructure","description":"## Overview\nBuild the SSH execution layer that runs commands on remote machines securely and efficiently.\n\n## Background\nTo collect data from remote machines, we need a robust SSH execution layer. This handles connection pooling, command execution, file transfers, and error handling.\n\n## Architecture\n```\n┌─────────────────┐\n│   SshRunner     │\n├─────────────────┤\n│ - pool: HashMap │  Connection pool per machine\n│ - config: Cfg   │\n└────────┬────────┘\n         │\n    ┌────▼────┐\n    │ Session │  Per-connection state\n    ├─────────┤\n    │ exec()  │  Run commands\n    │ scp()   │  Transfer files\n    │ sftp()  │  File operations\n    └─────────┘\n```\n\n## Implementation (vc_collect/src/ssh.rs)\n```rust\nuse russh::{client, ChannelMsg};\nuse russh_keys::key;\n\npub struct SshRunner {\n    connections: DashMap<String, Arc<SshSession>>,\n    config: SshConfig,\n}\n\npub struct SshConfig {\n    pub connect_timeout: Duration,\n    pub command_timeout: Duration,\n    pub max_retries: u32,\n    pub keepalive_interval: Duration,\n}\n\nimpl SshRunner {\n    pub async fn new(config: SshConfig) -> Result<Self>;\n    \n    /// Execute a command on a remote machine\n    pub async fn exec(\n        &self,\n        machine: &Machine,\n        cmd: &str,\n    ) -> Result<CommandOutput> {\n        let session = self.get_or_connect(machine).await?;\n        let channel = session.channel_open_session().await?;\n        channel.exec(true, cmd).await?;\n        \n        let mut stdout = Vec::new();\n        let mut stderr = Vec::new();\n        let mut exit_code = None;\n        \n        loop {\n            match channel.wait().await {\n                Some(ChannelMsg::Data { data }) => stdout.extend_from_slice(&data),\n                Some(ChannelMsg::ExtendedData { data, .. }) => stderr.extend_from_slice(&data),\n                Some(ChannelMsg::ExitStatus { exit_status }) => exit_code = Some(exit_status),\n                Some(ChannelMsg::Eof) | None => break,\n                _ => {}\n            }\n        }\n        \n        Ok(CommandOutput {\n            stdout: String::from_utf8_lossy(&stdout).to_string(),\n            stderr: String::from_utf8_lossy(&stderr).to_string(),\n            exit_code: exit_code.unwrap_or(0),\n        })\n    }\n    \n    /// Transfer file from remote to local\n    pub async fn fetch_file(\n        &self,\n        machine: &Machine,\n        remote_path: &str,\n        local_path: &Path,\n    ) -> Result<()>;\n    \n    /// Check if connection is alive\n    pub async fn ping(&self, machine: &Machine) -> Result<bool>;\n}\n\npub struct CommandOutput {\n    pub stdout: String,\n    pub stderr: String,\n    pub exit_code: u32,\n}\n```\n\n## Connection Management\n```rust\nimpl SshRunner {\n    async fn get_or_connect(&self, machine: &Machine) -> Result<Arc<SshSession>> {\n        // Check pool first\n        if let Some(session) = self.connections.get(&machine.machine_id) {\n            if session.is_alive().await {\n                return Ok(session.clone());\n            }\n        }\n        \n        // Create new connection\n        let ssh_config = machine.ssh_config.as_ref()\n            .ok_or_else(|| Error::NoSshConfig(machine.machine_id.clone()))?;\n            \n        let key = russh_keys::load_secret_key(&ssh_config.key_path, None)?;\n        let config = client::Config::default();\n        \n        let session = client::connect(\n            Arc::new(config),\n            (ssh_config.host.as_str(), ssh_config.port),\n            SshHandler,\n        ).await?;\n        \n        session.authenticate_publickey(\n            &ssh_config.user,\n            Arc::new(key),\n        ).await?;\n        \n        let session = Arc::new(session);\n        self.connections.insert(machine.machine_id.clone(), session.clone());\n        Ok(session)\n    }\n}\n```\n\n## Error Handling\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum SshError {\n    #[error(\"Connection failed: {0}\")]\n    ConnectionFailed(String),\n    \n    #[error(\"Authentication failed for {user}@{host}\")]\n    AuthFailed { user: String, host: String },\n    \n    #[error(\"Command timed out after {0:?}\")]\n    Timeout(Duration),\n    \n    #[error(\"Command failed with exit code {code}: {stderr}\")]\n    CommandFailed { code: u32, stderr: String },\n    \n    #[error(\"No SSH configuration for machine {0}\")]\n    NoSshConfig(String),\n}\n```\n\n## Dependencies\n- russh (async SSH2 implementation)\n- russh-keys (key loading)\n- dashmap (concurrent connection pool)\n\n## Acceptance Criteria\n- [ ] SshRunner connects to remote machines via SSH\n- [ ] Connection pooling avoids repeated handshakes\n- [ ] Commands execute with proper timeout handling\n- [ ] File transfer works for log/data retrieval\n- [ ] Graceful handling of connection drops\n- [ ] Health checks via ping command\n\n## Testing & Logging\n- Unit tests for validation, defaults, or timeout behavior.\n- Integration tests with temporary DuckDB/fixtures.\n- Failure-mode tests (invalid config, corrupt db, partial output).\n- E2E: add scenario scripts under `tests/e2e/<area>/` with structured logs.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:28:17.722593132Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:27.416869010Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3nb.2","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:27.416715160Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.2","depends_on_id":"bd-3nb","type":"parent-child","created_at":"2026-01-27T16:28:17.722593132Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.2","depends_on_id":"bd-3nb.1","type":"blocks","created_at":"2026-01-27T16:29:37.330545237Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3nb.3","title":"Implement tool probing system","description":"## Overview\nBuild the tool probing system that discovers which tools are installed on each machine and their versions.\n\n## Background\nBefore collecting data from a remote machine, we need to know which collectors can run there. Tool probing discovers installed tools (caut, ntm, rch, etc.) and their locations/versions.\n\n## Tool Detection Strategy\n```rust\n// Each tool has a detection spec\npub struct ToolSpec {\n    pub name: &str,\n    pub detect_commands: Vec<&str>,  // Try in order\n    pub version_flag: &str,          // --version, -V, etc.\n    pub version_regex: &str,         // Extract version from output\n}\n\npub const TOOL_SPECS: &[ToolSpec] = &[\n    ToolSpec {\n        name: \"caut\",\n        detect_commands: &[\"which caut\", \"~/.cargo/bin/caut\"],\n        version_flag: \"--version\",\n        version_regex: r\"caut (\\d+\\.\\d+\\.\\d+)\",\n    },\n    ToolSpec {\n        name: \"ntm\",\n        detect_commands: &[\"which ntm\", \"~/.local/bin/ntm\"],\n        version_flag: \"--version\",\n        version_regex: r\"ntm (\\d+\\.\\d+\\.\\d+)\",\n    },\n    ToolSpec {\n        name: \"rch\",\n        detect_commands: &[\"which rch\", \"~/.cargo/bin/rch\"],\n        version_flag: \"--version\",\n        version_regex: r\"rch (\\d+\\.\\d+\\.\\d+)\",\n    },\n    // ... more tools\n];\n```\n\n## Implementation (vc_collect/src/probe.rs)\n```rust\npub struct ToolProber {\n    ssh: Arc<SshRunner>,\n    store: Arc<VcStore>,\n}\n\nimpl ToolProber {\n    /// Probe a single machine for all known tools\n    pub async fn probe_machine(&self, machine: &Machine) -> Result<ProbeResult> {\n        let mut found_tools = Vec::new();\n        let mut errors = Vec::new();\n        \n        for spec in TOOL_SPECS {\n            match self.probe_tool(machine, spec).await {\n                Ok(Some(info)) => found_tools.push(info),\n                Ok(None) => {},  // Tool not found, thats OK\n                Err(e) => errors.push((spec.name, e)),\n            }\n        }\n        \n        // Update database\n        for tool in &found_tools {\n            self.store.record_tool(&machine.machine_id, tool).await?;\n        }\n        \n        self.store.update_machine_probe_time(&machine.machine_id).await?;\n        \n        Ok(ProbeResult { found_tools, errors })\n    }\n    \n    async fn probe_tool(&self, machine: &Machine, spec: &ToolSpec) -> Result<Option<ToolInfo>> {\n        // Try each detection command\n        for cmd in &spec.detect_commands {\n            let result = self.ssh.exec(machine, cmd).await;\n            if let Ok(output) = result {\n                if output.exit_code == 0 {\n                    let path = output.stdout.trim().to_string();\n                    \n                    // Get version\n                    let version_cmd = format\\!(\"{} {}\", path, spec.version_flag);\n                    let version = match self.ssh.exec(machine, &version_cmd).await {\n                        Ok(out) => {\n                            let re = Regex::new(spec.version_regex)?;\n                            re.captures(&out.stdout)\n                                .and_then(|c| c.get(1))\n                                .map(|m| m.as_str().to_string())\n                        }\n                        Err(_) => None,\n                    };\n                    \n                    return Ok(Some(ToolInfo {\n                        name: spec.name.to_string(),\n                        path,\n                        version,\n                        is_available: true,\n                    }));\n                }\n            }\n        }\n        Ok(None)\n    }\n    \n    /// Probe all registered machines\n    pub async fn probe_all(&self) -> Result<Vec<(String, ProbeResult)>> {\n        let machines = self.store.list_machines(None).await?;\n        \n        let results: Vec<_> = futures::stream::iter(machines)\n            .map(|m| async move {\n                let result = self.probe_machine(&m).await;\n                (m.machine_id, result)\n            })\n            .buffer_unordered(4)  // Probe 4 machines in parallel\n            .collect()\n            .await;\n            \n        Ok(results)\n    }\n}\n\npub struct ProbeResult {\n    pub found_tools: Vec<ToolInfo>,\n    pub errors: Vec<(String, Error)>,\n}\n```\n\n## CLI Commands\n```bash\n# Probe single machine\nvc machines probe mac-mini-1\n\n# Probe all machines\nvc machines probe --all\n\n# Show tool inventory\nvc machines tools\nvc machines tools --tool rch  # Which machines have rch?\n\n# Robot mode\nvc robot tools --format json\n```\n\n## Scheduled Probing\n```rust\n// In daemon mode, probe periodically\nimpl VcDaemon {\n    async fn run_probe_schedule(&self) {\n        let mut interval = tokio::time::interval(Duration::from_secs(3600)); // Hourly\n        loop {\n            interval.tick().await;\n            if let Err(e) = self.prober.probe_all().await {\n                tracing::error\\!(\"Probe failed: {}\", e);\n            }\n        }\n    }\n}\n```\n\n## Acceptance Criteria\n- [ ] ToolSpec definitions for all 15 source tools\n- [ ] probe_machine detects tools and versions\n- [ ] Results stored in machine_tools table\n- [ ] CLI commands for manual probing\n- [ ] Parallel probing of multiple machines\n- [ ] Probe timestamp tracked for staleness detection\n\n## Testing & Logging\n- Unit tests for validation, defaults, or timeout behavior.\n- Integration tests with temporary DuckDB/fixtures.\n- Failure-mode tests (invalid config, corrupt db, partial output).\n- E2E: add scenario scripts under `tests/e2e/<area>/` with structured logs.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:28:39.328817277Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:28.533592929Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3nb.3","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:28.533019648Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.3","depends_on_id":"bd-3nb","type":"parent-child","created_at":"2026-01-27T16:28:39.328817277Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.3","depends_on_id":"bd-3nb.2","type":"blocks","created_at":"2026-01-27T16:29:37.472931736Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3nb.4","title":"Implement remote collector execution","description":"## Overview\nBuild the remote collector execution system that runs collectors on remote machines via SSH.\n\n## Background\nMany collectors (caut, ntm, rch, etc.) need to run on the machine where the data lives. This task implements the RemoteCollector wrapper that executes collectors over SSH and retrieves results.\n\n## Architecture\n```\n┌─────────────────────────────────────────────────────┐\n│                 CollectorScheduler                   │\n├─────────────────────────────────────────────────────┤\n│  For each (collector, machine) pair:                │\n│    1. Check if tool available on machine            │\n│    2. Get cursor from local store                   │\n│    3. Execute collector remotely                    │\n│    4. Parse and store results locally               │\n└─────────────────────────────────────────────────────┘\n          │\n          ▼\n┌─────────────────┐     ┌─────────────────┐\n│ RemoteCollector │────▶│    SshRunner    │\n├─────────────────┤     └─────────────────┘\n│ exec_collect()  │\n│ parse_output()  │\n└─────────────────┘\n```\n\n## Implementation (vc_collect/src/remote.rs)\n```rust\npub struct RemoteCollector<C: Collector> {\n    inner: C,\n    ssh: Arc<SshRunner>,\n    machine: Machine,\n}\n\nimpl<C: Collector> RemoteCollector<C> {\n    pub fn new(inner: C, ssh: Arc<SshRunner>, machine: Machine) -> Self {\n        Self { inner, ssh, machine }\n    }\n    \n    /// Execute collection on remote machine\n    pub async fn collect_remote(&self, ctx: &CollectContext) -> Result<CollectResult> {\n        // Build the remote command\n        let tool_info = self.machine.tools.iter()\n            .find(|t| t.name == C::TOOL_NAME)\n            .ok_or_else(|| Error::ToolNotFound(C::TOOL_NAME.to_string()))?;\n        \n        let cursor_arg = ctx.cursor.as_ref()\n            .map(|c| format\\!(\"--since {}\", c.to_string()))\n            .unwrap_or_default();\n        \n        let cmd = format\\!(\n            \"{} {} --robot --json {}\",\n            tool_info.path,\n            C::SUBCOMMAND,\n            cursor_arg\n        );\n        \n        // Execute remotely\n        let output = self.ssh.exec(&self.machine, &cmd).await?;\n        \n        if output.exit_code \\!= 0 {\n            return Err(Error::RemoteCommandFailed {\n                machine: self.machine.machine_id.clone(),\n                cmd,\n                stderr: output.stderr,\n            });\n        }\n        \n        // Parse the JSON output\n        let result: CollectResult = serde_json::from_str(&output.stdout)?;\n        Ok(result)\n    }\n}\n```\n\n## Multi-Machine Collection\n```rust\npub struct MultiMachineCollector {\n    registry: Arc<MachineRegistry>,\n    ssh: Arc<SshRunner>,\n    store: Arc<VcStore>,\n}\n\nimpl MultiMachineCollector {\n    /// Collect from all machines that have the tool\n    pub async fn collect_all<C: Collector>(\n        &self,\n        collector: C,\n    ) -> Result<Vec<(String, CollectResult)>> {\n        // Get machines with this tool\n        let machines = self.registry\n            .list_machines_with_tool(C::TOOL_NAME)\n            .await?;\n        \n        // Collect from each in parallel\n        let results: Vec<_> = futures::stream::iter(machines)\n            .map(|machine| {\n                let ssh = self.ssh.clone();\n                let ctx = CollectContext {\n                    cursor: self.store.get_cursor(C::TOOL_NAME, &machine.machine_id),\n                    machine_id: Some(machine.machine_id.clone()),\n                };\n                async move {\n                    let remote = RemoteCollector::new(collector.clone(), ssh, machine.clone());\n                    let result = remote.collect_remote(&ctx).await;\n                    (machine.machine_id, result)\n                }\n            })\n            .buffer_unordered(4)\n            .collect()\n            .await;\n        \n        // Store results with machine_id tags\n        for (machine_id, result) in &results {\n            if let Ok(r) = result {\n                self.store.ingest_with_machine(r, machine_id).await?;\n            }\n        }\n        \n        Ok(results)\n    }\n}\n```\n\n## Data Tagging\nAll data collected remotely is tagged with machine_id:\n```sql\n-- Example: rch_compilations table\nCREATE TABLE rch_compilations (\n    id INTEGER PRIMARY KEY,\n    machine_id VARCHAR NOT NULL,  -- Which machine this came from\n    crate_name VARCHAR,\n    duration_ms INTEGER,\n    cache_hit BOOLEAN,\n    collected_at TIMESTAMP\n);\n```\n\n## Error Handling\n```rust\n#[derive(Debug, thiserror::Error)]\npub enum RemoteCollectError {\n    #[error(\"Tool {0} not found on machine\")]\n    ToolNotFound(String),\n    \n    #[error(\"Remote command failed on {machine}: {stderr}\")]\n    RemoteCommandFailed {\n        machine: String,\n        cmd: String,\n        stderr: String,\n    },\n    \n    #[error(\"Failed to parse remote output: {0}\")]\n    ParseError(#[from] serde_json::Error),\n    \n    #[error(\"Machine {0} is offline\")]\n    MachineOffline(String),\n}\n```\n\n## Acceptance Criteria\n- [ ] RemoteCollector executes tool commands over SSH\n- [ ] Cursor state maintained per (collector, machine) pair\n- [ ] Multi-machine parallel collection works\n- [ ] Results tagged with machine_id in DuckDB\n- [ ] Graceful handling of offline machines\n- [ ] Timeout handling for slow remotes\n\n## Testing & Logging\n- Unit tests for validation, defaults, or timeout behavior.\n- Integration tests with temporary DuckDB/fixtures.\n- Failure-mode tests (invalid config, corrupt db, partial output).\n- E2E: add scenario scripts under `tests/e2e/<area>/` with structured logs.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:29:02.919606308Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:29.587382343Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3nb.4","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T18:22:35.754402675Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.4","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:29.586691492Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.4","depends_on_id":"bd-3nb","type":"parent-child","created_at":"2026-01-27T16:29:02.919606308Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.4","depends_on_id":"bd-3nb.2","type":"blocks","created_at":"2026-01-27T16:29:37.614286152Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.4","depends_on_id":"bd-3nb.3","type":"blocks","created_at":"2026-01-27T16:29:37.762524362Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3nb.5","title":"Implement TUI machine drill-down views","description":"## Overview\nAdd TUI screens for machine inventory, individual machine details, and remote data visualization.\n\n## Background\nWith machine registry, SSH runner, and remote collectors in place, users need visual interfaces to monitor their fleet of machines and drill down into individual machine health and data.\n\n## Screens to Implement\n\n### 1. Machines Overview Screen\n```rust\npub struct MachinesScreen {\n    machines: Vec<MachineRow>,\n    selected: usize,\n    sort_col: MachineSort,\n}\n\npub struct MachineRow {\n    machine_id: String,\n    hostname: String,\n    status: MachineStatus,      // Online, Offline, Unknown\n    tool_count: usize,          // Number of available tools\n    last_seen: DateTime<Utc>,\n    last_probe: DateTime<Utc>,\n    tags: Vec<String>,\n}\n\n// Visual layout:\n// ╭─ Machines ─────────────────────────────────────────╮\n// │ ID          Hostname        Status  Tools  Last   │\n// │ ─────────── ─────────────── ─────── ────── ────── │\n// │ local       devbox          ● 🟢     12    now    │\n// │ mac-mini-1  192.168.1.100   ● 🟢     10    2m ago │\n// │ mac-mini-2  192.168.1.101   ● 🔴     --    1h ago │\n// │ hetzner-1   hetzner1.ex...  ● 🟢      8    5m ago │\n// ╰────────────────────────────────────────────────────╯\n```\n\n### 2. Machine Detail Screen\n```rust\npub struct MachineDetailScreen {\n    machine: Machine,\n    tools: Vec<ToolInfo>,\n    recent_collections: Vec<CollectionEvent>,\n    system_stats: Option<SystemStats>,  // If sysmoni available\n}\n\n// Visual layout:\n// ╭─ mac-mini-1 ───────────────────────────────────────╮\n// │ Hostname: 192.168.1.100    Status: 🟢 Online       │\n// │ SSH: ubuntu@192.168.1.100  Tags: [mini, builder]   │\n// ├────────────────────────────────────────────────────┤\n// │ Available Tools:                                   │\n// │   caut v0.3.2    ntm v0.5.1     rch v0.2.0        │\n// │   ru v0.4.0      sysmoni v0.1.0                   │\n// ├────────────────────────────────────────────────────┤\n// │ System Stats (via sysmoni):                        │\n// │   CPU: ████████░░ 78%    Memory: ██████░░░░ 62%   │\n// │   Load: 2.4 / 4.0        Disk: ███░░░░░░░ 34%     │\n// ├────────────────────────────────────────────────────┤\n// │ Recent Collections:                                │\n// │   caut    2m ago   324 records   0.3s             │\n// │   ntm     5m ago   12 records    0.1s             │\n// │   rch     5m ago   156 records   0.5s             │\n// ╰────────────────────────────────────────────────────╯\n```\n\n### 3. Cross-Machine Comparison View\n```rust\npub struct MachineCompareScreen {\n    machines: Vec<String>,      // Selected machines to compare\n    metric: CompareMetric,      // CPU, Memory, Build Times, etc.\n    time_range: TimeRange,\n}\n\n// Visual: Side-by-side sparklines or bar charts\n// comparing same metric across selected machines\n```\n\n## Key Interactions\n- `m` to open machines screen\n- `Enter` on a machine to drill into details\n- `p` to trigger probe on selected machine\n- `c` to enter compare mode (multi-select)\n- `r` to refresh / re-collect data\n- `t` to filter by tags\n\n## Data Queries\n```sql\n-- Machine overview\nSELECT \n    m.machine_id, m.hostname, m.status,\n    COUNT(mt.tool_name) as tool_count,\n    m.last_seen_at, m.last_probe_at,\n    m.tags\nFROM machines m\nLEFT JOIN machine_tools mt ON m.machine_id = mt.machine_id AND mt.is_available\nGROUP BY m.machine_id;\n\n-- Machine recent collections\nSELECT \n    tool_name,\n    collected_at,\n    record_count,\n    duration_ms\nFROM collection_log\nWHERE machine_id = ?\nORDER BY collected_at DESC\nLIMIT 10;\n\n-- Cross-machine comparison (e.g., rch build times)\nSELECT \n    machine_id,\n    AVG(duration_ms) as avg_build_time,\n    COUNT(*) as build_count\nFROM rch_compilations\nWHERE collected_at > now() - INTERVAL 1 DAY\nGROUP BY machine_id;\n```\n\n## Acceptance Criteria\n- [ ] Machines overview screen shows all registered machines\n- [ ] Status icons indicate online/offline state\n- [ ] Machine detail shows tools, stats, recent collections\n- [ ] Drill-down navigation from overview to detail\n- [ ] Cross-machine comparison for key metrics\n- [ ] Tag filtering works on machines list\n- [ ] Probe action triggerable from TUI\n\n## Testing & Logging\n- View-model unit tests for filtering, sorting, and state transitions.\n- Render snapshot tests using ratatui TestBackend for key layouts.\n- Navigation/keybinding tests (tab/enter/filter/search).\n- E2E: tests/e2e/tui/test_<screen>.sh with expect/pexpect; capture logs and screenshots on failure.\n","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T16:29:28.291341990Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:30.489831837Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3nb.5","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:30.489017553Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.5","depends_on_id":"bd-33h.3","type":"blocks","created_at":"2026-01-27T16:29:38.190321180Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.5","depends_on_id":"bd-3nb","type":"parent-child","created_at":"2026-01-27T16:29:28.291341990Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.5","depends_on_id":"bd-3nb.1","type":"blocks","created_at":"2026-01-27T16:29:37.909223674Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3nb.5","depends_on_id":"bd-3nb.4","type":"blocks","created_at":"2026-01-27T16:29:38.052623943Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-69d","title":"Implement afsc (Automated Flywheel Setup Checker) collector","description":"Implement collector for afsc (Automated Flywheel Setup Checker) - flywheel setup health monitoring.\n\nINTEGRATION METHOD:\n- automated_flywheel_setup_checker status --format json\n- automated_flywheel_setup_checker list --format jsonl\n- automated_flywheel_setup_checker validate --format jsonl\n- automated_flywheel_setup_checker classify-error --format jsonl\n- Use CLI Incremental Window pattern (time-bounded)\n\nDUCKDB TABLES:\nCREATE TABLE afsc_status_snapshot(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    overall_health TEXT,\n    installers_total INTEGER,\n    installers_healthy INTEGER,\n    last_run_at TIMESTAMP,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, collected_at)\n);\n\nCREATE TABLE afsc_run_facts(\n    machine_id TEXT,\n    run_id TEXT,\n    ts TIMESTAMP,\n    status TEXT,\n    duration_ms BIGINT,\n    error_category TEXT,\n    installer_name TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, run_id)\n);\n\nCREATE TABLE afsc_event_logs(\n    machine_id TEXT,\n    ts TIMESTAMP,\n    event_type TEXT,\n    severity TEXT,\n    message TEXT,\n    installer_name TEXT,\n    raw_json TEXT\n);\n\nCREATE TABLE afsc_error_clusters(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    error_category TEXT,\n    occurrence_count INTEGER,\n    last_seen TIMESTAMP,\n    example_errors_json TEXT,\n    PRIMARY KEY (machine_id, collected_at, error_category)\n);\n\nCOLLECTOR IMPLEMENTATION:\n- Implement AfscCollector with Collector trait\n- Parse status output for overall health summary\n- Track run history with success/failure status\n- Cluster errors by category for pattern detection\n- Handle missing afsc binary gracefully\n\nUI VALUE:\n- Flywheel setup health dashboard\n- Installer success rates\n- Error pattern analysis\n- Setup drift detection\n\nTESTS:\n- Mock afsc CLI outputs\n- Verify error clustering\n- Test incremental collection\n\n## E2E & Logging\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:48:24.200927609Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:32.091330599Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-69d","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T17:00:12.142085266Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-69d","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:53:50.922421453Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-69d","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:53:49.499126323Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-69d","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:32.091125292Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-69d","depends_on_id":"bd-3nb","type":"parent-child","created_at":"2026-01-27T16:55:45.826209129Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-69d","depends_on_id":"bd-3nb.4","type":"blocks","created_at":"2026-01-27T16:53:52.254011267Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-7b7","title":"Implement cloud_benchmarker collector","description":"Implement collector for cloud_benchmarker - VPS machine performance baseline and drift detection.\n\nINTEGRATION METHOD:\n- GET /data/raw/ from its FastAPI server\n- GET /data/overall/ for aggregate scores\n- Or read SQLite directly if available\n- Use CLI Snapshot or HTTP scrape pattern\n\nDUCKDB TABLES:\nCREATE TABLE cloud_bench_raw(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    benchmark_type TEXT,\n    benchmark_name TEXT,\n    value REAL,\n    unit TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, collected_at, benchmark_type, benchmark_name)\n);\n\nCREATE TABLE cloud_bench_overall(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    overall_score REAL,\n    cpu_score REAL,\n    memory_score REAL,\n    disk_score REAL,\n    network_score REAL,\n    subscores_json TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, collected_at)\n);\n\nCREATE TABLE cloud_bench_history(\n    machine_id TEXT,\n    benchmark_date DATE,\n    overall_score REAL,\n    delta_from_baseline REAL,\n    anomaly_detected BOOLEAN,\n    PRIMARY KEY (machine_id, benchmark_date)\n);\n\nCOLLECTOR IMPLEMENTATION:\n- Implement CloudBenchCollector with Collector trait\n- Try HTTP endpoint first (if benchmarker server running)\n- Fall back to SQLite direct read\n- Calculate drift from baseline (first benchmark = baseline)\n- Flag significant performance degradation (>10% from baseline)\n- Handle missing benchmarker gracefully\n\nUI VALUE:\n- Machine performance baselines\n- Performance drift alerts\n- VPS provider comparison\n- Capacity planning data\n\nDRIFT DETECTION:\n- Store first benchmark as baseline per machine\n- Calculate delta on each collection\n- Alert if delta > threshold (configurable, default 10%)\n- Track trends over time\n\nTESTS:\n- Mock FastAPI responses\n- Test SQLite fallback\n- Verify drift calculation\n\n## E2E & Logging\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T16:48:35.215825477Z","created_by":"ubuntu","updated_at":"2026-01-27T20:37:33.131841013Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-7b7","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T17:00:13.509871530Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7b7","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:53:55.098501743Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7b7","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:53:53.709299871Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7b7","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:33.131556086Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7b7","depends_on_id":"bd-3nb","type":"parent-child","created_at":"2026-01-27T16:55:47.510615529Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7b7","depends_on_id":"bd-3nb.4","type":"blocks","created_at":"2026-01-27T16:53:56.547129510Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new","title":"Phase 7: Scale + Governance + Integrations","description":"## Background\\nThese additions extend vc beyond the current Phase 0-6 roadmap with scale, governance, and ecosystem integrations that are explicitly called out in the long-form plans but not yet represented in beads. They focus on data trustworthiness, incident replay, push-mode collection, external observability, GitHub correlation, and safer operator UX.\\n\\n## Goals\\n- Add scale primitives (push-mode vc-node, offline sync) without disrupting pull-first MVP.\\n- Improve data quality (freshness, drift detection, adaptive polling) so health/alerts are trustworthy.\\n- Provide incident replay/time-travel to make incident management actionable.\\n- Expand ecosystem integrations (Prometheus export, GitHub issues/PRs).\\n- Add governance/contract tooling (schema docs, config lint, query guardrails, redaction).\\n\\n## Non-Goals\\n- No replacement of upstream tools; vc remains an integration layer.\\n- No breaking changes to existing schemas; use additive migrations only.\\n- No mandatory SaaS dependencies; keep local-first.\\n\\n## Considerations\\n- Must integrate with existing vc_store/collector/audit/alerts work.\\n- All features require explicit unit tests + E2E scripts with structured logs per bd-30z.\\n- Avoid cycles in deps; validate with bv.\\n","status":"in_progress","priority":2,"issue_type":"epic","created_at":"2026-01-27T21:35:48.421154816Z","created_by":"ubuntu","updated_at":"2026-01-28T05:08:13.285674068Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-new.1","title":"Implement collector health + freshness + drift detection","description":"## Background\nvc health and alerts are only trustworthy if we can quantify data freshness and detect drift from expected baselines. The long-form plans emphasize “stale data” badges and incident replay; this bead establishes the data-quality layer that all higher-level reasoning depends on.\n\n## Goals\n- Track per-collector freshness and success rate per machine.\n- Detect baseline drift for machine profiles (CPU/mem/disk/net) and collector payloads.\n- Surface data-quality signals to health score, alerts, and UI/robot outputs.\n- Record provenance/lineage metadata to make explanations auditable.\n\n## Implementation\n### Tables\n```sql\nCREATE TABLE collector_health(\n    machine_id TEXT,\n    collector TEXT,\n    collected_at TIMESTAMP,\n    success BOOLEAN,\n    duration_ms BIGINT,\n    rows_inserted BIGINT,\n    bytes_parsed BIGINT,\n    error_class TEXT,\n    freshness_seconds BIGINT,\n    payload_hash TEXT,\n    collector_version TEXT,\n    schema_version TEXT,\n    cursor_json TEXT,\n    PRIMARY KEY (machine_id, collector, collected_at)\n);\n\nCREATE TABLE machine_baselines(\n    machine_id TEXT,\n    baseline_window TEXT,       -- e.g. \"7d\" or \"30d\"\n    computed_at TIMESTAMP,\n    metrics_json TEXT,          -- mean/std/percentiles per metric\n    PRIMARY KEY (machine_id, baseline_window)\n);\n\nCREATE TABLE drift_events(\n    machine_id TEXT,\n    detected_at TIMESTAMP,\n    metric TEXT,\n    current_value REAL,\n    baseline_mean REAL,\n    baseline_std REAL,\n    z_score REAL,\n    severity TEXT,\n    evidence_json TEXT\n);\n```\n\n### Logic\n- Update `collector_health` from audit_events + collector runtime metadata.\n- Freshness = now - last_successful_collect for that collector/machine.\n- Baseline drift detection: rolling z-score or percentile divergence for core sys metrics.\n- Emit `drift_events` when z-score exceeds configurable threshold.\n- Add vc.toml config for freshness thresholds and drift sensitivity.\n\n### Outputs\n- Add staleness + drift summaries into health score inputs.\n- Surface to TUI overview + robot health output as warnings.\n\n## Risks\n- False positives on drift during planned load spikes.\n- Overhead from baseline computation if window too large.\n\n## Success Criteria\n- Staleness is visible in robot output and UI for each collector.\n- Drift events are recorded and appear in incident evidence.\n- Health score incorporates data freshness weighting.\n- Lineage metadata allows tracing any alert back to inputs.\n\n## Tests & E2E\n- Unit tests for freshness calculation and baseline stats (fixed seeds).\n- Golden-data tests for drift detection on known time series.\n- E2E: `tests/e2e/system/test_data_quality.sh` with JSON summary and log artifacts.\n- Structured logs: collector, machine_id, freshness_seconds, drift_count.","status":"open","priority":1,"issue_type":"task","created_at":"2026-01-27T21:37:24.472207696Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:12.107058243Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.1","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T21:37:24.472207696Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.1","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T22:16:11.254659491Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.1","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T21:37:24.472207696Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.1","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T21:37:24.472207696Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.1","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:12.106694788Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.1","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:37:24.472207696Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.10","title":"Implement config lint + wizard","description":"## Background\nvc.toml will grow in complexity. Operators need validation, linting, and a guided wizard to prevent misconfiguration.\n\n## Goals\n- `vc config lint` that validates machines, SSH targets, and collector settings.\n- `vc config wizard` to generate minimal config interactively or from templates.\n- Emit actionable error messages with fixes.\n- Provide non‑destructive diff preview when generating changes.\n\n## Implementation\n- Add JSON schema for config + custom validators (SSH, paths, tokens).\n- Support `--fix` suggestions (non‑destructive).\n- Validate machine tags and tool enablement coherence.\n- Wizard writes to new file unless `--overwrite` is explicitly set.\n\n## Risks\n- Wizard must not overwrite existing config without explicit user action.\n\n## Success Criteria\n- Lint catches invalid configs and suggests fixes.\n- Wizard can bootstrap a valid config from scratch.\n- Diff preview shows exact changes before write.\n\n## Tests & E2E\n- Unit tests for validators + schema rules.\n- E2E: `tests/e2e/config/test_config_lint.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:38:54.133716478Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:25.574555428Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.10","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T21:38:54.133716478Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.10","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T21:38:54.133716478Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.10","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:25.573935691Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.10","depends_on_id":"bd-3nb.1","type":"blocks","created_at":"2026-01-27T21:38:54.133716478Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.10","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:38:54.133716478Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.11","title":"Implement query guardrails + safe templates","description":"## Background\n`vc query` is powerful but dangerous if used freely by agents. Guardrails and safe templates prevent expensive or unsafe queries.\n\n## Goals\n- Add a read‑only query sandbox with allowlisted templates.\n- Enforce query timeouts + row limits.\n- Provide explainability (why query allowed/denied).\n- Ensure queries execute on a read‑only connection.\n\n## Implementation\n- `vc query --template <name> --params ...` for safe presets.\n- Parse and reject non‑SELECT queries.\n- Add max runtime and max output rows/bytes.\n- Log query hashes + duration to audit_events.\n\n## Risks\n- Over‑restrictive guardrails could block legitimate use cases.\n\n## Success Criteria\n- Unsafe queries are rejected with clear reasons.\n- Templates cover 80% of typical use.\n- Read‑only enforcement is validated.\n\n## Tests & E2E\n- Unit tests for query validator + template expansion.\n- E2E: `tests/e2e/system/test_query_guardrails.sh` with JSON summary + logs.","status":"in_progress","priority":2,"issue_type":"task","created_at":"2026-01-27T21:39:01.267430595Z","created_by":"ubuntu","updated_at":"2026-01-28T18:18:48.725513126Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.11","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T21:39:01.267430595Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.11","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T21:39:01.267430595Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.11","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:26.666677546Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.11","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:39:01.267430595Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.12","title":"Implement secrets/PII redaction pipeline","description":"## Background\nCollectors ingest logs and artifacts that may contain secrets or PII. We need systematic redaction before storage and export.\n\n## Goals\n- Redact known secret patterns (tokens, keys) from raw_json/artifacts.\n- Support per‑collector redaction rules.\n- Preserve auditability while minimizing sensitive leakage.\n- Provide rule versioning + test fixtures to avoid regressions.\n\n## Implementation\n- Redaction pipeline stage before insert into DuckDB.\n- Configurable regex rules + allowlist for safe fields.\n- Store redaction stats per batch.\n\n### Tables\n```sql\nCREATE TABLE redaction_events(\n    collected_at TIMESTAMP,\n    machine_id TEXT,\n    collector TEXT,\n    redacted_fields INTEGER,\n    redacted_bytes INTEGER,\n    sample_hash TEXT,\n    rules_version TEXT,\n    raw_json TEXT\n);\n```\n\n## Risks\n- Over‑redaction can reduce debugging value.\n- Under‑redaction is a security risk.\n\n## Success Criteria\n- Secrets do not appear in stored artifacts for test fixtures.\n- Redaction stats available for audits.\n- Rule version changes are tracked.\n\n## Tests & E2E\n- Unit tests for redaction regex rules.\n- Fixture tests with seeded secrets.\n- E2E: `tests/e2e/security/test_redaction.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:39:11.131141382Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:28.512328612Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.12","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T21:39:11.131141382Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.12","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T22:16:27.575103661Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.12","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T21:39:11.131141382Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.12","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T21:39:11.131141382Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.12","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:28.511713503Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.12","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:39:11.131141382Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.13","title":"Implement data export/backup + restore","description":"## Background\nOperators need backups and portable exports of vc data for audits, sharing, and disaster recovery.\n\n## Goals\n- Export DuckDB tables to Parquet/JSONL bundles.\n- Provide restore/import path for snapshots.\n- Support rotating backups with retention policy.\n- Allow time‑window filtering to control export size.\n\n## Implementation\n- `vc db export --format parquet --out <dir> --since <ts> --until <ts>`.\n- `vc db import --from <dir>` for restore.\n- Store manifest with schema versions + checksums.\n\n## Risks\n- Large exports; must allow filtering by time window.\n\n## Success Criteria\n- Export/import round‑trip preserves data integrity.\n- Backups can be scheduled without impacting poll cycle.\n- Manifest validates schema + checksum correctness.\n\n## Tests & E2E\n- Unit tests for export manifest validation.\n- Integration tests for round‑trip import.\n- E2E: `tests/e2e/system/test_export_backup.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:39:18.562333144Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:29.428333815Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.13","depends_on_id":"bd-264","type":"blocks","created_at":"2026-01-27T21:39:18.562333144Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.13","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T21:39:18.562333144Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.13","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T21:39:18.562333144Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.13","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:29.427636812Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.13","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:39:18.562333144Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.14","title":"Implement alert routing + escalation policies","description":"## Background\nAlert delivery channels exist, but we need routing, escalation, and quiet hours to avoid noise and ensure critical incidents reach the right place.\n\n## Goals\n- Route alerts by severity, machine tags, and repo.\n- Escalation rules (e.g., auto‑page after N minutes unacked).\n- Quiet hours / suppression windows.\n- Audit routing decisions for traceability.\n\n## Implementation\n- Configurable routing rules in vc.toml.\n- Dedup + throttle logic per rule.\n- Escalation to MCP Agent Mail, webhook, Slack, or desktop channel.\n- Store decisions in `alert_routing_events` for audit.\n\n### Table\n```sql\nCREATE TABLE alert_routing_events(\n    alert_id TEXT,\n    routed_at TIMESTAMP,\n    rule_id TEXT,\n    channel TEXT,\n    action TEXT,          -- sent, suppressed, escalated\n    reason_json TEXT\n);\n```\n\n## Risks\n- Over‑aggressive suppression could hide critical alerts.\n\n## Success Criteria\n- Critical alerts always reach at least one channel.\n- Quiet hours reduce low‑severity noise.\n- Escalation triggers after configured SLA.\n\n## Tests & E2E\n- Unit tests for routing rule evaluation.\n- E2E: `tests/e2e/alerts/test_alert_routing.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:39:26.664114195Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:31.202819729Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.14","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T21:39:26.664114195Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.14","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T22:16:30.338867811Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.14","depends_on_id":"bd-2br","type":"blocks","created_at":"2026-01-27T21:39:26.664114195Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.14","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T21:39:26.664114195Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.14","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:31.202459521Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.14","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:39:26.664114195Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.15","title":"Implement web/API auth + multi‑token RBAC","description":"## Background\nThe web dashboard is currently local‑only. For remote access we need multi‑token auth with scoped permissions and audit trails.\n\n## Goals\n- Support multiple API tokens with role scopes (read‑only, operator, admin).\n- Enforce token checks on web API + robot endpoints.\n- Log auth events in audit trail.\n- Support token rotation and optional IP allowlist.\n\n## Implementation\n- Config in vc.toml: tokens with scope + expiration.\n- Middleware in vc_web for auth/authorization.\n- Optional IP allowlist.\n- Add `vc token list|add|revoke` CLI helpers.\n\n## Risks\n- Misconfiguration could lock users out; provide fallback local bypass.\n\n## Success Criteria\n- Unauthorized requests receive 401/403.\n- Audit logs record auth failures and successes.\n- Tokens can be rotated without downtime.\n\n## Tests & E2E\n- Unit tests for scope enforcement.\n- E2E: `tests/e2e/web/test_api_auth.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:39:34.053901498Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:32.189408819Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.15","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T21:39:34.053901498Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.15","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T21:39:34.053901498Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.15","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:32.188656071Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.15","depends_on_id":"bd-3e2.2","type":"blocks","created_at":"2026-01-27T21:39:34.053901498Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.15","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:39:34.053901498Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.2","title":"Implement adaptive polling + on-demand profiling","description":"## Background\nStatic poll intervals are either too expensive or too slow. We need adaptive scheduling and on-demand high-frequency profiling to react to incidents without permanent overhead.\n\n## Goals\n- Adaptive poll scheduler that increases frequency for “hot” machines/collectors.\n- On-demand profiling bursts triggered by alerts or manual commands.\n- Ensure bounded concurrency and global rate limits still respected.\n- Quarantine/backoff for failing collectors to avoid thrash.\n\n## Implementation\n### Adaptive Scheduler\n- Use collector_health freshness + alert severity to compute per-machine poll intervals.\n- Policy: min/max interval (e.g., 15s–5m), exponential backoff on repeated failures.\n- Quarantine collectors after N consecutive failures; require manual or timed reset.\n- Store decisions in `poll_schedule_decisions` for auditability.\n- Configurable per-collector overrides in vc.toml (pin interval, disable adaptive).\n\n### On-Demand Profiling\n- New command: `vc profile --machine <id> --duration 120 --interval 2s`.\n- Temporarily switch sysmoni to high-frequency mode and capture snapshots.\n- Write to `sys_profile_samples` table and link to alert/incident IDs.\n\n### Tables\n```sql\nCREATE TABLE poll_schedule_decisions(\n    machine_id TEXT,\n    collector TEXT,\n    decided_at TIMESTAMP,\n    next_interval_seconds INTEGER,\n    reason_json TEXT\n);\n\nCREATE TABLE sys_profile_samples(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    profile_id TEXT,\n    metrics_json TEXT,\n    raw_json TEXT\n);\n```\n\n## Risks\n- Over-aggressive polling can overload remote machines.\n- Profiling bursts must be rate-limited and opt-in.\n\n## Success Criteria\n- Poll intervals adjust based on freshness/alerts without violating max concurrency.\n- Failing collectors back off and recover safely.\n- Profiling bursts capture high-resolution data linked to incidents.\n\n## Tests & E2E\n- Unit tests for interval selection, backoff, and quarantine logic.\n- Integration tests for scheduler respecting global limits.\n- E2E: `tests/e2e/system/test_adaptive_polling.sh` and `tests/e2e/system/test_profile_burst.sh` with JSON summaries and logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:37:36.437869781Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:13.660928008Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.2","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T22:16:12.884931934Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.2","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T21:37:36.437869781Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.2","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T21:37:36.437869781Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.2","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:13.660357995Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.2","depends_on_id":"bd-33h.1","type":"blocks","created_at":"2026-01-27T21:37:36.437869781Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.2","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:37:36.437869781Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.2","depends_on_id":"bd-new.1","type":"blocks","created_at":"2026-01-27T21:37:36.437869781Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.3","title":"Implement incident replay + time‑travel views","description":"## Background\nWe already track incidents, alerts, and audit events, but there is no “time machine” view to replay what changed and why. Incident replay makes post‑mortems and triage dramatically faster.\n\n## Goals\n- Reconstruct fleet state at arbitrary timestamps (time‑travel queries).\n- Provide an incident replay view in TUI + web + robot output.\n- Link evidence to alerts, audit actions, and collector data.\n- Allow export/share of replay snapshots for post‑mortems.\n\n## Implementation\n### Core Query Patterns\n- Snapshot-at-time: latest snapshot <= T for each table.\n- Context window: time range around incident for events (dcg, rano, mail, audit).\n- Cache replay snapshots for fast scroll/scrub in UI.\n\n### UI/CLI\n- TUI: Incident replay screen with timeline scrubber.\n- Web: `/api/incidents/:id/replay?at=...` returning structured JSON.\n- CLI: `vc robot incident <id> --at <ts>`.\n- Export: `vc incident export <id> --format json|md`.\n\n### Tables\n- Reuse `incident_*` tables + `audit_events` and `collector_health`.\n- Add `incident_replay_snapshots(incident_id, ts, snapshot_json)` cache for fast replay.\n\n## Risks\n- Expensive queries across large tables; use caching and bounded windows.\n- Ambiguity in “latest snapshot” if multiple collectors overlap.\n\n## Success Criteria\n- Incident replay reproduces health + alerts for known test incidents.\n- Replay JSON is stable and documented.\n- Exported snapshots can be shared without losing evidence context.\n\n## Tests & E2E\n- Unit tests for time‑travel query correctness.\n- Regression tests using fixture DuckDB snapshots.\n- E2E: `tests/e2e/system/test_incident_replay.sh` with JSON summary and log artifacts.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:37:48.223462254Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:15.173286849Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.3","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T21:37:48.223462254Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.3","depends_on_id":"bd-1h6","type":"blocks","created_at":"2026-01-27T22:16:14.386585958Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.3","depends_on_id":"bd-2v3","type":"blocks","created_at":"2026-01-27T21:37:48.223462254Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.3","depends_on_id":"bd-2xm.7","type":"blocks","created_at":"2026-01-27T21:40:26.440311623Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.3","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:15.172827534Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.3","depends_on_id":"bd-3e2.2","type":"blocks","created_at":"2026-01-27T21:37:48.223462254Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.3","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:37:48.223462254Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.3","depends_on_id":"bd-new.1","type":"blocks","created_at":"2026-01-27T21:40:24.718858096Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.4","title":"Implement daily/weekly digest reports","description":"## Background\nOperators need a compact daily/weekly digest that summarizes fleet health, alerts, usage, and progress without opening the UI. This is a low‑friction “ops heartbeat.”\n\n## Goals\n- Generate Markdown + JSON digests for daily/weekly windows.\n- Optionally deliver via MCP Agent Mail or webhook (reuse alert delivery channels where possible).\n- Include health, alerts, usage, cost, and beads throughput.\n\n## Implementation\n### Digest Content\n- Top alerts + incident counts.\n- Health score trends (overall + top 3 machines).\n- Usage & rate‑limit forecast summary.\n- Beads throughput + top blockers (bv snapshot).\n- Notable events (dcg spikes, rano anomalies, pt findings).\n\n### CLI/Automation\n- `vc report --window 24h --format md|json`.\n- Schedule via cron/systemd or vc daemon tick.\n\n### Tables\n- `digest_reports(report_id, window, generated_at, summary_json, markdown)`.\n\n## Risks\n- Over‑verbose digests; must stay concise.\n- Requires reliable data freshness indicators (bd-new.1).\n\n## Success Criteria\n- Digest is <1 page and highlights actionable items.\n- Reports are reproducible for the same window.\n\n## Tests & E2E\n- Unit tests for aggregation logic.\n- Golden‑file tests for Markdown rendering.\n- E2E: `tests/e2e/system/test_digest_reports.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:37:58.471678201Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:16.880663734Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.4","depends_on_id":"bd-1h6","type":"blocks","created_at":"2026-01-27T21:37:58.471678201Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.4","depends_on_id":"bd-2br","type":"blocks","created_at":"2026-01-27T22:16:16.094989777Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.4","depends_on_id":"bd-2mv.4","type":"blocks","created_at":"2026-01-27T21:37:58.471678201Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.4","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T21:37:58.471678201Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.4","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:16.880196994Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.4","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:37:58.471678201Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.4","depends_on_id":"bd-new.1","type":"blocks","created_at":"2026-01-27T21:40:18.889556257Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.4","depends_on_id":"bd-new.8","type":"blocks","created_at":"2026-01-27T21:40:21.888263503Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.4","depends_on_id":"bd-rf7","type":"blocks","created_at":"2026-01-27T21:37:58.471678201Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.5","title":"Implement vc-node push agent (store‑and‑forward)","description":"## Background\nPull‑only works for small fleets, but as machine count grows it becomes expensive and brittle. A tiny vc‑node enables store‑and‑forward collection when remotes are offline and reduces SSH overhead.\n\n## Goals\n- Build a lightweight vc‑node binary that runs locally on remote machines.\n- Collect locally, buffer artifacts, and push snapshots to vc‑hub on interval.\n- Support offline backfill (store‑and‑forward) with deduplication.\n\n## Implementation\n### vc‑node\n- Minimal config: hub URL/SSH target, machine_id, enabled collectors.\n- Local spool directory for compressed JSONL/Parquet batches.\n- Signed manifest per batch (hash + size + schema version).\n\n### Hub Ingest\n- Add `vc ingest --from <bundle>` to import snapshots.\n- Deduplicate using (machine_id, collector, cursor, payload_hash).\n- Record ingest audit events.\n\n### Transfer Modes\n- SSH push (scp/rsync) for MVP.\n- Optional HTTPS upload later.\n\n## Risks\n- Artifact size growth if network is down; require max spool size.\n- Duplicate ingestion if dedupe keys are weak.\n\n## Success Criteria\n- Remote machine can collect without continuous SSH polling.\n- Backfilled bundles ingest cleanly with no duplicates.\n\n## Tests & E2E\n- Unit tests for manifest hashing + dedupe keys.\n- Integration tests for bundle ingest with synthetic fixtures.\n- E2E: `tests/e2e/remote/test_vc_node_push.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:38:09.567898497Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:18.627602821Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.5","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T22:16:17.770662983Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.5","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T21:38:09.567898497Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.5","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T21:38:09.567898497Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.5","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:18.627128377Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.5","depends_on_id":"bd-3nb.1","type":"blocks","created_at":"2026-01-27T21:38:09.567898497Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.5","depends_on_id":"bd-3nb.2","type":"blocks","created_at":"2026-01-27T21:38:09.567898497Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.5","depends_on_id":"bd-3nb.4","type":"blocks","created_at":"2026-01-27T21:38:09.567898497Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.5","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:38:09.567898497Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.6","title":"Implement Prometheus exporter for vc metrics","description":"## Background\nvc should integrate with existing observability stacks. A Prometheus exporter lets you build Grafana dashboards and alerts without custom tooling.\n\n## Goals\n- Expose `/metrics` endpoint with vc internal metrics and key fleet KPIs.\n- Include collector freshness, alert counts, and health scores.\n- Keep labels stable and documented.\n- Provide a starter Grafana dashboard JSON.\n\n## Implementation\n### Metrics\n- `vc_collector_freshness_seconds{machine,collector}`\n- `vc_collector_success_total{machine,collector}`\n- `vc_alerts_open_total{severity}`\n- `vc_health_score{machine}`\n- `vc_poll_duration_seconds{machine}`\n\n### Server\n- Add metrics router to vc_web or standalone metrics server.\n- Use `prometheus` crate + `hyper` or axum.\n\n### Grafana\n- Ship a `docs/grafana/vc_overview.json` template referencing these metrics.\n\n## Risks\n- High-cardinality labels; must cap machine/collector label sets.\n\n## Success Criteria\n- Metrics scrape works locally and remotely.\n- Grafana dashboard can be built from exposed metrics.\n\n## Tests & E2E\n- Unit tests for metric registration + label cardinality limits.\n- E2E: `tests/e2e/web/test_prometheus_export.sh` with scrape verification + JSON summary.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:38:17.991935727Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:20.384261670Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.6","depends_on_id":"bd-15a","type":"blocks","created_at":"2026-01-27T21:38:17.991935727Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.6","depends_on_id":"bd-1h6","type":"blocks","created_at":"2026-01-27T21:38:17.991935727Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.6","depends_on_id":"bd-2xm.5","type":"blocks","created_at":"2026-01-27T22:16:19.458007829Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.6","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:20.383549438Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.6","depends_on_id":"bd-3e2.2","type":"blocks","created_at":"2026-01-27T21:38:17.991935727Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.6","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:38:17.991935727Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.6","depends_on_id":"bd-new.1","type":"blocks","created_at":"2026-01-27T21:40:27.764057727Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.7","title":"Implement GitHub issues/PR collector + correlation","description":"## Background\nRepo status is more useful when correlated with issues/PRs and repo activity. This integrates GitHub metadata to show “what’s blocked” and “what’s waiting.”\n\n## Goals\n- Collect issues/PR counts, states, and labels per repo.\n- Correlate with ru repo_id for dashboards and alerts.\n- Cache results to avoid API rate limits.\n\n## Implementation\n### Integration\n- Use `gh` CLI or GitHub REST API with token from config.\n- Support org-wide + per-repo queries.\n- Cache ETag/If‑Modified‑Since to reduce API calls.\n\n### Tables\n```sql\nCREATE TABLE gh_repo_issue_pr_snapshot(\n    repo_id TEXT,\n    collected_at TIMESTAMP,\n    open_issues INTEGER,\n    open_prs INTEGER,\n    triage_json TEXT,\n    label_breakdown_json TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (repo_id, collected_at)\n);\n```\n\n## Risks\n- API throttling; must respect rate limits.\n- Repo mapping inconsistencies (path vs URL).\n\n## Success Criteria\n- Repo dashboard shows issue/PR counts and top labels.\n- Collector respects GitHub rate limits.\n\n## Tests & E2E\n- Unit tests for mapping repo_id to GH repo.\n- Mock API tests for rate-limit handling.\n- E2E: `tests/e2e/collectors/test_github_collect.sh` with JSON summary + logs.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-27T21:38:27.672542286Z","created_by":"ubuntu","updated_at":"2026-01-28T18:09:46.345530803Z","closed_at":"2026-01-28T18:09:46.345400507Z","close_reason":"GhCollector implemented and registered - github.rs with issue/PR collection, triage summary, label breakdown","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.7","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T22:16:21.363975126Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.7","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T21:38:27.672542286Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.7","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T21:38:27.672542286Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.7","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:22.375529078Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.7","depends_on_id":"bd-33h.2","type":"blocks","created_at":"2026-01-27T21:38:27.672542286Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.7","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:38:27.672542286Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.8","title":"Implement cost attribution analytics","description":"## Background\nWe track usage (caut/caam) and session volume (cass) but lack cost attribution per repo/agent. This is crucial for optimizing spend and planning.\n\n## Goals\n- Attribute cost and usage to repo, machine, and agent type.\n- Estimate cost per completed bead/issue where possible.\n- Surface top cost drivers in TUI/web/robot.\n- Emit confidence scores for attribution quality.\n\n## Implementation\n### Inputs\n- caut usage snapshots, caam profiles, cass session stats, ru repo mappings.\n- Map sessions to repo via cass metadata + repo paths.\n- Confidence score derived from match quality (path match, session metadata, repo activity window).\n\n### Tables\n```sql\nCREATE TABLE cost_attribution_snapshot(\n    collected_at TIMESTAMP,\n    repo_id TEXT,\n    machine_id TEXT,\n    agent_type TEXT,\n    provider TEXT,\n    estimated_cost_usd REAL,\n    tokens_used BIGINT,\n    sessions_count INTEGER,\n    confidence REAL,\n    raw_json TEXT\n);\n```\n\n### Output\n- “Top cost drivers” widget.\n- `vc robot cost --json` summary.\n\n## Risks\n- Imperfect mapping of sessions to repos; document confidence score.\n\n## Success Criteria\n- Cost summaries match manual sanity checks within ±10%.\n- Top 5 cost drivers visible in UI.\n- Low-confidence rows are flagged.\n\n## Tests & E2E\n- Unit tests for mapping + cost estimation.\n- Fixture‑based tests for cross‑tool joins.\n- E2E: `tests/e2e/system/test_cost_attribution.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:38:38.588520035Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:23.319092129Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.8","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T21:38:38.588520035Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.8","depends_on_id":"bd-2mv.1","type":"blocks","created_at":"2026-01-27T21:38:38.588520035Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.8","depends_on_id":"bd-2mv.2","type":"blocks","created_at":"2026-01-27T21:38:38.588520035Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.8","depends_on_id":"bd-2mv.3","type":"blocks","created_at":"2026-01-27T21:38:38.588520035Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.8","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:23.318314434Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.8","depends_on_id":"bd-33h.2","type":"blocks","created_at":"2026-01-27T21:38:38.588520035Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.8","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:38:38.588520035Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-new.9","title":"Implement schema registry + robot/API docs generator","description":"## Background\nRobot outputs and API schemas must remain stable for agents. A formal schema registry + doc generator prevents contract drift and makes integration safe.\n\n## Goals\n- Maintain JSON Schema definitions for robot outputs, config, and web API responses.\n- Generate human‑readable docs + examples automatically.\n- Validate outputs in tests (schema compliance).\n- Detect schema changes in CI via diff checks.\n\n## Implementation\n### Schema Registry\n- Store schema files under `docs/schemas/` with versioning.\n- Add `vc robot-docs schemas|examples` to render docs.\n- Include config schema for `vc.toml` validation.\n\n### Validation\n- Test helper to validate JSON against schema in unit tests and E2E.\n- Fail fast if schema version mismatches.\n- CI step to diff generated schemas vs committed ones.\n\n## Risks\n- Schema maintenance overhead; automate generation where possible.\n\n## Success Criteria\n- Robot outputs pass schema validation in CI.\n- Docs list all schemas with examples.\n- Schema diffs are explicit and reviewed.\n\n## Tests & E2E\n- Unit tests for schema validation helpers.\n- E2E: `tests/e2e/system/test_robot_schemas.sh` with JSON summary + logs.","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-27T21:38:46.451400553Z","created_by":"ubuntu","updated_at":"2026-01-27T22:16:24.353125511Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-new.9","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T21:38:46.451400553Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.9","depends_on_id":"bd-2a9.5","type":"blocks","created_at":"2026-01-27T21:38:46.451400553Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.9","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T22:16:24.352303753Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.9","depends_on_id":"bd-3e2.2","type":"blocks","created_at":"2026-01-27T21:38:46.451400553Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-new.9","depends_on_id":"bd-new","type":"parent-child","created_at":"2026-01-27T21:38:46.451400553Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-rf7","title":"Implement bv + br (Beads) collector","description":"Implement collector for bv (Beads Viewer) and br (Beads Rust) - task tracking and productivity metrics.\n\nINTEGRATION METHOD:\n- bv --robot-triage for mega-command JSON output\n- br ready --json for actionable work\n- br sync --flush-only to export JSONL, then read .beads/issues.jsonl\n- Use CLI Snapshot pattern\n\nDUCKDB TABLES:\nCREATE TABLE beads_triage_snapshots(\n    machine_id TEXT,\n    collected_at TIMESTAMP,\n    repo_id TEXT,\n    quick_ref_json TEXT,\n    recommendations_json TEXT,\n    project_health_json TEXT,\n    raw_json TEXT,\n    PRIMARY KEY (machine_id, collected_at, repo_id)\n);\n\nCREATE TABLE beads_issues(\n    repo_id TEXT,\n    issue_id TEXT,\n    status TEXT,\n    priority TEXT,\n    type TEXT,\n    title TEXT,\n    labels_json TEXT,\n    deps_json TEXT,\n    updated_at TIMESTAMP,\n    raw_json TEXT,\n    PRIMARY KEY (repo_id, issue_id)\n);\n\nCREATE TABLE beads_graph_metrics(\n    repo_id TEXT,\n    collected_at TIMESTAMP,\n    pagerank_json TEXT,\n    betweenness_json TEXT,\n    critical_path_json TEXT,\n    cycles_json TEXT,\n    PRIMARY KEY (repo_id, collected_at)\n);\n\nCOLLECTOR IMPLEMENTATION:\n- Implement BeadsCollector with Collector trait\n- Run bv --robot-triage in each repo with .beads directory\n- Parse triage output for quick_ref, recommendations, health\n- Store graph metrics (pagerank, betweenness, critical path)\n- Handle missing bv/br binaries gracefully\n\nREPO DISCOVERY:\n- Get repo list from ru collector data\n- Check each repo for .beads directory\n- Only collect from repos with beads initialized\n\nUI VALUE:\n- What to work on next across all projects\n- Project health at a glance\n- Bottleneck identification\n- Dependency visualization\n\nTESTS:\n- Create test .beads databases\n- Mock bv --robot-triage output\n- Verify issue synchronization\n\n## E2E & Logging\n- Add/extend `tests/e2e/collectors/test_<collector>.sh` (see bd-30z).\n- Emit structured logs: collector, machine_id, duration_ms, rows_inserted, stderr.\n- Write JSON summary artifact to `tests/logs/`.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:48:13.569904049Z","created_by":"ubuntu","updated_at":"2026-01-28T18:10:15.380769541Z","closed_at":"2026-01-28T18:10:15.380490566Z","close_reason":"BeadsCollector implemented and registered - beads.rs with bv triage parsing, br list parsing, 6 unit tests","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-rf7","depends_on_id":"bd-2a9.2","type":"blocks","created_at":"2026-01-27T17:00:10.806701196Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-rf7","depends_on_id":"bd-2a9.3","type":"blocks","created_at":"2026-01-27T16:53:19.316979612Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-rf7","depends_on_id":"bd-2a9.4","type":"blocks","created_at":"2026-01-27T16:53:17.414649418Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-rf7","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:34.354059880Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-rf7","depends_on_id":"bd-33h","type":"parent-child","created_at":"2026-01-27T16:55:37.448394277Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-rf7","depends_on_id":"bd-33h.2","type":"blocks","created_at":"2026-01-27T16:53:21.077661621Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-tg1","title":"Implement TUI Guardian screen","description":"Implement the Guardian TUI screen showing healing protocol status, active interventions, and history.\n\nSCREEN CONTENT:\n- Guardian status: Active/Disabled, mode (suggest/execute)\n- Active protocols: Currently running healing actions\n- Pending interventions: Queued actions awaiting execution\n- History: Recent healing actions and outcomes\n\nLAYOUT:\n+------------------------------------------------+\n| GUARDIAN - SELF-HEALING            [t]oggle mode |\n+------------------------------------------------+\n| STATUS                                          |\n| ├─ Mode: execute-safe (allowlisted commands)    |\n| ├─ Patterns detected: 3 active                  |\n| ├─ Last action: 2 min ago                       |\n| └─ Success rate: 94% (47/50 last week)         |\n+------------------------------------------------+\n| ACTIVE PROTOCOLS                                |\n| ├─ rate_limit_recovery on orko                  |\n| │   └─ Step 2/4: Preparing account swap         |\n| │   └─ Started: 45 sec ago                      |\n| └─ disk_space_recovery on mac-mini              |\n|     └─ Step 1/4: Analyzing disk consumers       |\n+------------------------------------------------+\n| PENDING INTERVENTIONS                           |\n| ├─ cc_5 stuck_agent_recovery                    |\n| │   └─ Waiting for: manual approval             |\n| │   └─ Actions: send interrupt, force compact   |\n+------------------------------------------------+\n| HISTORY (last 24h)                              |\n| ├─ [OK] rate_limit_recovery (orko) - 2min ago  |\n| │   └─ Swapped jeff@email.com -> backup@email   |\n| ├─ [OK] disk_cleanup (sydneymc) - 1hr ago      |\n| │   └─ Cleared 12GB of agent caches             |\n| ├─ [FAIL] stuck_agent (orko) - 3hr ago         |\n| │   └─ Agent did not recover, escalated         |\n+------------------------------------------------+\n\nNAVIGATION:\n- Tab between sections\n- Enter on protocol to see details\n- a to approve pending intervention\n- r to reject/skip intervention\n- p to pause guardian temporarily\n- h to view full history\n\nDATA SOURCES:\n- Guardian state from vc_guardian crate\n- Healing protocol definitions\n- Audit events for history\n- Alert correlation data\n\nIMPLEMENTATION:\n- Create guardian.rs in vc_tui/src/screens/\n- Real-time protocol step progress\n- Color-code by outcome (green=success, red=fail, yellow=pending)\n- Show detailed audit trail for each action\n- Support mode switching (off/suggest/execute-safe/with-approval)\n\nTESTS:\n- Snapshot tests for rendering\n- Protocol state display tests\n- History pagination tests\n\n## E2E & Logging\n- Add/extend `tests/e2e/tui/test_guardian_screen.sh` with expect/pexpect.\n- Capture logs and screenshots on failure; emit JSON summary in tests/logs/.\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-27T16:49:52.146503462Z","created_by":"ubuntu","updated_at":"2026-01-27T21:37:43.659321991Z","closed_at":"2026-01-27T21:37:43.659303116Z","close_reason":"Merged into bd-2xm.7 (comprehensive Phase 4 TUI screens bead)","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-tg1","depends_on_id":"bd-2xm","type":"parent-child","created_at":"2026-01-27T16:55:44.589034144Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-tg1","depends_on_id":"bd-2xm.6","type":"blocks","created_at":"2026-01-27T16:54:31.351949120Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-tg1","depends_on_id":"bd-30z","type":"blocks","created_at":"2026-01-27T20:37:35.535041566Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-tg1","depends_on_id":"bd-33h.3","type":"blocks","created_at":"2026-01-27T16:54:30.204292783Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
